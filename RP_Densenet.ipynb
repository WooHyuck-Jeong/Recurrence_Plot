{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\jwhyu\\AppData\\Local\\Temp/ipykernel_3912/3517717524.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__) \n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8390508395338722161\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1059449336242919708\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/Result/ver.3.22/RP/RP.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X_data,y_data,i_data, test_size = 0.2, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 300, 300, 2)\n",
      "(943,)\n",
      "(236, 300, 300, 2)\n",
      "(236,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train,X_test))\n",
    "targets = np.concatenate((y_train,y_test))\n",
    "index = np.concatenate((i_train,i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "targets = np_utils.to_categorical(targets)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 306, 306, 2)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 150, 150, 64  6272        ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 150, 150, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 150, 150, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 152, 152, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 75, 75, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 75, 75, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 75, 75, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 75, 75, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 75, 75, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 75, 75, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 75, 75, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 75, 75, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 75, 75, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 75, 75, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 75, 75, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 75, 75, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 75, 75, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 75, 75, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 75, 75, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 75, 75, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 75, 75, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 75, 75, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 75, 75, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 75, 75, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 75, 75, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 75, 75, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 75, 75, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 75, 75, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 75, 75, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 75, 75, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 37, 37, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 37, 37, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 37, 37, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 37, 37, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 37, 37, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 37, 37, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 37, 37, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 37, 37, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 37, 37, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 37, 37, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 37, 37, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 37, 37, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 37, 37, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 37, 37, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 37, 37, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 37, 37, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 37, 37, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 37, 37, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 37, 37, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 37, 37, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 37, 37, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 37, 37, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 37, 37, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 37, 37, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 37, 37, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 37, 37, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 37, 37, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 37, 37, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 37, 37, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 37, 37, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 37, 37, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 37, 37, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 37, 37, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 37, 37, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 37, 37, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 37, 37, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 37, 37, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 37, 37, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 37, 37, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 37, 37, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 37, 37, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 37, 37, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 37, 37, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 37, 37, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 37, 37, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 37, 37, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 37, 37, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 37, 37, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 37, 37, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 37, 37, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 37, 37, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 18, 18, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 18, 18, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 18, 18, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 18, 18, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 18, 18, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 18, 18, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 18, 18, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 18, 18, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 18, 18, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 18, 18, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 18, 18, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 18, 18, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 18, 18, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 18, 18, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 18, 18, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 18, 18, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 18, 18, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 18, 18, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 18, 18, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 18, 18, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 18, 18, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 18, 18, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 18, 18, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 18, 18, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 18, 18, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 18, 18, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 18, 18, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 18, 18, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 18, 18, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 18, 18, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 18, 18, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 18, 18, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 18, 18, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 18, 18, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 18, 18, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 18, 18, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 18, 18, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 18, 18, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 18, 18, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 18, 18, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 18, 18, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 18, 18, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 18, 18, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 18, 18, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 18, 18, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 18, 18, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 18, 18, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 18, 18, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 18, 18, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 18, 18, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 18, 18, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 18, 18, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 18, 18, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 18, 18, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 18, 18, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 18, 18, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 18, 18, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 18, 18, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 18, 18, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 18, 18, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 18, 18, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 18, 18, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 18, 18, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 18, 18, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 18, 18, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 18, 18, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 18, 18, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 18, 18, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 18, 18, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 18, 18, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 18, 18, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 18, 18, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 18, 18, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 18, 18, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 18, 18, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 18, 18, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 18, 18, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 18, 18, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 18, 18, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 18, 18, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 18, 18, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 18, 18, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 18, 18, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 18, 18, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 18, 18, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 18, 18, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 18, 18, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 18, 18, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 18, 18, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 18, 18, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 18, 18, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 18, 18, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 18, 18, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 18, 18, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 18, 18, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 18, 18, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 18, 18, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 18, 18, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 18, 18, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 18, 18, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 9, 9, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 9, 9, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 9, 9, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 9, 9, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 9, 9, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 9, 9, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 9, 9, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 9, 9, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 9, 9, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 9, 9, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 9, 9, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 9, 9, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 9, 9, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 9, 9, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 9, 9, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 9, 9, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 9, 9, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 9, 9, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 9, 9, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 9, 9, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 9, 9, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 9, 9, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 9, 9, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 9, 9, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 9, 9, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 9, 9, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 9, 9, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 9, 9, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 9, 9, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 9, 9, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 9, 9, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 9, 9, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 9, 9, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 9, 9, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 9, 9, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 9, 9, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 9, 9, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 9, 9, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 9, 9, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 9, 9, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 9, 9, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 9, 9, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 9, 9, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 9, 9, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 9, 9, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 9, 9, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 9, 9, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 9, 9, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 9, 9, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 9, 9, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 9, 9, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 9, 9, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 9, 9, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 9, 9, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 9, 9, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 9, 9, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 9, 9, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 9, 9, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 9, 9, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 9, 9, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 9, 9, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 9, 9, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 9, 9, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 9, 9, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 9, 9, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 9, 9, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 9, 9, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1024)        0           ['relu[0][0]']                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3075        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,037,443\n",
      "Trainable params: 6,953,795\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(300, 300, 2))\n",
    "model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    " \n",
    "x = model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateSchedule(Callback):\n",
    "    def __init__(self, selected_epochs=[]):\n",
    "        self.selected_epochs = selected_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) in self.selected_epochs:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            K.set_value(self.model.optimizer.lr, lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "117\n",
      "1062\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "test = []\n",
    "train= []\n",
    "test_ = []\n",
    "train_ = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1061]\n",
    "train = np.reshape(train, 1061)\n",
    "test = test_[0:117]\n",
    "test = np.reshape(test, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   9   17   34   40   46   53   54   83   97  101  109  113  128  140\n",
      "  141  146  168  171  179  186  204  228  253  263  264  280  306  316\n",
      "  358  372  374  377  379  389  412  422  431  432  446  449  459  463\n",
      "  467  473  484  492  508  509  510  513  523  551  564  571  597  602\n",
      "  620  631  632  642  663  665  666  685  698  716  724  729  732  734\n",
      "  756  761  771  793  797  804  815  819  831  836  840  841  879  883\n",
      "  884  890  907  912  915  920  936  941  946  955  963  975  982  983\n",
      "  985 1003 1013 1025 1047 1050 1053 1067 1070 1077 1083 1102 1105 1111\n",
      " 1120 1154 1166 1172 1177]\n",
      "[   0    1    2 ... 1174 1175 1176]\n",
      "['N_10' 'N_385' 'P_377' 'H_30' 'H_367' 'P_105' 'P_89' 'P_359' 'H_69'\n",
      " 'H_88' 'P_33' 'H_169' 'P_366' 'P_251' 'H_166' 'N_64' 'N_275' 'H_178'\n",
      " 'N_237' 'H_376' 'P_331' 'P_207' 'N_256' 'H_375' 'N_299' 'P_34' 'N_249'\n",
      " 'N_110' 'H_213' 'H_171' 'P_271' 'P_104' 'N_262' 'H_79' 'H_353' 'H_47'\n",
      " 'P_328' 'N_2' 'N_35' 'H_115' 'N_219' 'N_345' 'P_269' 'P_334' 'H_112'\n",
      " 'H_186' 'H_262' 'H_241' 'N_125' 'P_74' 'N_99' 'H_161' 'H_18' 'N_165'\n",
      " 'N_115' 'H_119' 'H_183' 'P_139' 'N_53' 'N_259' 'N_162' 'N_234' 'H_373'\n",
      " 'H_372' 'P_313' 'P_350' 'H_150' 'H_49' 'P_154' 'N_169' 'N_359' 'H_291'\n",
      " 'H_383' 'P_167' 'H_365' 'H_106' 'N_215' 'N_336' 'N_40' 'N_218' 'N_184'\n",
      " 'H_214' 'P_178' 'H_250' 'H_127' 'P_323' 'N_78' 'H_391' 'P_360' 'H_98'\n",
      " 'P_5' 'P_333' 'H_96' 'H_359' 'N_272' 'N_211' 'N_123' 'H_34' 'N_203'\n",
      " 'N_205' 'P_325' 'N_128' 'N_357' 'P_94' 'P_162' 'N_117' 'N_386' 'P_133'\n",
      " 'H_17' 'H_71' 'P_22' 'N_306' 'N_11' 'H_329' 'H_170' 'H_341' 'P_308']\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)\n",
    "print(index[test])\n",
    "print(targets[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir('E:/Result/ver.3.22/RP' + '/' + 'train')\n",
    "os.mkdir('E:/Result/ver.3.22/RP' + '/' + 'test')\n",
    "os.mkdir('E:/Result/ver.3.22/RP/' + 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848,) (95,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 35s 110ms/step - loss: 0.3555 - accuracy: 0.7736 - val_loss: 0.2947 - val_accuracy: 0.7474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.2962 - accuracy: 0.8314 - val_loss: 0.3791 - val_accuracy: 0.7263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.3013 - accuracy: 0.8160 - val_loss: 0.1034 - val_accuracy: 0.9474\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2436 - accuracy: 0.8632 - val_loss: 0.2215 - val_accuracy: 0.8632\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2090 - accuracy: 0.8797 - val_loss: 0.2077 - val_accuracy: 0.8842\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.1948 - accuracy: 0.8785 - val_loss: 0.1955 - val_accuracy: 0.8947\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.2423 - accuracy: 0.8573 - val_loss: 0.2865 - val_accuracy: 0.7579\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2019 - accuracy: 0.8915 - val_loss: 0.3547 - val_accuracy: 0.8316\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1747 - accuracy: 0.9057 - val_loss: 0.6011 - val_accuracy: 0.7263\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1877 - accuracy: 0.8880 - val_loss: 0.1198 - val_accuracy: 0.9158\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1565 - accuracy: 0.9151 - val_loss: 0.1231 - val_accuracy: 0.9158\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1584 - accuracy: 0.9092 - val_loss: 0.2620 - val_accuracy: 0.8526\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1684 - accuracy: 0.9139 - val_loss: 0.7096 - val_accuracy: 0.7579\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1358 - accuracy: 0.9304 - val_loss: 0.0996 - val_accuracy: 0.9684\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1347 - accuracy: 0.9340 - val_loss: 0.0969 - val_accuracy: 0.9263\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.1331 - accuracy: 0.9281 - val_loss: 0.8912 - val_accuracy: 0.6632\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.1262 - accuracy: 0.9340 - val_loss: 0.1119 - val_accuracy: 0.9263\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.1341 - accuracy: 0.9269 - val_loss: 0.1894 - val_accuracy: 0.8737\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0972 - accuracy: 0.9493 - val_loss: 0.1208 - val_accuracy: 0.9053\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.1021 - accuracy: 0.9469 - val_loss: 0.3380 - val_accuracy: 0.8211\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.1217 - accuracy: 0.9410 - val_loss: 0.1796 - val_accuracy: 0.8842\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0959 - accuracy: 0.9528 - val_loss: 0.1677 - val_accuracy: 0.8842\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0837 - accuracy: 0.9623 - val_loss: 0.1716 - val_accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0737 - accuracy: 0.9717 - val_loss: 0.1774 - val_accuracy: 0.8842\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.0795 - accuracy: 0.9599 - val_loss: 0.1744 - val_accuracy: 0.9158\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.0948 - accuracy: 0.9564 - val_loss: 0.1696 - val_accuracy: 0.8842\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0702 - accuracy: 0.9693 - val_loss: 0.1556 - val_accuracy: 0.9053\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0668 - accuracy: 0.9729 - val_loss: 0.1862 - val_accuracy: 0.8316\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0861 - accuracy: 0.9599 - val_loss: 0.1656 - val_accuracy: 0.9368\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0839 - accuracy: 0.9587 - val_loss: 0.1525 - val_accuracy: 0.9263\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0848 - accuracy: 0.9634 - val_loss: 0.1611 - val_accuracy: 0.8842\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0699 - accuracy: 0.9741 - val_loss: 0.1626 - val_accuracy: 0.8737\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0749 - accuracy: 0.9741 - val_loss: 0.1382 - val_accuracy: 0.9158\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0778 - accuracy: 0.9693 - val_loss: 0.1559 - val_accuracy: 0.8632\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0812 - accuracy: 0.9670 - val_loss: 0.1397 - val_accuracy: 0.9158\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0791 - accuracy: 0.9682 - val_loss: 0.1727 - val_accuracy: 0.8421\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0624 - accuracy: 0.9729 - val_loss: 0.1510 - val_accuracy: 0.8737\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0818 - accuracy: 0.9658 - val_loss: 0.1585 - val_accuracy: 0.8526\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0815 - accuracy: 0.9575 - val_loss: 0.1415 - val_accuracy: 0.9263\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0751 - accuracy: 0.9741 - val_loss: 0.1443 - val_accuracy: 0.8737\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0860 - accuracy: 0.9587 - val_loss: 0.1417 - val_accuracy: 0.8842\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0894 - accuracy: 0.9575 - val_loss: 0.1411 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0548 - accuracy: 0.9776 - val_loss: 0.1422 - val_accuracy: 0.8737\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0614 - accuracy: 0.9741 - val_loss: 0.1424 - val_accuracy: 0.8737\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0674 - accuracy: 0.9752 - val_loss: 0.1445 - val_accuracy: 0.8737\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0637 - accuracy: 0.9811 - val_loss: 0.1523 - val_accuracy: 0.8737\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0614 - accuracy: 0.9752 - val_loss: 0.1400 - val_accuracy: 0.8947\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0473 - accuracy: 0.9858 - val_loss: 0.1514 - val_accuracy: 0.8737\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0681 - accuracy: 0.9741 - val_loss: 0.1518 - val_accuracy: 0.8737\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0686 - accuracy: 0.9729 - val_loss: 0.1545 - val_accuracy: 0.8737\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0631 - accuracy: 0.9752 - val_loss: 0.1555 - val_accuracy: 0.8737\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0613 - accuracy: 0.9823 - val_loss: 0.1465 - val_accuracy: 0.8737\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0693 - accuracy: 0.9705 - val_loss: 0.1476 - val_accuracy: 0.8737\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0534 - accuracy: 0.9858 - val_loss: 0.1518 - val_accuracy: 0.8737\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0862 - accuracy: 0.9599 - val_loss: 0.1431 - val_accuracy: 0.8842\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0527 - accuracy: 0.9764 - val_loss: 0.1547 - val_accuracy: 0.8737\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0956 - accuracy: 0.9540 - val_loss: 0.1475 - val_accuracy: 0.8737\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0746 - accuracy: 0.9705 - val_loss: 0.1469 - val_accuracy: 0.8737\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0747 - accuracy: 0.9682 - val_loss: 0.1485 - val_accuracy: 0.8737\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0821 - accuracy: 0.9646 - val_loss: 0.1495 - val_accuracy: 0.8737\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0883 - accuracy: 0.9481 - val_loss: 0.1431 - val_accuracy: 0.8737\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0672 - accuracy: 0.9705 - val_loss: 0.1482 - val_accuracy: 0.8737\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0612 - accuracy: 0.9705 - val_loss: 0.1478 - val_accuracy: 0.8737\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0591 - accuracy: 0.9800 - val_loss: 0.1497 - val_accuracy: 0.8737\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0764 - accuracy: 0.9717 - val_loss: 0.1476 - val_accuracy: 0.8737\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0626 - accuracy: 0.9764 - val_loss: 0.1497 - val_accuracy: 0.8737\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0574 - accuracy: 0.9823 - val_loss: 0.1467 - val_accuracy: 0.8737\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0935 - accuracy: 0.9575 - val_loss: 0.1466 - val_accuracy: 0.8737\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0609 - accuracy: 0.9764 - val_loss: 0.1383 - val_accuracy: 0.8842\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0644 - accuracy: 0.9752 - val_loss: 0.1447 - val_accuracy: 0.8737\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.1003 - accuracy: 0.9481 - val_loss: 0.1474 - val_accuracy: 0.8737\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0974 - accuracy: 0.9564 - val_loss: 0.1477 - val_accuracy: 0.8737\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0656 - accuracy: 0.9717 - val_loss: 0.1482 - val_accuracy: 0.8737\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0685 - accuracy: 0.9705 - val_loss: 0.1464 - val_accuracy: 0.8737\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0581 - accuracy: 0.9741 - val_loss: 0.1500 - val_accuracy: 0.8737\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 21s 99ms/step - loss: 0.0645 - accuracy: 0.9788 - val_loss: 0.1506 - val_accuracy: 0.8737\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 0.0794 - accuracy: 0.9599 - val_loss: 0.1467 - val_accuracy: 0.8737\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 21s 99ms/step - loss: 0.0703 - accuracy: 0.9729 - val_loss: 0.1462 - val_accuracy: 0.8737\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0942 - accuracy: 0.9458 - val_loss: 0.1441 - val_accuracy: 0.8737\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0805 - accuracy: 0.9599 - val_loss: 0.1463 - val_accuracy: 0.8737\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0558 - accuracy: 0.9811 - val_loss: 0.1454 - val_accuracy: 0.8737\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0793 - accuracy: 0.9658 - val_loss: 0.1439 - val_accuracy: 0.8737\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0900 - accuracy: 0.9623 - val_loss: 0.1475 - val_accuracy: 0.8737\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0724 - accuracy: 0.9717 - val_loss: 0.1461 - val_accuracy: 0.8737\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0608 - accuracy: 0.9729 - val_loss: 0.1455 - val_accuracy: 0.8737\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0744 - accuracy: 0.9670 - val_loss: 0.1455 - val_accuracy: 0.8737\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0858 - accuracy: 0.9670 - val_loss: 0.1397 - val_accuracy: 0.8737\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0563 - accuracy: 0.9835 - val_loss: 0.1447 - val_accuracy: 0.8737\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0549 - accuracy: 0.9882 - val_loss: 0.1512 - val_accuracy: 0.8737\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1065 - accuracy: 0.9505 - val_loss: 0.1432 - val_accuracy: 0.8737\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0800 - accuracy: 0.9717 - val_loss: 0.1425 - val_accuracy: 0.8737\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0626 - accuracy: 0.9717 - val_loss: 0.1475 - val_accuracy: 0.8737\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0763 - accuracy: 0.9658 - val_loss: 0.1498 - val_accuracy: 0.8737\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0756 - accuracy: 0.9646 - val_loss: 0.1496 - val_accuracy: 0.8737\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0749 - accuracy: 0.9682 - val_loss: 0.1471 - val_accuracy: 0.8737\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0700 - accuracy: 0.9752 - val_loss: 0.1491 - val_accuracy: 0.8737\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0796 - accuracy: 0.9646 - val_loss: 0.1472 - val_accuracy: 0.8737\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0710 - accuracy: 0.9693 - val_loss: 0.1502 - val_accuracy: 0.8737\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0949 - accuracy: 0.9587 - val_loss: 0.1454 - val_accuracy: 0.8737\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0570 - accuracy: 0.9788 - val_loss: 0.1487 - val_accuracy: 0.8737\n",
      "Score for fold 1: loss of 0.14867378771305084; accuracy of 87.36842274665833%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 27s 100ms/step - loss: 0.3968 - accuracy: 0.7406 - val_loss: 1.2728 - val_accuracy: 0.6526\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2886 - accuracy: 0.8172 - val_loss: 0.5065 - val_accuracy: 0.7579\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.2685 - accuracy: 0.8396 - val_loss: 0.4375 - val_accuracy: 0.7684\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2372 - accuracy: 0.8467 - val_loss: 0.4036 - val_accuracy: 0.7053\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2252 - accuracy: 0.8868 - val_loss: 0.1675 - val_accuracy: 0.8842\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2148 - accuracy: 0.8667 - val_loss: 0.1144 - val_accuracy: 0.9053\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2095 - accuracy: 0.8644 - val_loss: 1.0376 - val_accuracy: 0.7158\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2044 - accuracy: 0.8738 - val_loss: 0.2234 - val_accuracy: 0.8737\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1824 - accuracy: 0.8950 - val_loss: 0.0939 - val_accuracy: 0.9579\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1754 - accuracy: 0.8998 - val_loss: 0.1483 - val_accuracy: 0.8842\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.1746 - accuracy: 0.9139 - val_loss: 1.0137 - val_accuracy: 0.6947\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1656 - accuracy: 0.9104 - val_loss: 0.3519 - val_accuracy: 0.8211\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1367 - accuracy: 0.9210 - val_loss: 0.2129 - val_accuracy: 0.8632\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1373 - accuracy: 0.9375 - val_loss: 0.2859 - val_accuracy: 0.8526\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1291 - accuracy: 0.9340 - val_loss: 0.1707 - val_accuracy: 0.8947\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1288 - accuracy: 0.9304 - val_loss: 0.2011 - val_accuracy: 0.8737\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1312 - accuracy: 0.9351 - val_loss: 0.1425 - val_accuracy: 0.9053\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1131 - accuracy: 0.9410 - val_loss: 0.2100 - val_accuracy: 0.8842\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1036 - accuracy: 0.9575 - val_loss: 0.1615 - val_accuracy: 0.8947\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0907 - accuracy: 0.9623 - val_loss: 0.3054 - val_accuracy: 0.8211\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1227 - accuracy: 0.9387 - val_loss: 0.2040 - val_accuracy: 0.8632\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0975 - accuracy: 0.9517 - val_loss: 0.1841 - val_accuracy: 0.8842\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0998 - accuracy: 0.9493 - val_loss: 0.1710 - val_accuracy: 0.8947\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0778 - accuracy: 0.9634 - val_loss: 0.1602 - val_accuracy: 0.8737\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0797 - accuracy: 0.9682 - val_loss: 0.1676 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0914 - accuracy: 0.9634 - val_loss: 0.1898 - val_accuracy: 0.8842\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1152 - accuracy: 0.9399 - val_loss: 0.1763 - val_accuracy: 0.8947\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0802 - accuracy: 0.9705 - val_loss: 0.1676 - val_accuracy: 0.8737\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0899 - accuracy: 0.9646 - val_loss: 0.2148 - val_accuracy: 0.8632\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0694 - accuracy: 0.9717 - val_loss: 0.1815 - val_accuracy: 0.8947\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0665 - accuracy: 0.9717 - val_loss: 0.2269 - val_accuracy: 0.8632\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0738 - accuracy: 0.9693 - val_loss: 0.1661 - val_accuracy: 0.8947\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0753 - accuracy: 0.9705 - val_loss: 0.1844 - val_accuracy: 0.8947\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0954 - accuracy: 0.9564 - val_loss: 0.1629 - val_accuracy: 0.8842\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0582 - accuracy: 0.9800 - val_loss: 0.1969 - val_accuracy: 0.8842\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0799 - accuracy: 0.9670 - val_loss: 0.1694 - val_accuracy: 0.8947\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0689 - accuracy: 0.9693 - val_loss: 0.1630 - val_accuracy: 0.8947\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0647 - accuracy: 0.9752 - val_loss: 0.1722 - val_accuracy: 0.8947\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0836 - accuracy: 0.9623 - val_loss: 0.1671 - val_accuracy: 0.8947\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0781 - accuracy: 0.9646 - val_loss: 0.1760 - val_accuracy: 0.8947\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0584 - accuracy: 0.9752 - val_loss: 0.1765 - val_accuracy: 0.8947\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0745 - accuracy: 0.9705 - val_loss: 0.1821 - val_accuracy: 0.8947\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0575 - accuracy: 0.9788 - val_loss: 0.1859 - val_accuracy: 0.8842\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0836 - accuracy: 0.9634 - val_loss: 0.1820 - val_accuracy: 0.8947\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0673 - accuracy: 0.9729 - val_loss: 0.1874 - val_accuracy: 0.8842\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0707 - accuracy: 0.9646 - val_loss: 0.1832 - val_accuracy: 0.8947\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0789 - accuracy: 0.9670 - val_loss: 0.1873 - val_accuracy: 0.8842\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0688 - accuracy: 0.9693 - val_loss: 0.1766 - val_accuracy: 0.8947\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0894 - accuracy: 0.9646 - val_loss: 0.1840 - val_accuracy: 0.8947\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0817 - accuracy: 0.9623 - val_loss: 0.1745 - val_accuracy: 0.8947\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0973 - accuracy: 0.9540 - val_loss: 0.1809 - val_accuracy: 0.8947\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0640 - accuracy: 0.9776 - val_loss: 0.1860 - val_accuracy: 0.8842\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0619 - accuracy: 0.9752 - val_loss: 0.1849 - val_accuracy: 0.8842\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0951 - accuracy: 0.9564 - val_loss: 0.1832 - val_accuracy: 0.8947\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0664 - accuracy: 0.9717 - val_loss: 0.1831 - val_accuracy: 0.8947\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0725 - accuracy: 0.9634 - val_loss: 0.1784 - val_accuracy: 0.8947\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0835 - accuracy: 0.9634 - val_loss: 0.1832 - val_accuracy: 0.8842\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0756 - accuracy: 0.9693 - val_loss: 0.1805 - val_accuracy: 0.8947\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.1756 - val_accuracy: 0.8947\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1050 - accuracy: 0.9422 - val_loss: 0.1802 - val_accuracy: 0.8947\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0725 - accuracy: 0.9670 - val_loss: 0.1779 - val_accuracy: 0.8947\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0852 - accuracy: 0.9646 - val_loss: 0.1847 - val_accuracy: 0.8842\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0708 - accuracy: 0.9670 - val_loss: 0.1843 - val_accuracy: 0.8947\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0846 - accuracy: 0.9599 - val_loss: 0.1841 - val_accuracy: 0.8842\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0708 - accuracy: 0.9717 - val_loss: 0.1812 - val_accuracy: 0.8947\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0773 - accuracy: 0.9611 - val_loss: 0.1750 - val_accuracy: 0.8947\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0781 - accuracy: 0.9693 - val_loss: 0.1770 - val_accuracy: 0.8947\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0631 - accuracy: 0.9741 - val_loss: 0.1852 - val_accuracy: 0.8842\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0720 - accuracy: 0.9693 - val_loss: 0.1750 - val_accuracy: 0.8947\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0857 - accuracy: 0.9564 - val_loss: 0.1763 - val_accuracy: 0.8947\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0785 - accuracy: 0.9658 - val_loss: 0.1835 - val_accuracy: 0.8947\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0711 - accuracy: 0.9693 - val_loss: 0.1857 - val_accuracy: 0.8842\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0560 - accuracy: 0.9752 - val_loss: 0.1805 - val_accuracy: 0.8947\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0892 - accuracy: 0.9552 - val_loss: 0.1701 - val_accuracy: 0.8947\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0942 - accuracy: 0.9517 - val_loss: 0.1739 - val_accuracy: 0.8947\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0748 - accuracy: 0.9741 - val_loss: 0.1817 - val_accuracy: 0.8947\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0677 - accuracy: 0.9705 - val_loss: 0.1882 - val_accuracy: 0.8842\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0728 - accuracy: 0.9693 - val_loss: 0.1818 - val_accuracy: 0.8947\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0756 - accuracy: 0.9658 - val_loss: 0.1846 - val_accuracy: 0.8947\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0883 - accuracy: 0.9670 - val_loss: 0.1856 - val_accuracy: 0.8842\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0528 - accuracy: 0.9823 - val_loss: 0.1814 - val_accuracy: 0.8947\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0705 - accuracy: 0.9717 - val_loss: 0.1750 - val_accuracy: 0.8947\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0788 - accuracy: 0.9682 - val_loss: 0.1747 - val_accuracy: 0.8947\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0649 - accuracy: 0.9705 - val_loss: 0.1849 - val_accuracy: 0.8842\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0631 - accuracy: 0.9741 - val_loss: 0.1798 - val_accuracy: 0.8947\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0899 - accuracy: 0.9634 - val_loss: 0.1778 - val_accuracy: 0.8947\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0573 - accuracy: 0.9788 - val_loss: 0.1826 - val_accuracy: 0.8947\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0713 - accuracy: 0.9788 - val_loss: 0.1841 - val_accuracy: 0.8842\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0974 - accuracy: 0.9540 - val_loss: 0.1742 - val_accuracy: 0.8947\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0688 - accuracy: 0.9729 - val_loss: 0.1899 - val_accuracy: 0.8842\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0765 - accuracy: 0.9646 - val_loss: 0.1861 - val_accuracy: 0.8842\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0653 - accuracy: 0.9752 - val_loss: 0.1809 - val_accuracy: 0.8947\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0530 - accuracy: 0.9835 - val_loss: 0.1875 - val_accuracy: 0.8842\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0607 - accuracy: 0.9752 - val_loss: 0.1861 - val_accuracy: 0.8842\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0904 - accuracy: 0.9623 - val_loss: 0.1822 - val_accuracy: 0.8947\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0800 - accuracy: 0.9646 - val_loss: 0.1748 - val_accuracy: 0.8947\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1130 - accuracy: 0.9399 - val_loss: 0.1703 - val_accuracy: 0.8947\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0678 - accuracy: 0.9682 - val_loss: 0.1798 - val_accuracy: 0.8947\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0552 - accuracy: 0.9776 - val_loss: 0.1725 - val_accuracy: 0.8947\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0466 - accuracy: 0.9858 - val_loss: 0.1768 - val_accuracy: 0.8947\n",
      "Score for fold 2: loss of 0.17679157853126526; accuracy of 89.47368264198303%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 27s 101ms/step - loss: 0.3681 - accuracy: 0.7535 - val_loss: 0.6347 - val_accuracy: 0.6737\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.3047 - accuracy: 0.8125 - val_loss: 2.0019 - val_accuracy: 0.6632\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2484 - accuracy: 0.8479 - val_loss: 1.0819 - val_accuracy: 0.6632\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2368 - accuracy: 0.8526 - val_loss: 0.1640 - val_accuracy: 0.9158\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2536 - accuracy: 0.8644 - val_loss: 0.2609 - val_accuracy: 0.8842\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2448 - accuracy: 0.8620 - val_loss: 0.1733 - val_accuracy: 0.8842\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2275 - accuracy: 0.8691 - val_loss: 0.3271 - val_accuracy: 0.7789\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1979 - accuracy: 0.8939 - val_loss: 0.1857 - val_accuracy: 0.8842\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1768 - accuracy: 0.9021 - val_loss: 0.1939 - val_accuracy: 0.8842\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1916 - accuracy: 0.8927 - val_loss: 0.2121 - val_accuracy: 0.8632\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1359 - accuracy: 0.9351 - val_loss: 0.5993 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1565 - accuracy: 0.9210 - val_loss: 0.1333 - val_accuracy: 0.8947\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1664 - accuracy: 0.9033 - val_loss: 0.1052 - val_accuracy: 0.9263\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1415 - accuracy: 0.9233 - val_loss: 0.1305 - val_accuracy: 0.9263\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1324 - accuracy: 0.9328 - val_loss: 0.1288 - val_accuracy: 0.9053\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1334 - accuracy: 0.9363 - val_loss: 0.5040 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1239 - accuracy: 0.9304 - val_loss: 0.1719 - val_accuracy: 0.8947\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1324 - accuracy: 0.9351 - val_loss: 0.1932 - val_accuracy: 0.8632\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1482 - accuracy: 0.9292 - val_loss: 0.1600 - val_accuracy: 0.9053\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1193 - accuracy: 0.9481 - val_loss: 0.1282 - val_accuracy: 0.9158\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0976 - accuracy: 0.9599 - val_loss: 0.1087 - val_accuracy: 0.9263\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0853 - accuracy: 0.9634 - val_loss: 0.1059 - val_accuracy: 0.9263\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0851 - accuracy: 0.9658 - val_loss: 0.1005 - val_accuracy: 0.9263\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1057 - accuracy: 0.9540 - val_loss: 0.1108 - val_accuracy: 0.9158\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0842 - accuracy: 0.9658 - val_loss: 0.1111 - val_accuracy: 0.9158\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0876 - accuracy: 0.9587 - val_loss: 0.1065 - val_accuracy: 0.9053\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0771 - accuracy: 0.9670 - val_loss: 0.1265 - val_accuracy: 0.9053\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0601 - accuracy: 0.9764 - val_loss: 0.1194 - val_accuracy: 0.8947\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0659 - accuracy: 0.9729 - val_loss: 0.1073 - val_accuracy: 0.9263\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0608 - accuracy: 0.9800 - val_loss: 0.1030 - val_accuracy: 0.9263\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0768 - accuracy: 0.9693 - val_loss: 0.1189 - val_accuracy: 0.9053\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0770 - accuracy: 0.9670 - val_loss: 0.1156 - val_accuracy: 0.9053\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0955 - accuracy: 0.9564 - val_loss: 0.1202 - val_accuracy: 0.8947\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0670 - accuracy: 0.9764 - val_loss: 0.1234 - val_accuracy: 0.9053\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0714 - accuracy: 0.9693 - val_loss: 0.1134 - val_accuracy: 0.9053\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0704 - accuracy: 0.9693 - val_loss: 0.1193 - val_accuracy: 0.8947\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0572 - accuracy: 0.9811 - val_loss: 0.1235 - val_accuracy: 0.8947\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1102 - val_accuracy: 0.9263\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0654 - accuracy: 0.9717 - val_loss: 0.1182 - val_accuracy: 0.8947\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0548 - accuracy: 0.9811 - val_loss: 0.1065 - val_accuracy: 0.9263\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0742 - accuracy: 0.9693 - val_loss: 0.1142 - val_accuracy: 0.9053\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0764 - accuracy: 0.9682 - val_loss: 0.1090 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0910 - accuracy: 0.9575 - val_loss: 0.1107 - val_accuracy: 0.9158\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0617 - accuracy: 0.9776 - val_loss: 0.1211 - val_accuracy: 0.8947\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0491 - accuracy: 0.9847 - val_loss: 0.1188 - val_accuracy: 0.8947\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0723 - accuracy: 0.9682 - val_loss: 0.1163 - val_accuracy: 0.8947\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0646 - accuracy: 0.9729 - val_loss: 0.1160 - val_accuracy: 0.8947\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0722 - accuracy: 0.9788 - val_loss: 0.1171 - val_accuracy: 0.8947\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0726 - accuracy: 0.9670 - val_loss: 0.1141 - val_accuracy: 0.9053\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0871 - accuracy: 0.9646 - val_loss: 0.1131 - val_accuracy: 0.9053\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 0.1168 - val_accuracy: 0.8947\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0779 - accuracy: 0.9611 - val_loss: 0.1127 - val_accuracy: 0.9158\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0616 - accuracy: 0.9811 - val_loss: 0.1126 - val_accuracy: 0.9053\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0744 - accuracy: 0.9717 - val_loss: 0.1157 - val_accuracy: 0.8947\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0595 - accuracy: 0.9800 - val_loss: 0.1191 - val_accuracy: 0.8947\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0748 - accuracy: 0.9705 - val_loss: 0.1168 - val_accuracy: 0.8947\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0887 - accuracy: 0.9587 - val_loss: 0.1087 - val_accuracy: 0.9158\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0624 - accuracy: 0.9776 - val_loss: 0.1118 - val_accuracy: 0.9158\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0666 - accuracy: 0.9693 - val_loss: 0.1198 - val_accuracy: 0.8947\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0887 - accuracy: 0.9599 - val_loss: 0.1138 - val_accuracy: 0.9053\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0760 - accuracy: 0.9705 - val_loss: 0.1096 - val_accuracy: 0.9158\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0573 - accuracy: 0.9788 - val_loss: 0.1169 - val_accuracy: 0.8947\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0681 - accuracy: 0.9729 - val_loss: 0.1149 - val_accuracy: 0.9053\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0690 - accuracy: 0.9741 - val_loss: 0.1149 - val_accuracy: 0.9053\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0558 - accuracy: 0.9788 - val_loss: 0.1152 - val_accuracy: 0.9053\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0792 - accuracy: 0.9599 - val_loss: 0.1117 - val_accuracy: 0.9158\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0623 - accuracy: 0.9752 - val_loss: 0.1149 - val_accuracy: 0.9053\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0708 - accuracy: 0.9729 - val_loss: 0.1166 - val_accuracy: 0.8947\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0546 - accuracy: 0.9835 - val_loss: 0.1174 - val_accuracy: 0.8947\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0819 - accuracy: 0.9646 - val_loss: 0.1114 - val_accuracy: 0.9158\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0636 - accuracy: 0.9741 - val_loss: 0.1118 - val_accuracy: 0.9053\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0682 - accuracy: 0.9752 - val_loss: 0.1208 - val_accuracy: 0.8947\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0624 - accuracy: 0.9776 - val_loss: 0.1141 - val_accuracy: 0.9053\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0697 - accuracy: 0.9717 - val_loss: 0.1138 - val_accuracy: 0.9158\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0731 - accuracy: 0.9693 - val_loss: 0.1168 - val_accuracy: 0.8947\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.0776 - accuracy: 0.9693 - val_loss: 0.1105 - val_accuracy: 0.9158\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0668 - accuracy: 0.9693 - val_loss: 0.1160 - val_accuracy: 0.8947\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0583 - accuracy: 0.9870 - val_loss: 0.1237 - val_accuracy: 0.8947\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0571 - accuracy: 0.9811 - val_loss: 0.1187 - val_accuracy: 0.8947\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0543 - accuracy: 0.9882 - val_loss: 0.1153 - val_accuracy: 0.9053\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0546 - accuracy: 0.9823 - val_loss: 0.1168 - val_accuracy: 0.8947\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0669 - accuracy: 0.9741 - val_loss: 0.1145 - val_accuracy: 0.9053\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0902 - accuracy: 0.9552 - val_loss: 0.1132 - val_accuracy: 0.9053\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0644 - accuracy: 0.9752 - val_loss: 0.1114 - val_accuracy: 0.9158\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0648 - accuracy: 0.9752 - val_loss: 0.1141 - val_accuracy: 0.9053\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0594 - accuracy: 0.9811 - val_loss: 0.1163 - val_accuracy: 0.8947\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0694 - accuracy: 0.9729 - val_loss: 0.1162 - val_accuracy: 0.8947\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0648 - accuracy: 0.9752 - val_loss: 0.1124 - val_accuracy: 0.9053\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0599 - accuracy: 0.9788 - val_loss: 0.1174 - val_accuracy: 0.8947\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0742 - accuracy: 0.9646 - val_loss: 0.1137 - val_accuracy: 0.9053\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.1141 - val_accuracy: 0.9053\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0674 - accuracy: 0.9729 - val_loss: 0.1175 - val_accuracy: 0.8947\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0695 - accuracy: 0.9717 - val_loss: 0.1167 - val_accuracy: 0.8947\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0717 - accuracy: 0.9705 - val_loss: 0.1192 - val_accuracy: 0.8947\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0736 - accuracy: 0.9682 - val_loss: 0.1189 - val_accuracy: 0.8947\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0740 - accuracy: 0.9682 - val_loss: 0.1137 - val_accuracy: 0.9053\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0535 - accuracy: 0.9835 - val_loss: 0.1160 - val_accuracy: 0.8947\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0736 - accuracy: 0.9705 - val_loss: 0.1123 - val_accuracy: 0.9158\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0732 - accuracy: 0.9705 - val_loss: 0.1242 - val_accuracy: 0.8947\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0712 - accuracy: 0.9705 - val_loss: 0.1137 - val_accuracy: 0.9053\n",
      "Score for fold 3: loss of 0.11370990425348282; accuracy of 90.52631855010986%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 32s 125ms/step - loss: 0.4005 - accuracy: 0.7303 - val_loss: 0.8296 - val_accuracy: 0.6596\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3239 - accuracy: 0.8068 - val_loss: 1.8075 - val_accuracy: 0.6702\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2612 - accuracy: 0.8433 - val_loss: 0.2325 - val_accuracy: 0.8085\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2511 - accuracy: 0.8563 - val_loss: 0.2296 - val_accuracy: 0.8191\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2089 - accuracy: 0.8740 - val_loss: 0.1581 - val_accuracy: 0.8830\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2413 - accuracy: 0.8787 - val_loss: 0.5890 - val_accuracy: 0.7234\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2030 - accuracy: 0.8928 - val_loss: 0.1095 - val_accuracy: 0.9468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1830 - accuracy: 0.9069 - val_loss: 0.1196 - val_accuracy: 0.9468\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1705 - accuracy: 0.9128 - val_loss: 0.5455 - val_accuracy: 0.7872\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1673 - accuracy: 0.9022 - val_loss: 0.0634 - val_accuracy: 0.9787\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1683 - accuracy: 0.9117 - val_loss: 0.0895 - val_accuracy: 0.9362\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1299 - accuracy: 0.9388 - val_loss: 0.1075 - val_accuracy: 0.9362\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1518 - accuracy: 0.9293 - val_loss: 0.1071 - val_accuracy: 0.9255\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1199 - accuracy: 0.9352 - val_loss: 0.2452 - val_accuracy: 0.8298\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1030 - accuracy: 0.9494 - val_loss: 0.1361 - val_accuracy: 0.9362\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1312 - accuracy: 0.9340 - val_loss: 0.1634 - val_accuracy: 0.8723\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0959 - accuracy: 0.9600 - val_loss: 0.1073 - val_accuracy: 0.9468\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1066 - accuracy: 0.9541 - val_loss: 0.1873 - val_accuracy: 0.8723\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1267 - accuracy: 0.9411 - val_loss: 0.1809 - val_accuracy: 0.8830\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1042 - accuracy: 0.9470 - val_loss: 0.1744 - val_accuracy: 0.9043\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1131 - accuracy: 0.9446 - val_loss: 0.0797 - val_accuracy: 0.9787\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0937 - accuracy: 0.9588 - val_loss: 0.0832 - val_accuracy: 0.9787\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0982 - accuracy: 0.9564 - val_loss: 0.0840 - val_accuracy: 0.9787\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1090 - accuracy: 0.9505 - val_loss: 0.0863 - val_accuracy: 0.9681\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0663 - accuracy: 0.9706 - val_loss: 0.0883 - val_accuracy: 0.9787\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0815 - accuracy: 0.9658 - val_loss: 0.0819 - val_accuracy: 0.9681\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0901 - accuracy: 0.9611 - val_loss: 0.0872 - val_accuracy: 0.9574\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0816 - accuracy: 0.9623 - val_loss: 0.0878 - val_accuracy: 0.9681\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0874 - accuracy: 0.9623 - val_loss: 0.0948 - val_accuracy: 0.9574\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0884 - accuracy: 0.9729 - val_loss: 0.1033 - val_accuracy: 0.9362\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1014 - accuracy: 0.9529 - val_loss: 0.1002 - val_accuracy: 0.9574\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0602 - accuracy: 0.9800 - val_loss: 0.0951 - val_accuracy: 0.9574\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0745 - accuracy: 0.9717 - val_loss: 0.0948 - val_accuracy: 0.9468\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0629 - accuracy: 0.9800 - val_loss: 0.0914 - val_accuracy: 0.9681\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0639 - accuracy: 0.9741 - val_loss: 0.0939 - val_accuracy: 0.9681\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0733 - accuracy: 0.9706 - val_loss: 0.0880 - val_accuracy: 0.9681\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1064 - accuracy: 0.9541 - val_loss: 0.0914 - val_accuracy: 0.9681\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0745 - accuracy: 0.9717 - val_loss: 0.0938 - val_accuracy: 0.9681\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0972 - accuracy: 0.9541 - val_loss: 0.0976 - val_accuracy: 0.9574\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0864 - accuracy: 0.9588 - val_loss: 0.0967 - val_accuracy: 0.9362\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0760 - accuracy: 0.9670 - val_loss: 0.0911 - val_accuracy: 0.9574\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0646 - accuracy: 0.9694 - val_loss: 0.0921 - val_accuracy: 0.9681\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0376 - accuracy: 0.9929 - val_loss: 0.0917 - val_accuracy: 0.9681\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1089 - accuracy: 0.9517 - val_loss: 0.0906 - val_accuracy: 0.9574\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0903 - accuracy: 0.9611 - val_loss: 0.0904 - val_accuracy: 0.9574\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0772 - accuracy: 0.9682 - val_loss: 0.0908 - val_accuracy: 0.9681\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0795 - accuracy: 0.9600 - val_loss: 0.0883 - val_accuracy: 0.9681\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0871 - accuracy: 0.9588 - val_loss: 0.0925 - val_accuracy: 0.9681\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0634 - accuracy: 0.9753 - val_loss: 0.0929 - val_accuracy: 0.9681\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0590 - accuracy: 0.9788 - val_loss: 0.0912 - val_accuracy: 0.9574\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0611 - accuracy: 0.9800 - val_loss: 0.0912 - val_accuracy: 0.9574\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0725 - accuracy: 0.9682 - val_loss: 0.0912 - val_accuracy: 0.9681\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0858 - accuracy: 0.9658 - val_loss: 0.0925 - val_accuracy: 0.9681\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0565 - accuracy: 0.9859 - val_loss: 0.0924 - val_accuracy: 0.9681\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0907 - accuracy: 0.9552 - val_loss: 0.0923 - val_accuracy: 0.9574\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0813 - accuracy: 0.9682 - val_loss: 0.0889 - val_accuracy: 0.9574\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0775 - accuracy: 0.9694 - val_loss: 0.0899 - val_accuracy: 0.9681\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0636 - accuracy: 0.9753 - val_loss: 0.0904 - val_accuracy: 0.9681\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0722 - accuracy: 0.9741 - val_loss: 0.0896 - val_accuracy: 0.9574\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0705 - accuracy: 0.9694 - val_loss: 0.0921 - val_accuracy: 0.9574\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0672 - accuracy: 0.9741 - val_loss: 0.0922 - val_accuracy: 0.9681\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0780 - accuracy: 0.9635 - val_loss: 0.0893 - val_accuracy: 0.9681\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0682 - accuracy: 0.9753 - val_loss: 0.0914 - val_accuracy: 0.9681\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0482 - accuracy: 0.9894 - val_loss: 0.0915 - val_accuracy: 0.9681\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0824 - accuracy: 0.9611 - val_loss: 0.0894 - val_accuracy: 0.9574\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0895 - accuracy: 0.9682 - val_loss: 0.0916 - val_accuracy: 0.9681\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0401 - accuracy: 0.9929 - val_loss: 0.0948 - val_accuracy: 0.9681\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0807 - accuracy: 0.9706 - val_loss: 0.0916 - val_accuracy: 0.9681\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0484 - accuracy: 0.9882 - val_loss: 0.0955 - val_accuracy: 0.9681\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0772 - accuracy: 0.9753 - val_loss: 0.0913 - val_accuracy: 0.9681\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0903 - accuracy: 0.9541 - val_loss: 0.0910 - val_accuracy: 0.9681\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0661 - accuracy: 0.9682 - val_loss: 0.0896 - val_accuracy: 0.9681\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0586 - accuracy: 0.9835 - val_loss: 0.0917 - val_accuracy: 0.9681\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0828 - accuracy: 0.9670 - val_loss: 0.0883 - val_accuracy: 0.9681\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0867 - accuracy: 0.9682 - val_loss: 0.0944 - val_accuracy: 0.9681\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0739 - accuracy: 0.9694 - val_loss: 0.0913 - val_accuracy: 0.9681\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0587 - accuracy: 0.9812 - val_loss: 0.0918 - val_accuracy: 0.9681\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0598 - accuracy: 0.9788 - val_loss: 0.0913 - val_accuracy: 0.9681\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0660 - accuracy: 0.9788 - val_loss: 0.0915 - val_accuracy: 0.9681\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1111 - accuracy: 0.9411 - val_loss: 0.0912 - val_accuracy: 0.9681\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0526 - accuracy: 0.9800 - val_loss: 0.0943 - val_accuracy: 0.9681\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0876 - accuracy: 0.9647 - val_loss: 0.0904 - val_accuracy: 0.9681\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0735 - accuracy: 0.9741 - val_loss: 0.0896 - val_accuracy: 0.9681\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0774 - accuracy: 0.9635 - val_loss: 0.0924 - val_accuracy: 0.9681\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0497 - accuracy: 0.9847 - val_loss: 0.0930 - val_accuracy: 0.9681\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0586 - accuracy: 0.9823 - val_loss: 0.0903 - val_accuracy: 0.9574\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0753 - accuracy: 0.9670 - val_loss: 0.0878 - val_accuracy: 0.9574\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0873 - accuracy: 0.9647 - val_loss: 0.0890 - val_accuracy: 0.9681\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0775 - accuracy: 0.9682 - val_loss: 0.0890 - val_accuracy: 0.9681\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0614 - accuracy: 0.9776 - val_loss: 0.0927 - val_accuracy: 0.9681\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0699 - accuracy: 0.9670 - val_loss: 0.0897 - val_accuracy: 0.9681\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0709 - accuracy: 0.9706 - val_loss: 0.0892 - val_accuracy: 0.9574\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0718 - accuracy: 0.9741 - val_loss: 0.0896 - val_accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0530 - accuracy: 0.9847 - val_loss: 0.0926 - val_accuracy: 0.9681\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0725 - accuracy: 0.9741 - val_loss: 0.0887 - val_accuracy: 0.9681\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0660 - accuracy: 0.9823 - val_loss: 0.0928 - val_accuracy: 0.9681\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0941 - accuracy: 0.9576 - val_loss: 0.0913 - val_accuracy: 0.9681\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1119 - accuracy: 0.9446 - val_loss: 0.0886 - val_accuracy: 0.9574\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.0907 - val_accuracy: 0.9681\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0791 - accuracy: 0.9647 - val_loss: 0.0908 - val_accuracy: 0.9681\n",
      "Score for fold 4: loss of 0.09078632295131683; accuracy of 96.80851101875305%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 104ms/step - loss: 0.3581 - accuracy: 0.7585 - val_loss: 2.7807 - val_accuracy: 0.5319\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3367 - accuracy: 0.8080 - val_loss: 0.4870 - val_accuracy: 0.7447\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2619 - accuracy: 0.8457 - val_loss: 0.1891 - val_accuracy: 0.9149\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2432 - accuracy: 0.8645 - val_loss: 0.6104 - val_accuracy: 0.7021\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2289 - accuracy: 0.8693 - val_loss: 0.2173 - val_accuracy: 0.8298\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2376 - accuracy: 0.8575 - val_loss: 0.1453 - val_accuracy: 0.8830\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2170 - accuracy: 0.8763 - val_loss: 0.4835 - val_accuracy: 0.7660\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1608 - accuracy: 0.9128 - val_loss: 0.1179 - val_accuracy: 0.9149\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1579 - accuracy: 0.9152 - val_loss: 0.5947 - val_accuracy: 0.7447\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1777 - accuracy: 0.9034 - val_loss: 0.3460 - val_accuracy: 0.7340\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1780 - accuracy: 0.9034 - val_loss: 0.3632 - val_accuracy: 0.7872\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1579 - accuracy: 0.9152 - val_loss: 0.1643 - val_accuracy: 0.9149\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1711 - accuracy: 0.8987 - val_loss: 0.1392 - val_accuracy: 0.8936\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1228 - accuracy: 0.9329 - val_loss: 0.0961 - val_accuracy: 0.9681\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1505 - accuracy: 0.9246 - val_loss: 0.1226 - val_accuracy: 0.9149\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1105 - accuracy: 0.9352 - val_loss: 0.3635 - val_accuracy: 0.8191\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1393 - accuracy: 0.9199 - val_loss: 0.1388 - val_accuracy: 0.9362\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1385 - accuracy: 0.9199 - val_loss: 0.1267 - val_accuracy: 0.9468\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1275 - accuracy: 0.9293 - val_loss: 0.2450 - val_accuracy: 0.8404\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1291 - accuracy: 0.9270 - val_loss: 0.1699 - val_accuracy: 0.8511\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0830 - accuracy: 0.9588 - val_loss: 0.1221 - val_accuracy: 0.9362\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0751 - accuracy: 0.9717 - val_loss: 0.1060 - val_accuracy: 0.9362\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0772 - accuracy: 0.9694 - val_loss: 0.0975 - val_accuracy: 0.9255\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0962 - accuracy: 0.9494 - val_loss: 0.1010 - val_accuracy: 0.9362\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0943 - accuracy: 0.9541 - val_loss: 0.1156 - val_accuracy: 0.9362\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0734 - accuracy: 0.9717 - val_loss: 0.1096 - val_accuracy: 0.9255\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0816 - accuracy: 0.9611 - val_loss: 0.1052 - val_accuracy: 0.9362\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0679 - accuracy: 0.9717 - val_loss: 0.0991 - val_accuracy: 0.9362\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0987 - accuracy: 0.9494 - val_loss: 0.1063 - val_accuracy: 0.9362\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0749 - accuracy: 0.9658 - val_loss: 0.1151 - val_accuracy: 0.9362\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0841 - accuracy: 0.9635 - val_loss: 0.1117 - val_accuracy: 0.9362\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0767 - accuracy: 0.9670 - val_loss: 0.1081 - val_accuracy: 0.9362\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0802 - accuracy: 0.9623 - val_loss: 0.1076 - val_accuracy: 0.9362\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0726 - accuracy: 0.9729 - val_loss: 0.1083 - val_accuracy: 0.9362\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0635 - accuracy: 0.9776 - val_loss: 0.1067 - val_accuracy: 0.9362\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0743 - accuracy: 0.9658 - val_loss: 0.1042 - val_accuracy: 0.9362\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0680 - accuracy: 0.9647 - val_loss: 0.1168 - val_accuracy: 0.9362\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0506 - accuracy: 0.9835 - val_loss: 0.1127 - val_accuracy: 0.9255\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0492 - accuracy: 0.9847 - val_loss: 0.1110 - val_accuracy: 0.9362\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0629 - accuracy: 0.9812 - val_loss: 0.1094 - val_accuracy: 0.9362\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0554 - accuracy: 0.9788 - val_loss: 0.1083 - val_accuracy: 0.9362\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0738 - accuracy: 0.9600 - val_loss: 0.1096 - val_accuracy: 0.9255\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0836 - accuracy: 0.9576 - val_loss: 0.1106 - val_accuracy: 0.9362\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0824 - accuracy: 0.9552 - val_loss: 0.1061 - val_accuracy: 0.9362\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0673 - accuracy: 0.9753 - val_loss: 0.1049 - val_accuracy: 0.9362\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0583 - accuracy: 0.9776 - val_loss: 0.1080 - val_accuracy: 0.9362\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0716 - accuracy: 0.9694 - val_loss: 0.1035 - val_accuracy: 0.9362\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0804 - accuracy: 0.9600 - val_loss: 0.1077 - val_accuracy: 0.9362\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0694 - accuracy: 0.9694 - val_loss: 0.1105 - val_accuracy: 0.9255\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0900 - accuracy: 0.9470 - val_loss: 0.1099 - val_accuracy: 0.9362\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0605 - accuracy: 0.9741 - val_loss: 0.1049 - val_accuracy: 0.9362\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0865 - accuracy: 0.9623 - val_loss: 0.1065 - val_accuracy: 0.9362\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0835 - accuracy: 0.9647 - val_loss: 0.1061 - val_accuracy: 0.9362\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0747 - accuracy: 0.9635 - val_loss: 0.1071 - val_accuracy: 0.9362\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0558 - accuracy: 0.9753 - val_loss: 0.1078 - val_accuracy: 0.9362\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0619 - accuracy: 0.9729 - val_loss: 0.1121 - val_accuracy: 0.9255\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0600 - accuracy: 0.9788 - val_loss: 0.1080 - val_accuracy: 0.9362\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0554 - accuracy: 0.9812 - val_loss: 0.1117 - val_accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0603 - accuracy: 0.9717 - val_loss: 0.1122 - val_accuracy: 0.9362\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0586 - accuracy: 0.9776 - val_loss: 0.1092 - val_accuracy: 0.9362\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0722 - accuracy: 0.9741 - val_loss: 0.1081 - val_accuracy: 0.9362\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0554 - accuracy: 0.9753 - val_loss: 0.1110 - val_accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0707 - accuracy: 0.9694 - val_loss: 0.1129 - val_accuracy: 0.9255\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0639 - accuracy: 0.9729 - val_loss: 0.1063 - val_accuracy: 0.9362\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0717 - accuracy: 0.9647 - val_loss: 0.1075 - val_accuracy: 0.9362\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0471 - accuracy: 0.9870 - val_loss: 0.1101 - val_accuracy: 0.9255\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0777 - accuracy: 0.9623 - val_loss: 0.1077 - val_accuracy: 0.9362\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0659 - accuracy: 0.9741 - val_loss: 0.1073 - val_accuracy: 0.9362\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0492 - accuracy: 0.9882 - val_loss: 0.1108 - val_accuracy: 0.9255\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0551 - accuracy: 0.9800 - val_loss: 0.1105 - val_accuracy: 0.9255\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0655 - accuracy: 0.9741 - val_loss: 0.1115 - val_accuracy: 0.9362\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0549 - accuracy: 0.9835 - val_loss: 0.1127 - val_accuracy: 0.9255\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0773 - accuracy: 0.9694 - val_loss: 0.1116 - val_accuracy: 0.9255\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0675 - accuracy: 0.9682 - val_loss: 0.1070 - val_accuracy: 0.9362\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0801 - accuracy: 0.9623 - val_loss: 0.1073 - val_accuracy: 0.9362\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0675 - accuracy: 0.9694 - val_loss: 0.1076 - val_accuracy: 0.9362\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0610 - accuracy: 0.9717 - val_loss: 0.1089 - val_accuracy: 0.9362\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0651 - accuracy: 0.9694 - val_loss: 0.1043 - val_accuracy: 0.9362\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0715 - accuracy: 0.9670 - val_loss: 0.1070 - val_accuracy: 0.9362\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0634 - accuracy: 0.9741 - val_loss: 0.1094 - val_accuracy: 0.9255\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0639 - accuracy: 0.9729 - val_loss: 0.1090 - val_accuracy: 0.9255\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0536 - accuracy: 0.9823 - val_loss: 0.1101 - val_accuracy: 0.9255\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0632 - accuracy: 0.9753 - val_loss: 0.1056 - val_accuracy: 0.9362\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0756 - accuracy: 0.9611 - val_loss: 0.1055 - val_accuracy: 0.9362\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0744 - accuracy: 0.9600 - val_loss: 0.1056 - val_accuracy: 0.9362\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0665 - accuracy: 0.9729 - val_loss: 0.1073 - val_accuracy: 0.9362\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0715 - accuracy: 0.9600 - val_loss: 0.1053 - val_accuracy: 0.9362\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0707 - accuracy: 0.9694 - val_loss: 0.1108 - val_accuracy: 0.9255\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0717 - accuracy: 0.9658 - val_loss: 0.1057 - val_accuracy: 0.9362\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0703 - accuracy: 0.9729 - val_loss: 0.1071 - val_accuracy: 0.9362\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0602 - accuracy: 0.9694 - val_loss: 0.1072 - val_accuracy: 0.9362\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0534 - accuracy: 0.9823 - val_loss: 0.1125 - val_accuracy: 0.9255\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0702 - accuracy: 0.9729 - val_loss: 0.1074 - val_accuracy: 0.9362\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0782 - accuracy: 0.9611 - val_loss: 0.1076 - val_accuracy: 0.9362\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0686 - accuracy: 0.9670 - val_loss: 0.1063 - val_accuracy: 0.9362\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0715 - accuracy: 0.9670 - val_loss: 0.1061 - val_accuracy: 0.9362\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0759 - accuracy: 0.9588 - val_loss: 0.1080 - val_accuracy: 0.9362\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0634 - accuracy: 0.9753 - val_loss: 0.1085 - val_accuracy: 0.9362\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0694 - accuracy: 0.9694 - val_loss: 0.1109 - val_accuracy: 0.9255\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0587 - accuracy: 0.9764 - val_loss: 0.1131 - val_accuracy: 0.9255\n",
      "Score for fold 5: loss of 0.11312119662761688; accuracy of 92.55319237709045%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 105ms/step - loss: 0.3863 - accuracy: 0.7362 - val_loss: 0.8303 - val_accuracy: 0.4362\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3100 - accuracy: 0.8269 - val_loss: 2.3870 - val_accuracy: 0.6489\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2831 - accuracy: 0.8433 - val_loss: 0.3161 - val_accuracy: 0.7447\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2467 - accuracy: 0.8704 - val_loss: 0.1683 - val_accuracy: 0.9255\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2025 - accuracy: 0.8822 - val_loss: 0.6329 - val_accuracy: 0.6915\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1968 - accuracy: 0.8881 - val_loss: 0.1698 - val_accuracy: 0.8936\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1813 - accuracy: 0.9117 - val_loss: 0.1386 - val_accuracy: 0.9468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1776 - accuracy: 0.8893 - val_loss: 0.3175 - val_accuracy: 0.7553\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1461 - accuracy: 0.9258 - val_loss: 0.3072 - val_accuracy: 0.8191\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1900 - accuracy: 0.8975 - val_loss: 0.2102 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1446 - accuracy: 0.9282 - val_loss: 0.2331 - val_accuracy: 0.9255\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1747 - accuracy: 0.9034 - val_loss: 0.3354 - val_accuracy: 0.7553\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1546 - accuracy: 0.9187 - val_loss: 0.3335 - val_accuracy: 0.8298\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1381 - accuracy: 0.9234 - val_loss: 0.2246 - val_accuracy: 0.8511\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1422 - accuracy: 0.9246 - val_loss: 0.1190 - val_accuracy: 0.9362\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1089 - accuracy: 0.9505 - val_loss: 0.1540 - val_accuracy: 0.9149\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1064 - accuracy: 0.9435 - val_loss: 0.2447 - val_accuracy: 0.8404\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0926 - accuracy: 0.9564 - val_loss: 0.2528 - val_accuracy: 0.8511\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1281 - accuracy: 0.9376 - val_loss: 0.2047 - val_accuracy: 0.8830\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1009 - accuracy: 0.9494 - val_loss: 0.3211 - val_accuracy: 0.8191\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1065 - accuracy: 0.9446 - val_loss: 0.1159 - val_accuracy: 0.9149\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0870 - accuracy: 0.9694 - val_loss: 0.1091 - val_accuracy: 0.9574\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0919 - accuracy: 0.9529 - val_loss: 0.1112 - val_accuracy: 0.9468\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0832 - accuracy: 0.9647 - val_loss: 0.1192 - val_accuracy: 0.9149\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0784 - accuracy: 0.9682 - val_loss: 0.1298 - val_accuracy: 0.9149\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0644 - accuracy: 0.9764 - val_loss: 0.1442 - val_accuracy: 0.9043\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0957 - accuracy: 0.9588 - val_loss: 0.1638 - val_accuracy: 0.8830\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1017 - accuracy: 0.9517 - val_loss: 0.1300 - val_accuracy: 0.9149\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0758 - accuracy: 0.9717 - val_loss: 0.1256 - val_accuracy: 0.9149\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0726 - accuracy: 0.9764 - val_loss: 0.1232 - val_accuracy: 0.9149\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0846 - accuracy: 0.9611 - val_loss: 0.1075 - val_accuracy: 0.9362\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0905 - accuracy: 0.9529 - val_loss: 0.1066 - val_accuracy: 0.9149\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0636 - accuracy: 0.9812 - val_loss: 0.1070 - val_accuracy: 0.9255\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0787 - accuracy: 0.9706 - val_loss: 0.1181 - val_accuracy: 0.9255\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0656 - accuracy: 0.9682 - val_loss: 0.1005 - val_accuracy: 0.9255\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0808 - accuracy: 0.9658 - val_loss: 0.1246 - val_accuracy: 0.9043\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0847 - accuracy: 0.9623 - val_loss: 0.1211 - val_accuracy: 0.9043\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0644 - accuracy: 0.9812 - val_loss: 0.1507 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0828 - accuracy: 0.9623 - val_loss: 0.1281 - val_accuracy: 0.8936\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0550 - accuracy: 0.9835 - val_loss: 0.1175 - val_accuracy: 0.9149\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0701 - accuracy: 0.9753 - val_loss: 0.1177 - val_accuracy: 0.9043\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0539 - accuracy: 0.9812 - val_loss: 0.1186 - val_accuracy: 0.8936\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0998 - accuracy: 0.9588 - val_loss: 0.1183 - val_accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0797 - accuracy: 0.9706 - val_loss: 0.1194 - val_accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0962 - accuracy: 0.9482 - val_loss: 0.1164 - val_accuracy: 0.9149\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0603 - accuracy: 0.9823 - val_loss: 0.1178 - val_accuracy: 0.9043\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9729 - val_loss: 0.1255 - val_accuracy: 0.8936\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0874 - accuracy: 0.9682 - val_loss: 0.1222 - val_accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0711 - accuracy: 0.9776 - val_loss: 0.1164 - val_accuracy: 0.9149\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0935 - accuracy: 0.9564 - val_loss: 0.1041 - val_accuracy: 0.9255\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0675 - accuracy: 0.9741 - val_loss: 0.1094 - val_accuracy: 0.9255\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0621 - accuracy: 0.9788 - val_loss: 0.1102 - val_accuracy: 0.9255\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0777 - accuracy: 0.9753 - val_loss: 0.1191 - val_accuracy: 0.9043\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0775 - accuracy: 0.9717 - val_loss: 0.1219 - val_accuracy: 0.9043\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0764 - accuracy: 0.9647 - val_loss: 0.1168 - val_accuracy: 0.9043\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0706 - accuracy: 0.9682 - val_loss: 0.1178 - val_accuracy: 0.9149\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1101 - accuracy: 0.9470 - val_loss: 0.1153 - val_accuracy: 0.9149\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0877 - accuracy: 0.9600 - val_loss: 0.1140 - val_accuracy: 0.9149\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0914 - accuracy: 0.9517 - val_loss: 0.1192 - val_accuracy: 0.9043\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0787 - accuracy: 0.9706 - val_loss: 0.1252 - val_accuracy: 0.9043\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0675 - accuracy: 0.9812 - val_loss: 0.1204 - val_accuracy: 0.9043\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0766 - accuracy: 0.9682 - val_loss: 0.1297 - val_accuracy: 0.9043\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0796 - accuracy: 0.9635 - val_loss: 0.1060 - val_accuracy: 0.9255\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0792 - accuracy: 0.9658 - val_loss: 0.1197 - val_accuracy: 0.9149\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0838 - accuracy: 0.9647 - val_loss: 0.1229 - val_accuracy: 0.9149\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0687 - accuracy: 0.9706 - val_loss: 0.1178 - val_accuracy: 0.9149\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.1148 - val_accuracy: 0.9149\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0796 - accuracy: 0.9658 - val_loss: 0.1227 - val_accuracy: 0.9149\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0762 - accuracy: 0.9647 - val_loss: 0.1173 - val_accuracy: 0.9149\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0977 - accuracy: 0.9552 - val_loss: 0.1241 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0547 - accuracy: 0.9812 - val_loss: 0.1253 - val_accuracy: 0.9043\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0561 - accuracy: 0.9812 - val_loss: 0.1206 - val_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0872 - accuracy: 0.9564 - val_loss: 0.1281 - val_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0795 - accuracy: 0.9611 - val_loss: 0.1086 - val_accuracy: 0.9255\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0791 - accuracy: 0.9635 - val_loss: 0.1129 - val_accuracy: 0.9149\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0478 - accuracy: 0.9835 - val_loss: 0.1157 - val_accuracy: 0.9149\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0980 - accuracy: 0.9505 - val_loss: 0.1144 - val_accuracy: 0.9149\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0578 - accuracy: 0.9823 - val_loss: 0.1229 - val_accuracy: 0.9043\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0754 - accuracy: 0.9706 - val_loss: 0.1219 - val_accuracy: 0.9043\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9776 - val_loss: 0.1312 - val_accuracy: 0.9043\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0616 - accuracy: 0.9800 - val_loss: 0.1139 - val_accuracy: 0.9149\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0737 - accuracy: 0.9694 - val_loss: 0.1108 - val_accuracy: 0.9149\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0618 - accuracy: 0.9764 - val_loss: 0.1141 - val_accuracy: 0.9149\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0707 - accuracy: 0.9717 - val_loss: 0.1134 - val_accuracy: 0.9149\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0860 - accuracy: 0.9623 - val_loss: 0.1061 - val_accuracy: 0.9255\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0521 - accuracy: 0.9812 - val_loss: 0.1144 - val_accuracy: 0.9149\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0732 - accuracy: 0.9729 - val_loss: 0.1079 - val_accuracy: 0.9255\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0627 - accuracy: 0.9764 - val_loss: 0.1093 - val_accuracy: 0.9255\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0629 - accuracy: 0.9800 - val_loss: 0.1099 - val_accuracy: 0.9255\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0632 - accuracy: 0.9764 - val_loss: 0.1186 - val_accuracy: 0.9043\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0724 - accuracy: 0.9788 - val_loss: 0.1271 - val_accuracy: 0.9043\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0820 - accuracy: 0.9600 - val_loss: 0.1137 - val_accuracy: 0.9149\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0666 - accuracy: 0.9729 - val_loss: 0.1079 - val_accuracy: 0.9255\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0889 - accuracy: 0.9635 - val_loss: 0.1094 - val_accuracy: 0.9255\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0517 - accuracy: 0.9788 - val_loss: 0.1155 - val_accuracy: 0.9149\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0923 - accuracy: 0.9529 - val_loss: 0.1117 - val_accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0576 - accuracy: 0.9847 - val_loss: 0.1158 - val_accuracy: 0.9149\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0608 - accuracy: 0.9788 - val_loss: 0.1182 - val_accuracy: 0.9149\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0667 - accuracy: 0.9753 - val_loss: 0.1210 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0837 - accuracy: 0.9635 - val_loss: 0.1088 - val_accuracy: 0.9255\n",
      "Score for fold 6: loss of 0.10883263498544693; accuracy of 92.55319237709045%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 28s 102ms/step - loss: 0.4330 - accuracy: 0.7279 - val_loss: 1.8674 - val_accuracy: 0.6702\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3184 - accuracy: 0.8021 - val_loss: 0.3845 - val_accuracy: 0.7872\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2641 - accuracy: 0.8457 - val_loss: 0.1430 - val_accuracy: 0.9149\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2253 - accuracy: 0.8610 - val_loss: 0.2458 - val_accuracy: 0.8191\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2132 - accuracy: 0.8763 - val_loss: 0.3394 - val_accuracy: 0.6915\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2128 - accuracy: 0.8857 - val_loss: 0.1849 - val_accuracy: 0.8617\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1892 - accuracy: 0.8916 - val_loss: 1.1512 - val_accuracy: 0.6809\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1616 - accuracy: 0.9105 - val_loss: 0.3657 - val_accuracy: 0.7553\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1879 - accuracy: 0.8775 - val_loss: 0.5334 - val_accuracy: 0.7128\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1454 - accuracy: 0.9140 - val_loss: 4.2994 - val_accuracy: 0.6596\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1793 - accuracy: 0.8999 - val_loss: 0.4270 - val_accuracy: 0.7021\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1784 - accuracy: 0.9022 - val_loss: 0.2050 - val_accuracy: 0.8298\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1610 - accuracy: 0.9058 - val_loss: 0.3385 - val_accuracy: 0.7553\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1554 - accuracy: 0.9128 - val_loss: 0.2315 - val_accuracy: 0.8298\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1328 - accuracy: 0.9258 - val_loss: 0.0808 - val_accuracy: 0.9574\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1334 - accuracy: 0.9270 - val_loss: 0.1663 - val_accuracy: 0.8830\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1099 - accuracy: 0.9376 - val_loss: 0.1166 - val_accuracy: 0.9149\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1252 - accuracy: 0.9329 - val_loss: 0.4083 - val_accuracy: 0.7660\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1188 - accuracy: 0.9305 - val_loss: 0.8006 - val_accuracy: 0.7766\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1112 - accuracy: 0.9458 - val_loss: 0.4091 - val_accuracy: 0.7979\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0972 - accuracy: 0.9529 - val_loss: 0.1617 - val_accuracy: 0.8936\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0801 - accuracy: 0.9658 - val_loss: 0.1027 - val_accuracy: 0.9255\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0926 - accuracy: 0.9529 - val_loss: 0.1388 - val_accuracy: 0.9255\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0669 - accuracy: 0.9706 - val_loss: 0.1082 - val_accuracy: 0.9149\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0919 - accuracy: 0.9541 - val_loss: 0.1041 - val_accuracy: 0.9255\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0630 - accuracy: 0.9717 - val_loss: 0.1107 - val_accuracy: 0.9255\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0661 - accuracy: 0.9706 - val_loss: 0.0970 - val_accuracy: 0.9255\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0850 - accuracy: 0.9600 - val_loss: 0.1425 - val_accuracy: 0.9362\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0645 - accuracy: 0.9717 - val_loss: 0.1387 - val_accuracy: 0.9362\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0674 - accuracy: 0.9670 - val_loss: 0.1137 - val_accuracy: 0.9362\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0870 - accuracy: 0.9635 - val_loss: 0.1253 - val_accuracy: 0.9255\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0636 - accuracy: 0.9788 - val_loss: 0.1173 - val_accuracy: 0.9255\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0613 - accuracy: 0.9741 - val_loss: 0.1215 - val_accuracy: 0.9255\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0709 - accuracy: 0.9694 - val_loss: 0.1075 - val_accuracy: 0.9255\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0571 - accuracy: 0.9741 - val_loss: 0.1044 - val_accuracy: 0.9362\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0591 - accuracy: 0.9800 - val_loss: 0.1242 - val_accuracy: 0.9255\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0632 - accuracy: 0.9717 - val_loss: 0.1319 - val_accuracy: 0.9255\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0724 - accuracy: 0.9694 - val_loss: 0.1445 - val_accuracy: 0.9255\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0918 - accuracy: 0.9505 - val_loss: 0.1136 - val_accuracy: 0.9362\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0514 - accuracy: 0.9847 - val_loss: 0.1616 - val_accuracy: 0.9149\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0765 - accuracy: 0.9635 - val_loss: 0.1257 - val_accuracy: 0.9255\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0775 - accuracy: 0.9623 - val_loss: 0.1451 - val_accuracy: 0.9255\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0787 - accuracy: 0.9658 - val_loss: 0.1372 - val_accuracy: 0.9255\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0437 - accuracy: 0.9882 - val_loss: 0.1517 - val_accuracy: 0.9255\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0679 - accuracy: 0.9706 - val_loss: 0.1292 - val_accuracy: 0.9149\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0639 - accuracy: 0.9776 - val_loss: 0.1399 - val_accuracy: 0.9255\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0520 - accuracy: 0.9800 - val_loss: 0.1437 - val_accuracy: 0.9255\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0531 - accuracy: 0.9823 - val_loss: 0.1324 - val_accuracy: 0.9255\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0511 - accuracy: 0.9835 - val_loss: 0.1305 - val_accuracy: 0.9255\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 22s 101ms/step - loss: 0.0703 - accuracy: 0.9694 - val_loss: 0.1294 - val_accuracy: 0.9255\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0741 - accuracy: 0.9670 - val_loss: 0.1455 - val_accuracy: 0.9255\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0598 - accuracy: 0.9753 - val_loss: 0.1267 - val_accuracy: 0.9255\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0674 - accuracy: 0.9764 - val_loss: 0.1252 - val_accuracy: 0.9149\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0707 - accuracy: 0.9741 - val_loss: 0.1331 - val_accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0687 - accuracy: 0.9623 - val_loss: 0.1337 - val_accuracy: 0.9255\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0678 - accuracy: 0.9635 - val_loss: 0.1217 - val_accuracy: 0.9149\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0612 - accuracy: 0.9741 - val_loss: 0.1457 - val_accuracy: 0.9255\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0710 - accuracy: 0.9647 - val_loss: 0.1351 - val_accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0465 - accuracy: 0.9788 - val_loss: 0.1352 - val_accuracy: 0.9255\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0560 - accuracy: 0.9823 - val_loss: 0.1510 - val_accuracy: 0.9255\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0582 - accuracy: 0.9812 - val_loss: 0.1389 - val_accuracy: 0.9255\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0784 - accuracy: 0.9694 - val_loss: 0.1175 - val_accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0839 - accuracy: 0.9658 - val_loss: 0.1295 - val_accuracy: 0.9255\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0798 - accuracy: 0.9611 - val_loss: 0.1388 - val_accuracy: 0.9255\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0581 - accuracy: 0.9800 - val_loss: 0.1443 - val_accuracy: 0.9255\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0582 - accuracy: 0.9800 - val_loss: 0.1290 - val_accuracy: 0.9255\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0553 - accuracy: 0.9729 - val_loss: 0.1367 - val_accuracy: 0.9255\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.1447 - val_accuracy: 0.9255\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0548 - accuracy: 0.9788 - val_loss: 0.1478 - val_accuracy: 0.9255\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0562 - accuracy: 0.9788 - val_loss: 0.1248 - val_accuracy: 0.9255\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0827 - accuracy: 0.9682 - val_loss: 0.1268 - val_accuracy: 0.9255\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0804 - accuracy: 0.9635 - val_loss: 0.1127 - val_accuracy: 0.9255\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0619 - accuracy: 0.9764 - val_loss: 0.1205 - val_accuracy: 0.9255\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0803 - accuracy: 0.9588 - val_loss: 0.1195 - val_accuracy: 0.9255\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0518 - accuracy: 0.9812 - val_loss: 0.1449 - val_accuracy: 0.9255\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0558 - accuracy: 0.9788 - val_loss: 0.1367 - val_accuracy: 0.9255\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0591 - accuracy: 0.9788 - val_loss: 0.1359 - val_accuracy: 0.9255\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0689 - accuracy: 0.9694 - val_loss: 0.1263 - val_accuracy: 0.9255\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0641 - accuracy: 0.9682 - val_loss: 0.1366 - val_accuracy: 0.9255\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0796 - accuracy: 0.9611 - val_loss: 0.1278 - val_accuracy: 0.9255\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0847 - accuracy: 0.9552 - val_loss: 0.1300 - val_accuracy: 0.9255\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0710 - accuracy: 0.9635 - val_loss: 0.1351 - val_accuracy: 0.9255\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0774 - accuracy: 0.9623 - val_loss: 0.1199 - val_accuracy: 0.9255\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0690 - accuracy: 0.9682 - val_loss: 0.1464 - val_accuracy: 0.9255\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0732 - accuracy: 0.9706 - val_loss: 0.1264 - val_accuracy: 0.9149\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0531 - accuracy: 0.9823 - val_loss: 0.1250 - val_accuracy: 0.9149\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0628 - accuracy: 0.9741 - val_loss: 0.1338 - val_accuracy: 0.9255\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0588 - accuracy: 0.9764 - val_loss: 0.1196 - val_accuracy: 0.9255\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0672 - accuracy: 0.9682 - val_loss: 0.1388 - val_accuracy: 0.9255\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0726 - accuracy: 0.9682 - val_loss: 0.1268 - val_accuracy: 0.9149\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0665 - accuracy: 0.9706 - val_loss: 0.1385 - val_accuracy: 0.9255\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0800 - accuracy: 0.9658 - val_loss: 0.1209 - val_accuracy: 0.9255\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0556 - accuracy: 0.9835 - val_loss: 0.1523 - val_accuracy: 0.9149\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0824 - accuracy: 0.9576 - val_loss: 0.1197 - val_accuracy: 0.9255\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9729 - val_loss: 0.1303 - val_accuracy: 0.9255\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0611 - accuracy: 0.9800 - val_loss: 0.1415 - val_accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0670 - accuracy: 0.9694 - val_loss: 0.1307 - val_accuracy: 0.9255\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0564 - accuracy: 0.9800 - val_loss: 0.1191 - val_accuracy: 0.9255\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0858 - accuracy: 0.9494 - val_loss: 0.1302 - val_accuracy: 0.9255\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0628 - accuracy: 0.9729 - val_loss: 0.1294 - val_accuracy: 0.9149\n",
      "Score for fold 7: loss of 0.1294270157814026; accuracy of 91.4893627166748%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 102ms/step - loss: 0.3689 - accuracy: 0.7762 - val_loss: 0.6244 - val_accuracy: 0.7128\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3141 - accuracy: 0.8092 - val_loss: 0.8539 - val_accuracy: 0.9255\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3083 - accuracy: 0.8233 - val_loss: 4.0519 - val_accuracy: 0.7128\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2570 - accuracy: 0.8481 - val_loss: 0.8468 - val_accuracy: 0.7021\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2230 - accuracy: 0.8716 - val_loss: 0.1148 - val_accuracy: 0.9149\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2157 - accuracy: 0.8645 - val_loss: 0.2293 - val_accuracy: 0.9362\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1872 - accuracy: 0.8810 - val_loss: 0.1214 - val_accuracy: 0.9468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1804 - accuracy: 0.9034 - val_loss: 0.7617 - val_accuracy: 0.7128\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1659 - accuracy: 0.9069 - val_loss: 0.8150 - val_accuracy: 0.7128\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1886 - accuracy: 0.9011 - val_loss: 0.1677 - val_accuracy: 0.9043\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1756 - accuracy: 0.9011 - val_loss: 0.1710 - val_accuracy: 0.8830\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1563 - accuracy: 0.9058 - val_loss: 0.0925 - val_accuracy: 0.9362\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1558 - accuracy: 0.9128 - val_loss: 0.1934 - val_accuracy: 0.8617\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1195 - accuracy: 0.9376 - val_loss: 0.1767 - val_accuracy: 0.9362\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1417 - accuracy: 0.9293 - val_loss: 0.1427 - val_accuracy: 0.9574\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1210 - accuracy: 0.9376 - val_loss: 0.0828 - val_accuracy: 0.9468\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1556 - accuracy: 0.9176 - val_loss: 0.2458 - val_accuracy: 0.8298\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1060 - accuracy: 0.9552 - val_loss: 0.2759 - val_accuracy: 0.7553\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1278 - accuracy: 0.9317 - val_loss: 0.0827 - val_accuracy: 0.9574\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1072 - accuracy: 0.9423 - val_loss: 0.5597 - val_accuracy: 0.6809\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0960 - accuracy: 0.9541 - val_loss: 0.1009 - val_accuracy: 0.9681\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0743 - accuracy: 0.9670 - val_loss: 0.1253 - val_accuracy: 0.9468\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0762 - accuracy: 0.9694 - val_loss: 0.1051 - val_accuracy: 0.9681\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9670 - val_loss: 0.0951 - val_accuracy: 0.9681\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0689 - accuracy: 0.9729 - val_loss: 0.0967 - val_accuracy: 0.9681\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0476 - accuracy: 0.9835 - val_loss: 0.1254 - val_accuracy: 0.9468\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0703 - accuracy: 0.9741 - val_loss: 0.1067 - val_accuracy: 0.9574\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0815 - accuracy: 0.9647 - val_loss: 0.1120 - val_accuracy: 0.9574\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0752 - accuracy: 0.9729 - val_loss: 0.1053 - val_accuracy: 0.9681\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0607 - accuracy: 0.9788 - val_loss: 0.1022 - val_accuracy: 0.9681\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0639 - accuracy: 0.9694 - val_loss: 0.1014 - val_accuracy: 0.9681\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0691 - accuracy: 0.9717 - val_loss: 0.0961 - val_accuracy: 0.9681\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9682 - val_loss: 0.1003 - val_accuracy: 0.9681\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0620 - accuracy: 0.9753 - val_loss: 0.1095 - val_accuracy: 0.9681\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0588 - accuracy: 0.9812 - val_loss: 0.1141 - val_accuracy: 0.9681\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0757 - accuracy: 0.9623 - val_loss: 0.1043 - val_accuracy: 0.9681\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0645 - accuracy: 0.9694 - val_loss: 0.1105 - val_accuracy: 0.9681\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0956 - accuracy: 0.9470 - val_loss: 0.1098 - val_accuracy: 0.9681\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0622 - accuracy: 0.9753 - val_loss: 0.1019 - val_accuracy: 0.9681\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0604 - accuracy: 0.9800 - val_loss: 0.1094 - val_accuracy: 0.9681\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0670 - accuracy: 0.9611 - val_loss: 0.1175 - val_accuracy: 0.9574\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0661 - accuracy: 0.9717 - val_loss: 0.1092 - val_accuracy: 0.9681\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0626 - accuracy: 0.9706 - val_loss: 0.1042 - val_accuracy: 0.9681\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0702 - accuracy: 0.9694 - val_loss: 0.1078 - val_accuracy: 0.9681\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0741 - accuracy: 0.9670 - val_loss: 0.1138 - val_accuracy: 0.9681\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0747 - accuracy: 0.9741 - val_loss: 0.1102 - val_accuracy: 0.9681\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0560 - accuracy: 0.9800 - val_loss: 0.1088 - val_accuracy: 0.9681\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0670 - accuracy: 0.9694 - val_loss: 0.1135 - val_accuracy: 0.9681\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0505 - accuracy: 0.9847 - val_loss: 0.1032 - val_accuracy: 0.9681\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0539 - accuracy: 0.9788 - val_loss: 0.1145 - val_accuracy: 0.9574\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0647 - accuracy: 0.9694 - val_loss: 0.1168 - val_accuracy: 0.9574\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0591 - accuracy: 0.9753 - val_loss: 0.1068 - val_accuracy: 0.9681\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0651 - accuracy: 0.9694 - val_loss: 0.1058 - val_accuracy: 0.9681\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0745 - accuracy: 0.9741 - val_loss: 0.1138 - val_accuracy: 0.9574\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0575 - accuracy: 0.9753 - val_loss: 0.1143 - val_accuracy: 0.9574\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0743 - accuracy: 0.9588 - val_loss: 0.1174 - val_accuracy: 0.9574\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0760 - accuracy: 0.9706 - val_loss: 0.1122 - val_accuracy: 0.9574\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0612 - accuracy: 0.9694 - val_loss: 0.1056 - val_accuracy: 0.9681\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0541 - accuracy: 0.9753 - val_loss: 0.1082 - val_accuracy: 0.9681\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0530 - accuracy: 0.9776 - val_loss: 0.1102 - val_accuracy: 0.9574\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0606 - accuracy: 0.9776 - val_loss: 0.1159 - val_accuracy: 0.9681\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0687 - accuracy: 0.9729 - val_loss: 0.1046 - val_accuracy: 0.9681\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0453 - accuracy: 0.9870 - val_loss: 0.1099 - val_accuracy: 0.9681\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0555 - accuracy: 0.9764 - val_loss: 0.1090 - val_accuracy: 0.9574\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0599 - accuracy: 0.9753 - val_loss: 0.1116 - val_accuracy: 0.9681\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9658 - val_loss: 0.1155 - val_accuracy: 0.9681\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0667 - accuracy: 0.9706 - val_loss: 0.1126 - val_accuracy: 0.9681\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0590 - accuracy: 0.9741 - val_loss: 0.1030 - val_accuracy: 0.9681\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0718 - accuracy: 0.9694 - val_loss: 0.1034 - val_accuracy: 0.9681\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0643 - accuracy: 0.9682 - val_loss: 0.1051 - val_accuracy: 0.9681\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.1081 - val_accuracy: 0.9681\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0766 - accuracy: 0.9588 - val_loss: 0.1064 - val_accuracy: 0.9681\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0461 - accuracy: 0.9870 - val_loss: 0.1109 - val_accuracy: 0.9681\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0482 - accuracy: 0.9870 - val_loss: 0.1173 - val_accuracy: 0.9468\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0787 - accuracy: 0.9611 - val_loss: 0.1070 - val_accuracy: 0.9681\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0535 - accuracy: 0.9788 - val_loss: 0.1100 - val_accuracy: 0.9681\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0622 - accuracy: 0.9741 - val_loss: 0.1110 - val_accuracy: 0.9681\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0606 - accuracy: 0.9776 - val_loss: 0.1167 - val_accuracy: 0.9574\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0820 - accuracy: 0.9600 - val_loss: 0.1112 - val_accuracy: 0.9681\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0568 - accuracy: 0.9788 - val_loss: 0.1122 - val_accuracy: 0.9681\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0588 - accuracy: 0.9717 - val_loss: 0.1076 - val_accuracy: 0.9681\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0672 - accuracy: 0.9670 - val_loss: 0.1125 - val_accuracy: 0.9681\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0776 - accuracy: 0.9623 - val_loss: 0.1160 - val_accuracy: 0.9681\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0546 - accuracy: 0.9764 - val_loss: 0.1078 - val_accuracy: 0.9681\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0547 - accuracy: 0.9788 - val_loss: 0.1170 - val_accuracy: 0.9468\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0545 - accuracy: 0.9800 - val_loss: 0.1076 - val_accuracy: 0.9681\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0442 - accuracy: 0.9870 - val_loss: 0.1156 - val_accuracy: 0.9574\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0659 - accuracy: 0.9717 - val_loss: 0.1190 - val_accuracy: 0.9574\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0692 - accuracy: 0.9741 - val_loss: 0.1170 - val_accuracy: 0.9681\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0690 - accuracy: 0.9647 - val_loss: 0.1104 - val_accuracy: 0.9681\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0636 - accuracy: 0.9729 - val_loss: 0.1111 - val_accuracy: 0.9681\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0658 - accuracy: 0.9729 - val_loss: 0.1041 - val_accuracy: 0.9681\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0662 - accuracy: 0.9706 - val_loss: 0.1158 - val_accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0612 - accuracy: 0.9788 - val_loss: 0.1100 - val_accuracy: 0.9681\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0615 - accuracy: 0.9729 - val_loss: 0.1101 - val_accuracy: 0.9681\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0919 - accuracy: 0.9482 - val_loss: 0.1091 - val_accuracy: 0.9681\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0644 - accuracy: 0.9812 - val_loss: 0.1135 - val_accuracy: 0.9681\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0688 - accuracy: 0.9706 - val_loss: 0.1092 - val_accuracy: 0.9681\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0666 - accuracy: 0.9753 - val_loss: 0.1127 - val_accuracy: 0.9681\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0747 - accuracy: 0.9647 - val_loss: 0.1081 - val_accuracy: 0.9681\n",
      "Score for fold 8: loss of 0.10810919851064682; accuracy of 96.80851101875305%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 102ms/step - loss: 0.3929 - accuracy: 0.7397 - val_loss: 2.5496 - val_accuracy: 0.3936\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3287 - accuracy: 0.7998 - val_loss: 2.5957 - val_accuracy: 0.6702\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2580 - accuracy: 0.8575 - val_loss: 1.7199 - val_accuracy: 0.6915\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2332 - accuracy: 0.8645 - val_loss: 0.0961 - val_accuracy: 0.9681\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2211 - accuracy: 0.8763 - val_loss: 0.2719 - val_accuracy: 0.7872\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2291 - accuracy: 0.8528 - val_loss: 0.5280 - val_accuracy: 0.7447\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1811 - accuracy: 0.8916 - val_loss: 0.1391 - val_accuracy: 0.9468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1926 - accuracy: 0.8963 - val_loss: 0.1591 - val_accuracy: 0.9149\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1737 - accuracy: 0.8987 - val_loss: 0.1778 - val_accuracy: 0.8511\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1892 - accuracy: 0.8963 - val_loss: 0.1249 - val_accuracy: 0.9362\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1611 - accuracy: 0.9069 - val_loss: 0.1988 - val_accuracy: 0.9362\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1903 - accuracy: 0.8987 - val_loss: 0.4085 - val_accuracy: 0.7447\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1341 - accuracy: 0.9164 - val_loss: 0.4686 - val_accuracy: 0.7234\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1500 - accuracy: 0.9211 - val_loss: 0.1388 - val_accuracy: 0.9362\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1107 - accuracy: 0.9470 - val_loss: 0.1025 - val_accuracy: 0.9574\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1087 - accuracy: 0.9399 - val_loss: 0.1215 - val_accuracy: 0.9468\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1184 - accuracy: 0.9435 - val_loss: 0.1419 - val_accuracy: 0.8723\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1372 - accuracy: 0.9270 - val_loss: 0.1611 - val_accuracy: 0.8404\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0975 - accuracy: 0.9517 - val_loss: 0.1565 - val_accuracy: 0.9043\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0968 - accuracy: 0.9564 - val_loss: 0.1690 - val_accuracy: 0.8723\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0971 - accuracy: 0.9564 - val_loss: 0.1013 - val_accuracy: 0.9574\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1021 - accuracy: 0.9517 - val_loss: 0.1127 - val_accuracy: 0.9574\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0723 - accuracy: 0.9694 - val_loss: 0.1170 - val_accuracy: 0.9574\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0874 - accuracy: 0.9576 - val_loss: 0.1059 - val_accuracy: 0.9362\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0809 - accuracy: 0.9647 - val_loss: 0.1162 - val_accuracy: 0.9468\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0868 - accuracy: 0.9658 - val_loss: 0.1105 - val_accuracy: 0.9468\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0796 - accuracy: 0.9658 - val_loss: 0.1173 - val_accuracy: 0.9468\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0731 - accuracy: 0.9741 - val_loss: 0.1165 - val_accuracy: 0.9468\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0765 - accuracy: 0.9635 - val_loss: 0.1160 - val_accuracy: 0.9468\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0903 - accuracy: 0.9588 - val_loss: 0.1331 - val_accuracy: 0.9468\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0618 - accuracy: 0.9788 - val_loss: 0.1186 - val_accuracy: 0.9468\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.1208 - val_accuracy: 0.9362\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0838 - accuracy: 0.9600 - val_loss: 0.1230 - val_accuracy: 0.9255\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0723 - accuracy: 0.9706 - val_loss: 0.1231 - val_accuracy: 0.9468\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0628 - accuracy: 0.9788 - val_loss: 0.1315 - val_accuracy: 0.9255\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0951 - accuracy: 0.9635 - val_loss: 0.1291 - val_accuracy: 0.9255\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0754 - accuracy: 0.9694 - val_loss: 0.1289 - val_accuracy: 0.9255\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0834 - accuracy: 0.9658 - val_loss: 0.1227 - val_accuracy: 0.9468\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0547 - accuracy: 0.9788 - val_loss: 0.1199 - val_accuracy: 0.9255\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0669 - accuracy: 0.9741 - val_loss: 0.1263 - val_accuracy: 0.9468\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0647 - accuracy: 0.9800 - val_loss: 0.1288 - val_accuracy: 0.9468\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1024 - accuracy: 0.9529 - val_loss: 0.1290 - val_accuracy: 0.9468\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0825 - accuracy: 0.9623 - val_loss: 0.1263 - val_accuracy: 0.9362\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0837 - accuracy: 0.9635 - val_loss: 0.1264 - val_accuracy: 0.9362\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0811 - accuracy: 0.9635 - val_loss: 0.1270 - val_accuracy: 0.9468\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0588 - accuracy: 0.9729 - val_loss: 0.1279 - val_accuracy: 0.9468\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0885 - accuracy: 0.9529 - val_loss: 0.1253 - val_accuracy: 0.9468\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0539 - accuracy: 0.9835 - val_loss: 0.1273 - val_accuracy: 0.9468\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0495 - accuracy: 0.9870 - val_loss: 0.1277 - val_accuracy: 0.9468\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0610 - accuracy: 0.9776 - val_loss: 0.1257 - val_accuracy: 0.9468\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0492 - accuracy: 0.9859 - val_loss: 0.1261 - val_accuracy: 0.9468\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0828 - accuracy: 0.9682 - val_loss: 0.1257 - val_accuracy: 0.9468\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0629 - accuracy: 0.9753 - val_loss: 0.1287 - val_accuracy: 0.9468\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0690 - accuracy: 0.9717 - val_loss: 0.1272 - val_accuracy: 0.9468\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0695 - accuracy: 0.9717 - val_loss: 0.1247 - val_accuracy: 0.9362\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0811 - accuracy: 0.9670 - val_loss: 0.1245 - val_accuracy: 0.9468\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0784 - accuracy: 0.9694 - val_loss: 0.1287 - val_accuracy: 0.9468\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0570 - accuracy: 0.9753 - val_loss: 0.1232 - val_accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1052 - accuracy: 0.9446 - val_loss: 0.1250 - val_accuracy: 0.9468\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0538 - accuracy: 0.9823 - val_loss: 0.1249 - val_accuracy: 0.9468\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0712 - accuracy: 0.9776 - val_loss: 0.1261 - val_accuracy: 0.9468\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0725 - accuracy: 0.9741 - val_loss: 0.1259 - val_accuracy: 0.9468\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0767 - accuracy: 0.9670 - val_loss: 0.1255 - val_accuracy: 0.9468\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0763 - accuracy: 0.9694 - val_loss: 0.1267 - val_accuracy: 0.9468\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0572 - accuracy: 0.9812 - val_loss: 0.1260 - val_accuracy: 0.9468\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0708 - accuracy: 0.9682 - val_loss: 0.1293 - val_accuracy: 0.9468\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0590 - accuracy: 0.9753 - val_loss: 0.1252 - val_accuracy: 0.9468\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0694 - accuracy: 0.9682 - val_loss: 0.1238 - val_accuracy: 0.9468\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0796 - accuracy: 0.9753 - val_loss: 0.1255 - val_accuracy: 0.9468\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0976 - accuracy: 0.9600 - val_loss: 0.1273 - val_accuracy: 0.9468\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0868 - accuracy: 0.9611 - val_loss: 0.1242 - val_accuracy: 0.9468\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.1245 - val_accuracy: 0.9468\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0796 - accuracy: 0.9682 - val_loss: 0.1245 - val_accuracy: 0.9362\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0665 - accuracy: 0.9682 - val_loss: 0.1260 - val_accuracy: 0.9468\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0914 - accuracy: 0.9541 - val_loss: 0.1232 - val_accuracy: 0.9468\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0569 - accuracy: 0.9776 - val_loss: 0.1236 - val_accuracy: 0.9468\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0597 - accuracy: 0.9753 - val_loss: 0.1262 - val_accuracy: 0.9468\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0541 - accuracy: 0.9800 - val_loss: 0.1271 - val_accuracy: 0.9468\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0982 - accuracy: 0.9552 - val_loss: 0.1263 - val_accuracy: 0.9468\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0797 - accuracy: 0.9717 - val_loss: 0.1279 - val_accuracy: 0.9468\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0743 - accuracy: 0.9753 - val_loss: 0.1256 - val_accuracy: 0.9468\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0710 - accuracy: 0.9694 - val_loss: 0.1275 - val_accuracy: 0.9468\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0552 - accuracy: 0.9823 - val_loss: 0.1262 - val_accuracy: 0.9468\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0589 - accuracy: 0.9741 - val_loss: 0.1251 - val_accuracy: 0.9468\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0808 - accuracy: 0.9611 - val_loss: 0.1267 - val_accuracy: 0.9468\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0777 - accuracy: 0.9658 - val_loss: 0.1245 - val_accuracy: 0.9468\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0891 - accuracy: 0.9611 - val_loss: 0.1257 - val_accuracy: 0.9468\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0502 - accuracy: 0.9835 - val_loss: 0.1289 - val_accuracy: 0.9362\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.1249 - val_accuracy: 0.9468\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0585 - accuracy: 0.9788 - val_loss: 0.1267 - val_accuracy: 0.9468\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0681 - accuracy: 0.9706 - val_loss: 0.1223 - val_accuracy: 0.9468\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0597 - accuracy: 0.9776 - val_loss: 0.1251 - val_accuracy: 0.9468\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0674 - accuracy: 0.9741 - val_loss: 0.1241 - val_accuracy: 0.9468\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0755 - accuracy: 0.9623 - val_loss: 0.1230 - val_accuracy: 0.9468\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0627 - accuracy: 0.9812 - val_loss: 0.1266 - val_accuracy: 0.9468\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0862 - accuracy: 0.9600 - val_loss: 0.1229 - val_accuracy: 0.9468\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0565 - accuracy: 0.9812 - val_loss: 0.1249 - val_accuracy: 0.9362\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0630 - accuracy: 0.9729 - val_loss: 0.1243 - val_accuracy: 0.9468\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0666 - accuracy: 0.9800 - val_loss: 0.1249 - val_accuracy: 0.9468\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0805 - accuracy: 0.9600 - val_loss: 0.1236 - val_accuracy: 0.9468\n",
      "Score for fold 9: loss of 0.12362325936555862; accuracy of 94.68085169792175%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 102ms/step - loss: 0.3536 - accuracy: 0.7786 - val_loss: 1.5634 - val_accuracy: 0.6702\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3028 - accuracy: 0.8163 - val_loss: 2.6771 - val_accuracy: 0.7021\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2722 - accuracy: 0.8327 - val_loss: 0.1076 - val_accuracy: 0.9362\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2458 - accuracy: 0.8634 - val_loss: 0.7597 - val_accuracy: 0.7447\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2390 - accuracy: 0.8693 - val_loss: 0.1343 - val_accuracy: 0.9362\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2031 - accuracy: 0.8834 - val_loss: 0.4518 - val_accuracy: 0.7766\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1951 - accuracy: 0.8975 - val_loss: 1.4562 - val_accuracy: 0.6702\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2011 - accuracy: 0.8846 - val_loss: 0.6098 - val_accuracy: 0.8511\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1615 - accuracy: 0.8999 - val_loss: 0.2114 - val_accuracy: 0.9149\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1615 - accuracy: 0.9093 - val_loss: 0.1541 - val_accuracy: 0.9255\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1569 - accuracy: 0.9046 - val_loss: 0.1132 - val_accuracy: 0.9255\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1611 - accuracy: 0.9046 - val_loss: 0.1176 - val_accuracy: 0.9255\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1462 - accuracy: 0.9105 - val_loss: 0.1767 - val_accuracy: 0.9362\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1539 - accuracy: 0.9128 - val_loss: 0.1865 - val_accuracy: 0.9149\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1478 - accuracy: 0.9199 - val_loss: 0.7324 - val_accuracy: 0.7553\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1409 - accuracy: 0.9234 - val_loss: 0.1558 - val_accuracy: 0.9255\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1129 - accuracy: 0.9435 - val_loss: 0.3096 - val_accuracy: 0.8936\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0987 - accuracy: 0.9541 - val_loss: 1.2666 - val_accuracy: 0.6915\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1429 - accuracy: 0.9293 - val_loss: 0.3440 - val_accuracy: 0.7660\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1217 - accuracy: 0.9399 - val_loss: 0.4848 - val_accuracy: 0.7340\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1153 - accuracy: 0.9446 - val_loss: 0.1084 - val_accuracy: 0.9362\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0919 - accuracy: 0.9576 - val_loss: 0.1104 - val_accuracy: 0.9468\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0890 - accuracy: 0.9658 - val_loss: 0.1078 - val_accuracy: 0.9362\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0895 - accuracy: 0.9458 - val_loss: 0.1403 - val_accuracy: 0.9149\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1092 - accuracy: 0.9446 - val_loss: 0.1075 - val_accuracy: 0.9468\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0960 - accuracy: 0.9541 - val_loss: 0.1015 - val_accuracy: 0.9468\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0704 - accuracy: 0.9611 - val_loss: 0.1015 - val_accuracy: 0.9362\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0820 - accuracy: 0.9670 - val_loss: 0.1019 - val_accuracy: 0.9362\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 22s 102ms/step - loss: 0.0815 - accuracy: 0.9729 - val_loss: 0.1050 - val_accuracy: 0.9362\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0792 - accuracy: 0.9682 - val_loss: 0.0956 - val_accuracy: 0.9574\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0862 - accuracy: 0.9647 - val_loss: 0.0972 - val_accuracy: 0.9468\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0813 - accuracy: 0.9623 - val_loss: 0.0972 - val_accuracy: 0.9362\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0812 - accuracy: 0.9670 - val_loss: 0.0953 - val_accuracy: 0.9574\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0685 - accuracy: 0.9706 - val_loss: 0.0994 - val_accuracy: 0.9574\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0879 - accuracy: 0.9647 - val_loss: 0.1035 - val_accuracy: 0.9468\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0717 - accuracy: 0.9658 - val_loss: 0.0955 - val_accuracy: 0.9362\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0876 - accuracy: 0.9611 - val_loss: 0.1112 - val_accuracy: 0.9362\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0869 - accuracy: 0.9611 - val_loss: 0.1208 - val_accuracy: 0.9362\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0782 - accuracy: 0.9635 - val_loss: 0.1030 - val_accuracy: 0.9468\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0693 - accuracy: 0.9776 - val_loss: 0.1146 - val_accuracy: 0.9362\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1020 - accuracy: 0.9470 - val_loss: 0.1044 - val_accuracy: 0.9362\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0508 - accuracy: 0.9859 - val_loss: 0.1068 - val_accuracy: 0.9362\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0620 - accuracy: 0.9812 - val_loss: 0.1054 - val_accuracy: 0.9362\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0616 - accuracy: 0.9823 - val_loss: 0.1063 - val_accuracy: 0.9362\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0666 - accuracy: 0.9800 - val_loss: 0.1001 - val_accuracy: 0.9362\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0751 - accuracy: 0.9694 - val_loss: 0.1010 - val_accuracy: 0.9362\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0753 - accuracy: 0.9694 - val_loss: 0.0978 - val_accuracy: 0.9362\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0660 - accuracy: 0.9729 - val_loss: 0.1008 - val_accuracy: 0.9362\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0814 - accuracy: 0.9670 - val_loss: 0.1050 - val_accuracy: 0.9362\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0750 - accuracy: 0.9635 - val_loss: 0.1101 - val_accuracy: 0.9362\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0752 - accuracy: 0.9635 - val_loss: 0.1018 - val_accuracy: 0.9362\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0836 - accuracy: 0.9564 - val_loss: 0.1044 - val_accuracy: 0.9362\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0636 - accuracy: 0.9741 - val_loss: 0.1011 - val_accuracy: 0.9362\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0559 - accuracy: 0.9823 - val_loss: 0.1025 - val_accuracy: 0.9468\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0519 - accuracy: 0.9835 - val_loss: 0.1076 - val_accuracy: 0.9362\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0747 - accuracy: 0.9682 - val_loss: 0.1064 - val_accuracy: 0.9468\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0639 - accuracy: 0.9788 - val_loss: 0.1077 - val_accuracy: 0.9468\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0823 - accuracy: 0.9600 - val_loss: 0.0987 - val_accuracy: 0.9362\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0608 - accuracy: 0.9729 - val_loss: 0.1064 - val_accuracy: 0.9468\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0980 - accuracy: 0.9446 - val_loss: 0.1023 - val_accuracy: 0.9362\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0750 - accuracy: 0.9741 - val_loss: 0.1082 - val_accuracy: 0.9362\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0689 - accuracy: 0.9741 - val_loss: 0.1086 - val_accuracy: 0.9362\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0709 - accuracy: 0.9635 - val_loss: 0.1031 - val_accuracy: 0.9362\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0785 - accuracy: 0.9623 - val_loss: 0.0963 - val_accuracy: 0.9362\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0587 - accuracy: 0.9788 - val_loss: 0.1114 - val_accuracy: 0.9362\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0759 - accuracy: 0.9741 - val_loss: 0.1108 - val_accuracy: 0.9362\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0750 - accuracy: 0.9647 - val_loss: 0.0943 - val_accuracy: 0.9255\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0704 - accuracy: 0.9706 - val_loss: 0.1104 - val_accuracy: 0.9362\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0661 - accuracy: 0.9706 - val_loss: 0.1027 - val_accuracy: 0.9362\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0838 - accuracy: 0.9670 - val_loss: 0.0962 - val_accuracy: 0.9362\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0781 - accuracy: 0.9682 - val_loss: 0.1031 - val_accuracy: 0.9468\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0641 - accuracy: 0.9800 - val_loss: 0.1022 - val_accuracy: 0.9362\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0612 - accuracy: 0.9717 - val_loss: 0.1110 - val_accuracy: 0.9362\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0649 - accuracy: 0.9741 - val_loss: 0.1039 - val_accuracy: 0.9362\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0930 - accuracy: 0.9517 - val_loss: 0.0963 - val_accuracy: 0.9362\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0652 - accuracy: 0.9694 - val_loss: 0.0964 - val_accuracy: 0.9362\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0485 - accuracy: 0.9894 - val_loss: 0.1024 - val_accuracy: 0.9362\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0582 - accuracy: 0.9812 - val_loss: 0.1007 - val_accuracy: 0.9362\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0893 - accuracy: 0.9611 - val_loss: 0.0985 - val_accuracy: 0.9362\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0834 - accuracy: 0.9600 - val_loss: 0.1000 - val_accuracy: 0.9362\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0850 - accuracy: 0.9552 - val_loss: 0.0986 - val_accuracy: 0.9362\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0765 - accuracy: 0.9658 - val_loss: 0.1005 - val_accuracy: 0.9468\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0661 - accuracy: 0.9682 - val_loss: 0.1007 - val_accuracy: 0.9362\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0672 - accuracy: 0.9706 - val_loss: 0.1021 - val_accuracy: 0.9362\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0693 - accuracy: 0.9706 - val_loss: 0.0971 - val_accuracy: 0.9362\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0749 - accuracy: 0.9647 - val_loss: 0.0971 - val_accuracy: 0.9362\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0595 - accuracy: 0.9753 - val_loss: 0.1010 - val_accuracy: 0.9468\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0751 - accuracy: 0.9670 - val_loss: 0.1061 - val_accuracy: 0.9468\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0742 - accuracy: 0.9694 - val_loss: 0.0965 - val_accuracy: 0.9362\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0676 - accuracy: 0.9658 - val_loss: 0.1023 - val_accuracy: 0.9468\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0606 - accuracy: 0.9764 - val_loss: 0.1142 - val_accuracy: 0.9468\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0590 - accuracy: 0.9788 - val_loss: 0.1052 - val_accuracy: 0.9362\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0760 - accuracy: 0.9694 - val_loss: 0.1069 - val_accuracy: 0.9468\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0696 - accuracy: 0.9658 - val_loss: 0.0986 - val_accuracy: 0.9362\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0597 - accuracy: 0.9764 - val_loss: 0.1040 - val_accuracy: 0.9362\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0653 - accuracy: 0.9729 - val_loss: 0.1138 - val_accuracy: 0.9362\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0807 - accuracy: 0.9647 - val_loss: 0.1012 - val_accuracy: 0.9362\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0683 - accuracy: 0.9729 - val_loss: 0.1158 - val_accuracy: 0.9362\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0692 - accuracy: 0.9729 - val_loss: 0.0995 - val_accuracy: 0.9362\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0667 - accuracy: 0.9753 - val_loss: 0.1046 - val_accuracy: 0.9468\n",
      "Score for fold 10: loss of 0.10461145639419556; accuracy of 94.68085169792175%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.14867378771305084 - Accuracy: 87.36842274665833%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.17679157853126526 - Accuracy: 89.47368264198303%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.11370990425348282 - Accuracy: 90.52631855010986%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.09078632295131683 - Accuracy: 96.80851101875305%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.11312119662761688 - Accuracy: 92.55319237709045%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.10883263498544693 - Accuracy: 92.55319237709045%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.1294270157814026 - Accuracy: 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.10810919851064682 - Accuracy: 96.80851101875305%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.12362325936555862 - Accuracy: 94.68085169792175%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.10461145639419556 - Accuracy: 94.68085169792175%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 92.69428968429565 (+- 2.944069033503616)\n",
      "> Loss: 0.12176863551139831\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        np.savetxt('E:/Result/ver.3.22/RP/train/' + f'train_{fold_no}.csv', train, delimiter=\",\")\n",
    "        np.savetxt('E:/Result/ver.3.22/RP/test/' + f'test_{fold_no}.csv', test, delimiter=\",\")\n",
    "\n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = callbacks_list) # 여기에 Validation set을 넣어야되는거 아닌가?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/Result/ver.3.22/RP/weight/' + f'RP_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7ec7915fb6663623914b2919607c696d31d8503b403c6d54bcb30c0bb224a7"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
