{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\jwhyu\\AppData\\Local\\Temp/ipykernel_12916/3517717524.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__) \n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1003176076050008149\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9697045489331392947\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new desktop test\n"
     ]
    }
   ],
   "source": [
    "print(\"new desktop test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/3.19/RP/RP.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 ... 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X_data,y_data,i_data, test_size = 0.2, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 300, 300, 2)\n",
      "(2400,)\n",
      "(600, 300, 300, 2)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train,X_test))\n",
    "targets = np.concatenate((y_train,y_test))\n",
    "index = np.concatenate((i_train,i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "targets = np_utils.to_categorical(targets)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 306, 306, 2)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 150, 150, 64  6272        ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 150, 150, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 150, 150, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 152, 152, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 75, 75, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 75, 75, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 75, 75, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 75, 75, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 75, 75, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 75, 75, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 75, 75, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 75, 75, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 75, 75, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 75, 75, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 75, 75, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 75, 75, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 75, 75, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 75, 75, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 75, 75, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 75, 75, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 75, 75, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 75, 75, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 75, 75, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 75, 75, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 75, 75, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 75, 75, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 75, 75, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 75, 75, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 75, 75, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 75, 75, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 37, 37, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 37, 37, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 37, 37, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 37, 37, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 37, 37, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 37, 37, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 37, 37, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 37, 37, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 37, 37, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 37, 37, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 37, 37, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 37, 37, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 37, 37, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 37, 37, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 37, 37, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 37, 37, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 37, 37, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 37, 37, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 37, 37, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 37, 37, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 37, 37, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 37, 37, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 37, 37, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 37, 37, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 37, 37, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 37, 37, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 37, 37, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 37, 37, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 37, 37, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 37, 37, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 37, 37, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 37, 37, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 37, 37, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 37, 37, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 37, 37, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 37, 37, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 37, 37, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 37, 37, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 37, 37, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 37, 37, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 37, 37, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 37, 37, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 37, 37, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 37, 37, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 37, 37, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 37, 37, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 37, 37, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 37, 37, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 37, 37, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 37, 37, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 37, 37, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 18, 18, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 18, 18, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 18, 18, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 18, 18, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 18, 18, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 18, 18, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 18, 18, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 18, 18, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 18, 18, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 18, 18, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 18, 18, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 18, 18, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 18, 18, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 18, 18, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 18, 18, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 18, 18, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 18, 18, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 18, 18, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 18, 18, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 18, 18, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 18, 18, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 18, 18, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 18, 18, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 18, 18, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 18, 18, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 18, 18, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 18, 18, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 18, 18, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 18, 18, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 18, 18, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 18, 18, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 18, 18, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 18, 18, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 18, 18, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 18, 18, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 18, 18, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 18, 18, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 18, 18, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 18, 18, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 18, 18, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 18, 18, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 18, 18, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 18, 18, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 18, 18, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 18, 18, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 18, 18, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 18, 18, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 18, 18, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 18, 18, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 18, 18, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 18, 18, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 18, 18, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 18, 18, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 18, 18, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 18, 18, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 18, 18, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 18, 18, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 18, 18, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 18, 18, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 18, 18, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 18, 18, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 18, 18, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 18, 18, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 18, 18, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 18, 18, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 18, 18, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 18, 18, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 18, 18, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 18, 18, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 18, 18, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 18, 18, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 18, 18, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 18, 18, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 18, 18, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 18, 18, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 18, 18, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 18, 18, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 18, 18, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 18, 18, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 18, 18, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 18, 18, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 18, 18, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 18, 18, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 18, 18, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 18, 18, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 18, 18, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 18, 18, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 18, 18, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 18, 18, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 18, 18, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 18, 18, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 18, 18, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 18, 18, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 18, 18, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 18, 18, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 18, 18, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 18, 18, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 18, 18, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 18, 18, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 18, 18, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 9, 9, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 9, 9, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 9, 9, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 9, 9, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 9, 9, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 9, 9, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 9, 9, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 9, 9, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 9, 9, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 9, 9, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 9, 9, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 9, 9, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 9, 9, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 9, 9, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 9, 9, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 9, 9, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 9, 9, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 9, 9, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 9, 9, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 9, 9, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 9, 9, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 9, 9, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 9, 9, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 9, 9, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 9, 9, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 9, 9, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 9, 9, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 9, 9, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 9, 9, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 9, 9, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 9, 9, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 9, 9, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 9, 9, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 9, 9, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 9, 9, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 9, 9, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 9, 9, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 9, 9, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 9, 9, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 9, 9, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 9, 9, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 9, 9, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 9, 9, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 9, 9, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 9, 9, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 9, 9, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 9, 9, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 9, 9, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 9, 9, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 9, 9, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 9, 9, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 9, 9, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 9, 9, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 9, 9, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 9, 9, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 9, 9, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 9, 9, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 9, 9, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 9, 9, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 9, 9, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 9, 9, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 9, 9, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 9, 9, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 9, 9, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 9, 9, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 9, 9, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 9, 9, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1024)        0           ['relu[0][0]']                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3075        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,037,443\n",
      "Trainable params: 6,953,795\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(300, 300, 2))\n",
    "model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    " \n",
    "x = model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateSchedule(Callback):\n",
    "    def __init__(self, selected_epochs=[]):\n",
    "        self.selected_epochs = selected_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) in self.selected_epochs:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            K.set_value(self.model.optimizer.lr, lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "test = []\n",
    "train= []\n",
    "test_ = []\n",
    "train_ = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:2700]\n",
    "train = np.reshape(train, 2700)\n",
    "test = test_[0:300]\n",
    "test = np.reshape(test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   34   36   70   99  118  121  125  142  151  172  180  186  190\n",
      "  203  206  224  259  278  280  296  314  318  325  340  356  361  372\n",
      "  375  377  386  412  414  415  419  420  423  426  429  430  433  440\n",
      "  454  456  468  474  514  526  532  536  539  561  562  568  582  584\n",
      "  587  592  631  637  643  658  672  674  684  699  707  718  720  727\n",
      "  729  732  734  735  737  750  752  770  773  785  802  803  804  807\n",
      "  819  832  852  855  858  868  871  879  883  890  891  928  933  964\n",
      "  974  979  983  992 1038 1051 1054 1057 1073 1077 1098 1102 1104 1109\n",
      " 1111 1123 1140 1141 1159 1162 1163 1164 1166 1181 1205 1210 1224 1229\n",
      " 1235 1242 1271 1306 1327 1351 1363 1377 1381 1389 1391 1396 1426 1435\n",
      " 1443 1448 1459 1463 1476 1483 1495 1513 1528 1530 1541 1543 1547 1564\n",
      " 1598 1600 1609 1611 1622 1634 1647 1667 1670 1698 1705 1707 1732 1734\n",
      " 1740 1742 1745 1759 1784 1806 1813 1845 1853 1877 1897 1898 1910 1963\n",
      " 1966 1968 1989 1992 2004 2037 2043 2051 2052 2054 2077 2087 2090 2101\n",
      " 2106 2108 2112 2129 2134 2136 2144 2149 2153 2171 2179 2186 2187 2205\n",
      " 2214 2216 2228 2233 2235 2259 2264 2268 2272 2282 2294 2307 2312 2314\n",
      " 2317 2336 2353 2358 2378 2385 2392 2409 2440 2449 2452 2453 2460 2467\n",
      " 2478 2479 2498 2502 2532 2533 2544 2551 2562 2565 2584 2585 2591 2592\n",
      " 2610 2614 2620 2649 2651 2674 2693 2715 2726 2730 2743 2747 2748 2767\n",
      " 2769 2771 2772 2773 2774 2783 2785 2799 2803 2805 2812 2815 2820 2823\n",
      " 2834 2837 2850 2862 2865 2866 2892 2902 2912 2930 2933 2946 2953 2957\n",
      " 2961 2962 2974 2976 2985 2995]\n",
      "[   0    1    3 ... 2997 2998 2999]\n",
      "['H_999' 'H_999' 'H_999' 'P_600' 'P_724' 'H_999' 'H_999' 'P_620' 'H_999'\n",
      " 'H_999' 'H_999' 'N_23' 'P_613' 'H_999' 'N_902' 'N_525' 'H_999' 'P_70'\n",
      " 'P_257' 'P_678' 'H_999' 'N_515' 'H_999' 'P_342' 'N_953' 'N_211' 'P_908'\n",
      " 'H_999' 'N_627' 'H_999' 'H_999' 'H_999' 'H_999' 'N_100' 'N_404' 'P_331'\n",
      " 'P_441' 'P_130' 'H_999' 'H_999' 'N_164' 'P_695' 'H_999' 'N_720' 'P_665'\n",
      " 'N_473' 'P_196' 'N_533' 'H_999' 'P_175' 'P_553' 'P_991' 'N_704' 'N_624'\n",
      " 'N_444' 'H_999' 'P_736' 'P_619' 'P_609' 'N_226' 'N_833' 'H_999' 'N_270'\n",
      " 'N_764' 'P_152' 'P_353' 'H_999' 'H_999' 'N_683' 'H_999' 'P_215' 'P_626'\n",
      " 'H_999' 'P_824' 'P_20' 'P_526' 'P_538' 'N_520' 'P_995' 'N_911' 'H_999'\n",
      " 'P_208' 'N_892' 'H_999' 'P_901' 'H_999' 'P_250' 'H_999' 'P_920' 'P_389'\n",
      " 'N_453' 'N_710' 'P_807' 'H_999' 'H_999' 'H_999' 'N_518' 'P_395' 'P_345'\n",
      " 'P_481' 'N_735' 'P_36' 'N_414' 'H_999' 'P_73' 'P_740' 'H_999' 'N_137'\n",
      " 'H_999' 'P_80' 'N_313' 'N_610' 'P_850' 'H_999' 'P_622' 'H_999' 'H_999'\n",
      " 'P_829' 'H_999' 'H_999' 'H_999' 'H_999' 'P_145' 'N_15' 'H_999' 'N_480'\n",
      " 'H_999' 'H_999' 'H_999' 'P_696' 'P_337' 'P_779' 'N_83' 'N_607' 'N_377'\n",
      " 'P_204' 'N_210' 'P_0' 'N_639' 'H_999' 'H_999' 'H_999' 'H_999' 'N_427'\n",
      " 'P_890' 'P_987' 'H_999' 'H_999' 'N_599' 'P_968' 'P_548' 'P_700' 'P_773'\n",
      " 'H_999' 'N_3' 'N_133' 'P_896' 'H_999' 'P_500' 'P_263' 'H_999' 'N_742'\n",
      " 'H_999' 'H_999' 'H_999' 'P_670' 'H_999' 'H_999' 'P_170' 'N_285' 'H_999'\n",
      " 'P_801' 'N_371' 'N_558' 'P_883' 'P_880' 'P_562' 'P_787' 'P_171' 'H_999'\n",
      " 'P_260' 'H_999' 'H_999' 'H_999' 'N_705' 'H_999' 'N_90' 'P_597' 'N_232'\n",
      " 'N_178' 'N_675' 'P_906' 'P_893' 'N_4' 'N_503' 'H_999' 'N_681' 'N_870'\n",
      " 'H_999' 'N_479' 'H_999' 'P_262' 'P_572' 'P_420' 'P_675' 'P_203' 'H_999'\n",
      " 'H_999' 'N_738' 'N_852' 'H_999' 'P_790' 'P_583' 'P_447' 'N_895' 'N_55'\n",
      " 'N_182' 'P_455' 'P_299' 'P_565' 'P_304' 'N_740' 'N_307' 'H_999' 'P_39'\n",
      " 'P_121' 'P_527' 'N_593' 'P_874' 'N_481' 'N_411' 'H_999' 'P_714' 'P_612'\n",
      " 'H_999' 'H_999' 'N_216' 'N_33' 'H_999' 'H_999' 'P_453' 'N_302' 'P_245'\n",
      " 'H_999' 'N_61' 'H_999' 'H_999' 'N_281' 'N_713' 'H_999' 'N_39' 'N_674'\n",
      " 'N_102' 'P_252' 'N_919' 'N_698' 'N_801' 'P_300' 'H_999' 'H_999' 'P_40'\n",
      " 'H_999' 'N_207' 'H_999' 'P_72' 'H_999' 'N_907' 'N_769' 'N_288' 'H_999'\n",
      " 'P_992' 'P_478' 'H_999' 'N_528' 'N_784' 'P_540' 'P_751' 'H_999' 'P_332'\n",
      " 'N_8' 'H_999' 'P_271' 'P_737' 'N_432' 'H_999' 'H_999' 'P_232' 'P_386'\n",
      " 'N_325' 'P_234' 'N_679' 'N_653' 'H_999' 'H_999' 'P_588' 'P_367' 'P_488'\n",
      " 'H_999' 'H_999' 'H_999']\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)\n",
    "print(index[test])\n",
    "print(targets[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir('E:/3.19/RP' + '/' + 'train')\n",
    "os.mkdir('E:/3.19/RP' + '/' + 'test')\n",
    "os.mkdir('E:/3.19/RP/' + 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160,) (240,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JWH\\anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 78s 117ms/step - loss: 0.3319 - accuracy: 0.7995 - val_loss: 0.2752 - val_accuracy: 0.8125\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.2583 - accuracy: 0.8477 - val_loss: 0.1768 - val_accuracy: 0.8792\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.2095 - accuracy: 0.8722 - val_loss: 0.4139 - val_accuracy: 0.8042\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.1893 - accuracy: 0.8898 - val_loss: 0.2166 - val_accuracy: 0.8667\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.1705 - accuracy: 0.9074 - val_loss: 0.6382 - val_accuracy: 0.6792\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1574 - accuracy: 0.9083 - val_loss: 0.2817 - val_accuracy: 0.7750\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1565 - accuracy: 0.9116 - val_loss: 0.1644 - val_accuracy: 0.8958\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1616 - accuracy: 0.9014 - val_loss: 0.0874 - val_accuracy: 0.9542\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.1265 - accuracy: 0.9250 - val_loss: 0.0657 - val_accuracy: 0.9667\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1323 - accuracy: 0.9282 - val_loss: 0.1934 - val_accuracy: 0.8583\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.1200 - accuracy: 0.9333 - val_loss: 0.1044 - val_accuracy: 0.9417\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1240 - accuracy: 0.9343 - val_loss: 0.1135 - val_accuracy: 0.9333\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1019 - accuracy: 0.9389 - val_loss: 0.2871 - val_accuracy: 0.7917\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.1001 - accuracy: 0.9440 - val_loss: 0.5313 - val_accuracy: 0.7250\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.1024 - accuracy: 0.9403 - val_loss: 0.0853 - val_accuracy: 0.9458\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0913 - accuracy: 0.9528 - val_loss: 0.1411 - val_accuracy: 0.9125\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0880 - accuracy: 0.9556 - val_loss: 0.1846 - val_accuracy: 0.8792\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0943 - accuracy: 0.9468 - val_loss: 0.1852 - val_accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0919 - accuracy: 0.9486 - val_loss: 0.2487 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0708 - accuracy: 0.9648 - val_loss: 0.1203 - val_accuracy: 0.9333\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0747 - accuracy: 0.9583 - val_loss: 0.0652 - val_accuracy: 0.9667\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0715 - accuracy: 0.9653 - val_loss: 0.0687 - val_accuracy: 0.9625\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0633 - accuracy: 0.9718 - val_loss: 0.0681 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0534 - accuracy: 0.9778 - val_loss: 0.0668 - val_accuracy: 0.9625\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0602 - accuracy: 0.9722 - val_loss: 0.0688 - val_accuracy: 0.9583\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0647 - accuracy: 0.9694 - val_loss: 0.0623 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0613 - accuracy: 0.9694 - val_loss: 0.0637 - val_accuracy: 0.9625\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0578 - accuracy: 0.9750 - val_loss: 0.1006 - val_accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0516 - accuracy: 0.9778 - val_loss: 0.0695 - val_accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0660 - accuracy: 0.9620 - val_loss: 0.0696 - val_accuracy: 0.9583\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0565 - accuracy: 0.9731 - val_loss: 0.0688 - val_accuracy: 0.9625\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0502 - accuracy: 0.9750 - val_loss: 0.0697 - val_accuracy: 0.9625\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0587 - accuracy: 0.9727 - val_loss: 0.0648 - val_accuracy: 0.9625\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0568 - accuracy: 0.9718 - val_loss: 0.0722 - val_accuracy: 0.9625\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.0658 - val_accuracy: 0.9625\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0542 - accuracy: 0.9745 - val_loss: 0.0603 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0571 - accuracy: 0.9699 - val_loss: 0.0654 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0520 - accuracy: 0.9755 - val_loss: 0.0646 - val_accuracy: 0.9583\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0548 - accuracy: 0.9787 - val_loss: 0.0657 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0597 - accuracy: 0.9718 - val_loss: 0.0705 - val_accuracy: 0.9667\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0560 - accuracy: 0.9731 - val_loss: 0.0651 - val_accuracy: 0.9625\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0567 - accuracy: 0.9741 - val_loss: 0.0715 - val_accuracy: 0.9667\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0480 - accuracy: 0.9801 - val_loss: 0.0753 - val_accuracy: 0.9667\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0492 - accuracy: 0.9755 - val_loss: 0.0700 - val_accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0471 - accuracy: 0.9782 - val_loss: 0.0691 - val_accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0517 - accuracy: 0.9764 - val_loss: 0.0736 - val_accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0506 - accuracy: 0.9773 - val_loss: 0.0676 - val_accuracy: 0.9625\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0595 - accuracy: 0.9667 - val_loss: 0.0652 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0536 - accuracy: 0.9708 - val_loss: 0.0681 - val_accuracy: 0.9625\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0494 - accuracy: 0.9801 - val_loss: 0.0653 - val_accuracy: 0.9625\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0548 - accuracy: 0.9782 - val_loss: 0.0654 - val_accuracy: 0.9625\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0461 - accuracy: 0.9819 - val_loss: 0.0650 - val_accuracy: 0.9625\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0534 - accuracy: 0.9769 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0522 - accuracy: 0.9769 - val_loss: 0.0731 - val_accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0519 - accuracy: 0.9750 - val_loss: 0.0719 - val_accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0490 - accuracy: 0.9810 - val_loss: 0.0679 - val_accuracy: 0.9667\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0606 - accuracy: 0.9727 - val_loss: 0.0654 - val_accuracy: 0.9625\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0450 - accuracy: 0.9792 - val_loss: 0.0684 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0567 - accuracy: 0.9755 - val_loss: 0.0643 - val_accuracy: 0.9625\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0517 - accuracy: 0.9782 - val_loss: 0.0711 - val_accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0509 - accuracy: 0.9755 - val_loss: 0.0693 - val_accuracy: 0.9667\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0577 - accuracy: 0.9741 - val_loss: 0.0663 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0529 - accuracy: 0.9792 - val_loss: 0.0708 - val_accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0428 - accuracy: 0.9843 - val_loss: 0.0679 - val_accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0466 - accuracy: 0.9806 - val_loss: 0.0650 - val_accuracy: 0.9625\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0405 - accuracy: 0.9852 - val_loss: 0.0676 - val_accuracy: 0.9667\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0543 - accuracy: 0.9731 - val_loss: 0.0694 - val_accuracy: 0.9667\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0547 - accuracy: 0.9708 - val_loss: 0.0720 - val_accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0512 - accuracy: 0.9755 - val_loss: 0.0643 - val_accuracy: 0.9625\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0472 - accuracy: 0.9769 - val_loss: 0.0668 - val_accuracy: 0.9667\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0539 - accuracy: 0.9745 - val_loss: 0.0662 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0486 - accuracy: 0.9824 - val_loss: 0.0698 - val_accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0604 - accuracy: 0.9713 - val_loss: 0.0659 - val_accuracy: 0.9625\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0506 - accuracy: 0.9787 - val_loss: 0.0642 - val_accuracy: 0.9625\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0477 - accuracy: 0.9806 - val_loss: 0.0665 - val_accuracy: 0.9625\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0623 - accuracy: 0.9685 - val_loss: 0.0667 - val_accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0505 - accuracy: 0.9792 - val_loss: 0.0635 - val_accuracy: 0.9625\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0559 - accuracy: 0.9708 - val_loss: 0.0665 - val_accuracy: 0.9625\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0595 - accuracy: 0.9718 - val_loss: 0.0697 - val_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0496 - accuracy: 0.9769 - val_loss: 0.0674 - val_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0508 - accuracy: 0.9778 - val_loss: 0.0710 - val_accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0590 - accuracy: 0.9727 - val_loss: 0.0661 - val_accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0519 - accuracy: 0.9773 - val_loss: 0.0649 - val_accuracy: 0.9625\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0538 - accuracy: 0.9718 - val_loss: 0.0658 - val_accuracy: 0.9625\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0592 - accuracy: 0.9722 - val_loss: 0.0668 - val_accuracy: 0.9625\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0480 - accuracy: 0.9810 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0498 - accuracy: 0.9778 - val_loss: 0.0654 - val_accuracy: 0.9625\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0489 - accuracy: 0.9755 - val_loss: 0.0732 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0558 - accuracy: 0.9722 - val_loss: 0.0672 - val_accuracy: 0.9625\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0512 - accuracy: 0.9769 - val_loss: 0.0642 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0531 - accuracy: 0.9759 - val_loss: 0.0673 - val_accuracy: 0.9667\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0651 - accuracy: 0.9653 - val_loss: 0.0666 - val_accuracy: 0.9625\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0568 - accuracy: 0.9727 - val_loss: 0.0698 - val_accuracy: 0.9625\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0519 - accuracy: 0.9778 - val_loss: 0.0652 - val_accuracy: 0.9625\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0584 - accuracy: 0.9671 - val_loss: 0.0638 - val_accuracy: 0.9625\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0570 - accuracy: 0.9731 - val_loss: 0.0685 - val_accuracy: 0.9667\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0533 - accuracy: 0.9722 - val_loss: 0.0657 - val_accuracy: 0.9625\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0429 - accuracy: 0.9861 - val_loss: 0.0687 - val_accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0566 - accuracy: 0.9722 - val_loss: 0.0687 - val_accuracy: 0.9625\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0558 - accuracy: 0.9722 - val_loss: 0.0635 - val_accuracy: 0.9625\n",
      "Score for fold 1: loss of 0.0634547546505928; accuracy of 96.24999761581421%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 67s 109ms/step - loss: 0.3214 - accuracy: 0.7889 - val_loss: 0.5185 - val_accuracy: 0.7167\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.2469 - accuracy: 0.8468 - val_loss: 0.4199 - val_accuracy: 0.7250\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1916 - accuracy: 0.8958 - val_loss: 0.2428 - val_accuracy: 0.8042\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2158 - accuracy: 0.8815 - val_loss: 0.1129 - val_accuracy: 0.9458\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1860 - accuracy: 0.8963 - val_loss: 0.0868 - val_accuracy: 0.9500\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1665 - accuracy: 0.9111 - val_loss: 0.1155 - val_accuracy: 0.9375\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1418 - accuracy: 0.9292 - val_loss: 0.1837 - val_accuracy: 0.8917\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 65s 120ms/step - loss: 0.1349 - accuracy: 0.9236 - val_loss: 0.1347 - val_accuracy: 0.9083\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.1367 - accuracy: 0.9287 - val_loss: 0.1510 - val_accuracy: 0.8750\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.1480 - accuracy: 0.9287 - val_loss: 0.1496 - val_accuracy: 0.8875\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1217 - accuracy: 0.9412 - val_loss: 0.0746 - val_accuracy: 0.9500\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1214 - accuracy: 0.9287 - val_loss: 0.1914 - val_accuracy: 0.8833\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1285 - accuracy: 0.9347 - val_loss: 0.0801 - val_accuracy: 0.9500\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1007 - accuracy: 0.9505 - val_loss: 0.1496 - val_accuracy: 0.9125\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1140 - accuracy: 0.9398 - val_loss: 0.0909 - val_accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1039 - accuracy: 0.9500 - val_loss: 0.1287 - val_accuracy: 0.9125\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0804 - accuracy: 0.9588 - val_loss: 0.4546 - val_accuracy: 0.7542\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0914 - accuracy: 0.9532 - val_loss: 0.0745 - val_accuracy: 0.9583\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0692 - accuracy: 0.9690 - val_loss: 0.0862 - val_accuracy: 0.9458\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0823 - accuracy: 0.9625 - val_loss: 0.1431 - val_accuracy: 0.9167\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0730 - accuracy: 0.9653 - val_loss: 0.0747 - val_accuracy: 0.9583\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0685 - accuracy: 0.9676 - val_loss: 0.0812 - val_accuracy: 0.9417\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0626 - accuracy: 0.9694 - val_loss: 0.0716 - val_accuracy: 0.9458\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0669 - accuracy: 0.9676 - val_loss: 0.0667 - val_accuracy: 0.9625\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0678 - accuracy: 0.9685 - val_loss: 0.0802 - val_accuracy: 0.9458\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0694 - accuracy: 0.9681 - val_loss: 0.0775 - val_accuracy: 0.9458\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0514 - accuracy: 0.9782 - val_loss: 0.0821 - val_accuracy: 0.9500\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0784 - accuracy: 0.9611 - val_loss: 0.0731 - val_accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0649 - accuracy: 0.9731 - val_loss: 0.0881 - val_accuracy: 0.9417\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0688 - accuracy: 0.9699 - val_loss: 0.0757 - val_accuracy: 0.9583\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0691 - accuracy: 0.9699 - val_loss: 0.0781 - val_accuracy: 0.9458\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0624 - accuracy: 0.9759 - val_loss: 0.0791 - val_accuracy: 0.9458\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0603 - accuracy: 0.9745 - val_loss: 0.0823 - val_accuracy: 0.9417\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0597 - accuracy: 0.9764 - val_loss: 0.0842 - val_accuracy: 0.9417\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0653 - accuracy: 0.9704 - val_loss: 0.0738 - val_accuracy: 0.9417\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0688 - accuracy: 0.9704 - val_loss: 0.0777 - val_accuracy: 0.9458\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0621 - accuracy: 0.9745 - val_loss: 0.0869 - val_accuracy: 0.9417\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0618 - accuracy: 0.9736 - val_loss: 0.0765 - val_accuracy: 0.9458\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0586 - accuracy: 0.9750 - val_loss: 0.0808 - val_accuracy: 0.9458\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0615 - accuracy: 0.9745 - val_loss: 0.0808 - val_accuracy: 0.9417\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0691 - accuracy: 0.9685 - val_loss: 0.0779 - val_accuracy: 0.9417\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0634 - accuracy: 0.9722 - val_loss: 0.0773 - val_accuracy: 0.9458\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0623 - accuracy: 0.9713 - val_loss: 0.0812 - val_accuracy: 0.9417\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0685 - accuracy: 0.9694 - val_loss: 0.0769 - val_accuracy: 0.9458\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0680 - accuracy: 0.9681 - val_loss: 0.0826 - val_accuracy: 0.9375\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0572 - accuracy: 0.9787 - val_loss: 0.0785 - val_accuracy: 0.9417\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0758 - val_accuracy: 0.9417\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0655 - accuracy: 0.9685 - val_loss: 0.0749 - val_accuracy: 0.9458\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0728 - accuracy: 0.9676 - val_loss: 0.0801 - val_accuracy: 0.9417\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0577 - accuracy: 0.9736 - val_loss: 0.0757 - val_accuracy: 0.9500\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0631 - accuracy: 0.9718 - val_loss: 0.0763 - val_accuracy: 0.9417\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0483 - accuracy: 0.9824 - val_loss: 0.0766 - val_accuracy: 0.9417\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0452 - accuracy: 0.9852 - val_loss: 0.0799 - val_accuracy: 0.9417\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0544 - accuracy: 0.9769 - val_loss: 0.0902 - val_accuracy: 0.9375\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0621 - accuracy: 0.9699 - val_loss: 0.0821 - val_accuracy: 0.9417\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0593 - accuracy: 0.9764 - val_loss: 0.0844 - val_accuracy: 0.9417\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0637 - accuracy: 0.9722 - val_loss: 0.0857 - val_accuracy: 0.9458\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0694 - accuracy: 0.9639 - val_loss: 0.0769 - val_accuracy: 0.9417\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0643 - accuracy: 0.9731 - val_loss: 0.0771 - val_accuracy: 0.9417\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0676 - accuracy: 0.9708 - val_loss: 0.0763 - val_accuracy: 0.9458\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0519 - accuracy: 0.9773 - val_loss: 0.0748 - val_accuracy: 0.9417\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0593 - accuracy: 0.9764 - val_loss: 0.0869 - val_accuracy: 0.9417\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0652 - accuracy: 0.9699 - val_loss: 0.0798 - val_accuracy: 0.9417\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0589 - accuracy: 0.9773 - val_loss: 0.0867 - val_accuracy: 0.9417\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0759 - accuracy: 0.9657 - val_loss: 0.0810 - val_accuracy: 0.9417\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0648 - accuracy: 0.9681 - val_loss: 0.0742 - val_accuracy: 0.9458\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0616 - accuracy: 0.9731 - val_loss: 0.0914 - val_accuracy: 0.9375\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0493 - accuracy: 0.9833 - val_loss: 0.0829 - val_accuracy: 0.9458\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.0852 - val_accuracy: 0.9458\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0686 - accuracy: 0.9741 - val_loss: 0.0767 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0533 - accuracy: 0.9815 - val_loss: 0.0804 - val_accuracy: 0.9458\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0680 - accuracy: 0.9699 - val_loss: 0.0851 - val_accuracy: 0.9417\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0544 - accuracy: 0.9764 - val_loss: 0.0826 - val_accuracy: 0.9417\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0622 - accuracy: 0.9745 - val_loss: 0.0823 - val_accuracy: 0.9458\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0614 - accuracy: 0.9699 - val_loss: 0.0808 - val_accuracy: 0.9458\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0618 - accuracy: 0.9699 - val_loss: 0.0763 - val_accuracy: 0.9417\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0460 - accuracy: 0.9852 - val_loss: 0.0892 - val_accuracy: 0.9417\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0638 - accuracy: 0.9699 - val_loss: 0.0832 - val_accuracy: 0.9417\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0594 - accuracy: 0.9750 - val_loss: 0.0844 - val_accuracy: 0.9417\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0538 - accuracy: 0.9796 - val_loss: 0.0799 - val_accuracy: 0.9458\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0527 - accuracy: 0.9773 - val_loss: 0.0778 - val_accuracy: 0.9417\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0551 - accuracy: 0.9764 - val_loss: 0.0749 - val_accuracy: 0.9458\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0696 - accuracy: 0.9713 - val_loss: 0.0805 - val_accuracy: 0.9417\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0599 - accuracy: 0.9718 - val_loss: 0.0735 - val_accuracy: 0.9458\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0551 - accuracy: 0.9764 - val_loss: 0.0796 - val_accuracy: 0.9417\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0473 - accuracy: 0.9847 - val_loss: 0.0850 - val_accuracy: 0.9417\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0605 - accuracy: 0.9759 - val_loss: 0.0812 - val_accuracy: 0.9458\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0662 - accuracy: 0.9699 - val_loss: 0.0776 - val_accuracy: 0.9417\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0625 - accuracy: 0.9727 - val_loss: 0.0781 - val_accuracy: 0.9417\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0589 - accuracy: 0.9745 - val_loss: 0.0825 - val_accuracy: 0.9458\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0519 - accuracy: 0.9782 - val_loss: 0.0811 - val_accuracy: 0.9417\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0519 - accuracy: 0.9801 - val_loss: 0.0771 - val_accuracy: 0.9417\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0664 - accuracy: 0.9718 - val_loss: 0.0815 - val_accuracy: 0.9458\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0704 - accuracy: 0.9690 - val_loss: 0.0766 - val_accuracy: 0.9500\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0548 - accuracy: 0.9769 - val_loss: 0.0819 - val_accuracy: 0.9458\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0597 - accuracy: 0.9731 - val_loss: 0.0749 - val_accuracy: 0.9458\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0543 - accuracy: 0.9778 - val_loss: 0.0716 - val_accuracy: 0.9458\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0663 - accuracy: 0.9708 - val_loss: 0.0740 - val_accuracy: 0.9458\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0593 - accuracy: 0.9773 - val_loss: 0.0780 - val_accuracy: 0.9417\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0693 - accuracy: 0.9690 - val_loss: 0.0814 - val_accuracy: 0.9458\n",
      "Score for fold 2: loss of 0.08136415481567383; accuracy of 94.58333253860474%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 111ms/step - loss: 0.3245 - accuracy: 0.8037 - val_loss: 4.5319 - val_accuracy: 0.6625\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.2405 - accuracy: 0.8565 - val_loss: 0.1064 - val_accuracy: 0.9458\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.2093 - accuracy: 0.8787 - val_loss: 0.1598 - val_accuracy: 0.9000\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1905 - accuracy: 0.8903 - val_loss: 0.1292 - val_accuracy: 0.9250\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 64s 119ms/step - loss: 0.1819 - accuracy: 0.8944 - val_loss: 0.6064 - val_accuracy: 0.7292\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1729 - accuracy: 0.9028 - val_loss: 0.1688 - val_accuracy: 0.8833\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1610 - accuracy: 0.9116 - val_loss: 0.6464 - val_accuracy: 0.6750\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1390 - accuracy: 0.9181 - val_loss: 0.0785 - val_accuracy: 0.9667\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1323 - accuracy: 0.9255 - val_loss: 0.1640 - val_accuracy: 0.9250\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 66s 123ms/step - loss: 0.1348 - accuracy: 0.9204 - val_loss: 0.0888 - val_accuracy: 0.9583\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 64s 119ms/step - loss: 0.1305 - accuracy: 0.9269 - val_loss: 0.1336 - val_accuracy: 0.9208\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.1207 - accuracy: 0.9333 - val_loss: 0.1525 - val_accuracy: 0.9000\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1084 - accuracy: 0.9403 - val_loss: 0.3059 - val_accuracy: 0.7917\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0950 - accuracy: 0.9500 - val_loss: 0.1714 - val_accuracy: 0.8958\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1009 - accuracy: 0.9486 - val_loss: 0.0768 - val_accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1020 - accuracy: 0.9454 - val_loss: 1.2080 - val_accuracy: 0.6625\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1029 - accuracy: 0.9486 - val_loss: 0.0685 - val_accuracy: 0.9583\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0927 - accuracy: 0.9486 - val_loss: 0.2130 - val_accuracy: 0.8958\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0878 - accuracy: 0.9514 - val_loss: 0.0926 - val_accuracy: 0.9375\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0859 - accuracy: 0.9532 - val_loss: 0.3298 - val_accuracy: 0.7958\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0672 - accuracy: 0.9699 - val_loss: 0.0913 - val_accuracy: 0.9625\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0710 - accuracy: 0.9662 - val_loss: 0.0847 - val_accuracy: 0.9667\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0655 - accuracy: 0.9699 - val_loss: 0.0723 - val_accuracy: 0.9708\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0651 - accuracy: 0.9704 - val_loss: 0.0721 - val_accuracy: 0.9750\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0602 - accuracy: 0.9671 - val_loss: 0.0784 - val_accuracy: 0.9708\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0584 - accuracy: 0.9741 - val_loss: 0.0732 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0614 - accuracy: 0.9741 - val_loss: 0.0793 - val_accuracy: 0.9583\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0530 - accuracy: 0.9764 - val_loss: 0.0765 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0618 - accuracy: 0.9704 - val_loss: 0.0607 - val_accuracy: 0.9708\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0598 - accuracy: 0.9699 - val_loss: 0.0643 - val_accuracy: 0.9792\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0667 - accuracy: 0.9694 - val_loss: 0.0695 - val_accuracy: 0.9708\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0531 - accuracy: 0.9750 - val_loss: 0.0636 - val_accuracy: 0.9792\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0513 - accuracy: 0.9759 - val_loss: 0.0686 - val_accuracy: 0.9750\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0539 - accuracy: 0.9769 - val_loss: 0.0710 - val_accuracy: 0.9792\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0451 - accuracy: 0.9838 - val_loss: 0.0723 - val_accuracy: 0.9625\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0526 - accuracy: 0.9755 - val_loss: 0.0722 - val_accuracy: 0.9708\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0574 - accuracy: 0.9745 - val_loss: 0.0758 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0533 - accuracy: 0.9787 - val_loss: 0.0762 - val_accuracy: 0.9708\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0463 - accuracy: 0.9801 - val_loss: 0.0670 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.0526 - accuracy: 0.9782 - val_loss: 0.0729 - val_accuracy: 0.9583\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0530 - accuracy: 0.9792 - val_loss: 0.0663 - val_accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0559 - accuracy: 0.9718 - val_loss: 0.0694 - val_accuracy: 0.9708\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0542 - accuracy: 0.9764 - val_loss: 0.0784 - val_accuracy: 0.9542\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0584 - accuracy: 0.9708 - val_loss: 0.0677 - val_accuracy: 0.9708\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0538 - accuracy: 0.9801 - val_loss: 0.0734 - val_accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0576 - accuracy: 0.9727 - val_loss: 0.0734 - val_accuracy: 0.9583\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0566 - accuracy: 0.9750 - val_loss: 0.0694 - val_accuracy: 0.9708\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0571 - accuracy: 0.9731 - val_loss: 0.0684 - val_accuracy: 0.9708\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0525 - accuracy: 0.9736 - val_loss: 0.0702 - val_accuracy: 0.9708\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0538 - accuracy: 0.9801 - val_loss: 0.0671 - val_accuracy: 0.9708\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0561 - accuracy: 0.9718 - val_loss: 0.0738 - val_accuracy: 0.9625\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0515 - accuracy: 0.9741 - val_loss: 0.0656 - val_accuracy: 0.9708\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0480 - accuracy: 0.9810 - val_loss: 0.0663 - val_accuracy: 0.9708\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0631 - val_accuracy: 0.9708\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0555 - accuracy: 0.9745 - val_loss: 0.0711 - val_accuracy: 0.9625\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0578 - accuracy: 0.9727 - val_loss: 0.0653 - val_accuracy: 0.9708\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0469 - accuracy: 0.9815 - val_loss: 0.0656 - val_accuracy: 0.9792\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0487 - accuracy: 0.9769 - val_loss: 0.0799 - val_accuracy: 0.9542\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0559 - accuracy: 0.9755 - val_loss: 0.0754 - val_accuracy: 0.9583\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0621 - accuracy: 0.9681 - val_loss: 0.0689 - val_accuracy: 0.9708\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0524 - accuracy: 0.9764 - val_loss: 0.0697 - val_accuracy: 0.9667\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0607 - accuracy: 0.9685 - val_loss: 0.0668 - val_accuracy: 0.9750\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0530 - accuracy: 0.9745 - val_loss: 0.0790 - val_accuracy: 0.9583\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0684 - accuracy: 0.9653 - val_loss: 0.0709 - val_accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0600 - accuracy: 0.9685 - val_loss: 0.0663 - val_accuracy: 0.9708\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0559 - accuracy: 0.9778 - val_loss: 0.0717 - val_accuracy: 0.9625\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0479 - accuracy: 0.9792 - val_loss: 0.0661 - val_accuracy: 0.9708\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0491 - accuracy: 0.9815 - val_loss: 0.0685 - val_accuracy: 0.9708\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0656 - accuracy: 0.9685 - val_loss: 0.0715 - val_accuracy: 0.9708\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0592 - accuracy: 0.9722 - val_loss: 0.0682 - val_accuracy: 0.9750\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0464 - accuracy: 0.9824 - val_loss: 0.0685 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0518 - accuracy: 0.9782 - val_loss: 0.0679 - val_accuracy: 0.9750\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0465 - accuracy: 0.9810 - val_loss: 0.0645 - val_accuracy: 0.9708\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0546 - accuracy: 0.9699 - val_loss: 0.0671 - val_accuracy: 0.9750\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0510 - accuracy: 0.9773 - val_loss: 0.0740 - val_accuracy: 0.9625\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0540 - accuracy: 0.9782 - val_loss: 0.0691 - val_accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0448 - accuracy: 0.9829 - val_loss: 0.0644 - val_accuracy: 0.9708\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0483 - accuracy: 0.9764 - val_loss: 0.0664 - val_accuracy: 0.9708\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0484 - accuracy: 0.9796 - val_loss: 0.0639 - val_accuracy: 0.9708\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0529 - accuracy: 0.9755 - val_loss: 0.0684 - val_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0552 - accuracy: 0.9787 - val_loss: 0.0751 - val_accuracy: 0.9583\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0514 - accuracy: 0.9741 - val_loss: 0.0674 - val_accuracy: 0.9750\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0583 - accuracy: 0.9731 - val_loss: 0.0759 - val_accuracy: 0.9583\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.0664 - val_accuracy: 0.9750\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0540 - accuracy: 0.9741 - val_loss: 0.0732 - val_accuracy: 0.9583\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0661 - accuracy: 0.9708 - val_loss: 0.0749 - val_accuracy: 0.9583\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0563 - accuracy: 0.9745 - val_loss: 0.0699 - val_accuracy: 0.9708\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0503 - accuracy: 0.9745 - val_loss: 0.0672 - val_accuracy: 0.9708\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0576 - accuracy: 0.9745 - val_loss: 0.0651 - val_accuracy: 0.9708\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0477 - accuracy: 0.9806 - val_loss: 0.0687 - val_accuracy: 0.9750\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0615 - accuracy: 0.9690 - val_loss: 0.0737 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0589 - accuracy: 0.9727 - val_loss: 0.0677 - val_accuracy: 0.9708\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0553 - accuracy: 0.9718 - val_loss: 0.0690 - val_accuracy: 0.9708\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0584 - accuracy: 0.9718 - val_loss: 0.0657 - val_accuracy: 0.9708\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0558 - accuracy: 0.9722 - val_loss: 0.0651 - val_accuracy: 0.9792\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0583 - accuracy: 0.9708 - val_loss: 0.0736 - val_accuracy: 0.9625\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0606 - accuracy: 0.9708 - val_loss: 0.0708 - val_accuracy: 0.9667\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0644 - accuracy: 0.9681 - val_loss: 0.0678 - val_accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0559 - accuracy: 0.9745 - val_loss: 0.0689 - val_accuracy: 0.9708\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0581 - accuracy: 0.9708 - val_loss: 0.0685 - val_accuracy: 0.9708\n",
      "Score for fold 3: loss of 0.06849122047424316; accuracy of 97.08333611488342%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 111ms/step - loss: 0.3424 - accuracy: 0.7815 - val_loss: 3.9110 - val_accuracy: 0.6625\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2539 - accuracy: 0.8477 - val_loss: 0.2833 - val_accuracy: 0.8125\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2425 - accuracy: 0.8514 - val_loss: 0.0654 - val_accuracy: 0.9833\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2115 - accuracy: 0.8718 - val_loss: 0.0914 - val_accuracy: 0.9792\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1938 - accuracy: 0.8787 - val_loss: 0.0706 - val_accuracy: 0.9833\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1687 - accuracy: 0.9134 - val_loss: 0.0595 - val_accuracy: 0.9792\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1596 - accuracy: 0.9116 - val_loss: 0.1781 - val_accuracy: 0.8625\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1474 - accuracy: 0.9111 - val_loss: 0.0920 - val_accuracy: 0.9333\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1687 - accuracy: 0.9042 - val_loss: 0.0403 - val_accuracy: 0.9958\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1483 - accuracy: 0.9130 - val_loss: 0.0520 - val_accuracy: 0.9833\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1420 - accuracy: 0.9153 - val_loss: 0.2250 - val_accuracy: 0.7917\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1187 - accuracy: 0.9370 - val_loss: 0.0410 - val_accuracy: 0.9917\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.1181 - accuracy: 0.9403 - val_loss: 0.0661 - val_accuracy: 0.9708\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1178 - accuracy: 0.9398 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1048 - accuracy: 0.9486 - val_loss: 0.1106 - val_accuracy: 0.9208\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1036 - accuracy: 0.9463 - val_loss: 0.7016 - val_accuracy: 0.6917\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1005 - accuracy: 0.9463 - val_loss: 0.5762 - val_accuracy: 0.6917\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0895 - accuracy: 0.9574 - val_loss: 0.1668 - val_accuracy: 0.8750\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0830 - accuracy: 0.9602 - val_loss: 0.0581 - val_accuracy: 0.9875\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0864 - accuracy: 0.9569 - val_loss: 0.0519 - val_accuracy: 0.9875\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0676 - accuracy: 0.9708 - val_loss: 0.1115 - val_accuracy: 0.9292\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0725 - accuracy: 0.9662 - val_loss: 0.0434 - val_accuracy: 0.9833\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0708 - accuracy: 0.9667 - val_loss: 0.0469 - val_accuracy: 0.9792\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0718 - accuracy: 0.9671 - val_loss: 0.0787 - val_accuracy: 0.9542\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0870 - accuracy: 0.9565 - val_loss: 0.0690 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0642 - accuracy: 0.9722 - val_loss: 0.0366 - val_accuracy: 0.9917\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0701 - accuracy: 0.9634 - val_loss: 0.0446 - val_accuracy: 0.9833\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0644 - accuracy: 0.9741 - val_loss: 0.0785 - val_accuracy: 0.9458\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0741 - accuracy: 0.9662 - val_loss: 0.0780 - val_accuracy: 0.9500\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0608 - accuracy: 0.9704 - val_loss: 0.0435 - val_accuracy: 0.9833\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0679 - accuracy: 0.9681 - val_loss: 0.0389 - val_accuracy: 0.9875\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0718 - accuracy: 0.9681 - val_loss: 0.0437 - val_accuracy: 0.9750\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0775 - accuracy: 0.9593 - val_loss: 0.0660 - val_accuracy: 0.9625\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0551 - accuracy: 0.9782 - val_loss: 0.0516 - val_accuracy: 0.9750\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0644 - accuracy: 0.9731 - val_loss: 0.0674 - val_accuracy: 0.9583\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0651 - accuracy: 0.9731 - val_loss: 0.0471 - val_accuracy: 0.9833\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0674 - accuracy: 0.9694 - val_loss: 0.0432 - val_accuracy: 0.9875\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0666 - accuracy: 0.9671 - val_loss: 0.0499 - val_accuracy: 0.9750\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0464 - accuracy: 0.9819 - val_loss: 0.0470 - val_accuracy: 0.9750\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0658 - accuracy: 0.9704 - val_loss: 0.0442 - val_accuracy: 0.9833\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0600 - accuracy: 0.9713 - val_loss: 0.0510 - val_accuracy: 0.9750\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0654 - accuracy: 0.9713 - val_loss: 0.0609 - val_accuracy: 0.9708\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0624 - accuracy: 0.9750 - val_loss: 0.0515 - val_accuracy: 0.9708\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0604 - accuracy: 0.9792 - val_loss: 0.0545 - val_accuracy: 0.9750\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0683 - accuracy: 0.9690 - val_loss: 0.0426 - val_accuracy: 0.9833\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0656 - accuracy: 0.9718 - val_loss: 0.0549 - val_accuracy: 0.9750\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0690 - accuracy: 0.9713 - val_loss: 0.0500 - val_accuracy: 0.9792\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0635 - accuracy: 0.9722 - val_loss: 0.0516 - val_accuracy: 0.9708\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0637 - accuracy: 0.9704 - val_loss: 0.0510 - val_accuracy: 0.9750\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0676 - accuracy: 0.9699 - val_loss: 0.0575 - val_accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0574 - accuracy: 0.9764 - val_loss: 0.0465 - val_accuracy: 0.9792\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0630 - accuracy: 0.9713 - val_loss: 0.0519 - val_accuracy: 0.9750\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0625 - accuracy: 0.9699 - val_loss: 0.0642 - val_accuracy: 0.9708\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0595 - accuracy: 0.9745 - val_loss: 0.0505 - val_accuracy: 0.9708\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.0545 - val_accuracy: 0.9708\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0708 - accuracy: 0.9657 - val_loss: 0.0565 - val_accuracy: 0.9750\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0612 - accuracy: 0.9722 - val_loss: 0.0500 - val_accuracy: 0.9750\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0707 - accuracy: 0.9699 - val_loss: 0.0513 - val_accuracy: 0.9750\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0635 - accuracy: 0.9727 - val_loss: 0.0570 - val_accuracy: 0.9750\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0685 - accuracy: 0.9690 - val_loss: 0.0535 - val_accuracy: 0.9750\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0721 - accuracy: 0.9657 - val_loss: 0.0480 - val_accuracy: 0.9750\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0547 - accuracy: 0.9801 - val_loss: 0.0491 - val_accuracy: 0.9792\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0690 - accuracy: 0.9685 - val_loss: 0.0441 - val_accuracy: 0.9792\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0677 - accuracy: 0.9676 - val_loss: 0.0499 - val_accuracy: 0.9750\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0663 - accuracy: 0.9699 - val_loss: 0.0511 - val_accuracy: 0.9750\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0747 - accuracy: 0.9630 - val_loss: 0.0486 - val_accuracy: 0.9792\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0711 - accuracy: 0.9667 - val_loss: 0.0444 - val_accuracy: 0.9833\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0645 - accuracy: 0.9713 - val_loss: 0.0460 - val_accuracy: 0.9792\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0750 - accuracy: 0.9681 - val_loss: 0.0553 - val_accuracy: 0.9750\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0725 - accuracy: 0.9676 - val_loss: 0.0498 - val_accuracy: 0.9792\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0599 - accuracy: 0.9755 - val_loss: 0.0482 - val_accuracy: 0.9750\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0681 - accuracy: 0.9690 - val_loss: 0.0460 - val_accuracy: 0.9792\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0544 - accuracy: 0.9782 - val_loss: 0.0454 - val_accuracy: 0.9792\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0626 - accuracy: 0.9731 - val_loss: 0.0516 - val_accuracy: 0.9792\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0657 - accuracy: 0.9731 - val_loss: 0.0499 - val_accuracy: 0.9792\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0599 - accuracy: 0.9731 - val_loss: 0.0537 - val_accuracy: 0.9750\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0610 - accuracy: 0.9722 - val_loss: 0.0519 - val_accuracy: 0.9750\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0556 - accuracy: 0.9755 - val_loss: 0.0494 - val_accuracy: 0.9792\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0602 - accuracy: 0.9708 - val_loss: 0.0465 - val_accuracy: 0.9792\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0740 - accuracy: 0.9648 - val_loss: 0.0582 - val_accuracy: 0.9708\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0670 - accuracy: 0.9713 - val_loss: 0.0534 - val_accuracy: 0.9750\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0634 - accuracy: 0.9718 - val_loss: 0.0630 - val_accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0600 - accuracy: 0.9755 - val_loss: 0.0540 - val_accuracy: 0.9750\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0599 - accuracy: 0.9764 - val_loss: 0.0553 - val_accuracy: 0.9708\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0619 - accuracy: 0.9745 - val_loss: 0.0506 - val_accuracy: 0.9708\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0619 - accuracy: 0.9731 - val_loss: 0.0557 - val_accuracy: 0.9708\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0652 - accuracy: 0.9681 - val_loss: 0.0538 - val_accuracy: 0.9750\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0644 - accuracy: 0.9676 - val_loss: 0.0563 - val_accuracy: 0.9708\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0650 - accuracy: 0.9769 - val_loss: 0.0562 - val_accuracy: 0.9750\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0585 - accuracy: 0.9736 - val_loss: 0.0502 - val_accuracy: 0.9750\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0549 - accuracy: 0.9755 - val_loss: 0.0550 - val_accuracy: 0.9708\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0694 - accuracy: 0.9662 - val_loss: 0.0597 - val_accuracy: 0.9708\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0565 - accuracy: 0.9750 - val_loss: 0.0519 - val_accuracy: 0.9708\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0584 - accuracy: 0.9741 - val_loss: 0.0526 - val_accuracy: 0.9750\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0550 - accuracy: 0.9778 - val_loss: 0.0494 - val_accuracy: 0.9750\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0617 - accuracy: 0.9694 - val_loss: 0.0521 - val_accuracy: 0.9750\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0608 - accuracy: 0.9731 - val_loss: 0.0564 - val_accuracy: 0.9708\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0576 - accuracy: 0.9778 - val_loss: 0.0458 - val_accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0557 - accuracy: 0.9764 - val_loss: 0.0488 - val_accuracy: 0.9750\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0655 - accuracy: 0.9704 - val_loss: 0.0544 - val_accuracy: 0.9708\n",
      "Score for fold 4: loss of 0.05439552292227745; accuracy of 97.08333611488342%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 68s 110ms/step - loss: 0.3360 - accuracy: 0.8014 - val_loss: 0.2149 - val_accuracy: 0.9083\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.2571 - accuracy: 0.8454 - val_loss: 0.1950 - val_accuracy: 0.8875\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2266 - accuracy: 0.8620 - val_loss: 0.1249 - val_accuracy: 0.9333\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1994 - accuracy: 0.8731 - val_loss: 0.1489 - val_accuracy: 0.9208\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1761 - accuracy: 0.8944 - val_loss: 0.1502 - val_accuracy: 0.9083\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1578 - accuracy: 0.9009 - val_loss: 0.1654 - val_accuracy: 0.8917\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1477 - accuracy: 0.9134 - val_loss: 0.1055 - val_accuracy: 0.9458\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1368 - accuracy: 0.9236 - val_loss: 0.1719 - val_accuracy: 0.8875\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1340 - accuracy: 0.9199 - val_loss: 0.0751 - val_accuracy: 0.9542\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1288 - accuracy: 0.9301 - val_loss: 0.0749 - val_accuracy: 0.9667\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1154 - accuracy: 0.9333 - val_loss: 0.1092 - val_accuracy: 0.9500\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1174 - accuracy: 0.9306 - val_loss: 0.1287 - val_accuracy: 0.9125\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1113 - accuracy: 0.9356 - val_loss: 0.1608 - val_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0935 - accuracy: 0.9519 - val_loss: 0.0900 - val_accuracy: 0.9500\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0873 - accuracy: 0.9565 - val_loss: 0.0730 - val_accuracy: 0.9667\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0925 - accuracy: 0.9491 - val_loss: 0.1506 - val_accuracy: 0.8917\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0838 - accuracy: 0.9532 - val_loss: 0.3527 - val_accuracy: 0.7708\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0803 - accuracy: 0.9551 - val_loss: 0.0671 - val_accuracy: 0.9792\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0842 - accuracy: 0.9546 - val_loss: 0.0656 - val_accuracy: 0.9542\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0717 - accuracy: 0.9611 - val_loss: 0.0842 - val_accuracy: 0.9417\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0658 - accuracy: 0.9639 - val_loss: 0.0515 - val_accuracy: 0.9875\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0622 - accuracy: 0.9685 - val_loss: 0.0496 - val_accuracy: 0.9875\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0520 - accuracy: 0.9745 - val_loss: 0.0503 - val_accuracy: 0.9833\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0464 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9833\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0598 - accuracy: 0.9699 - val_loss: 0.0476 - val_accuracy: 0.9833\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0488 - accuracy: 0.9801 - val_loss: 0.0466 - val_accuracy: 0.9833\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0562 - accuracy: 0.9745 - val_loss: 0.0487 - val_accuracy: 0.9833\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0594 - accuracy: 0.9681 - val_loss: 0.0494 - val_accuracy: 0.9833\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0487 - accuracy: 0.9773 - val_loss: 0.0544 - val_accuracy: 0.9833\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0522 - accuracy: 0.9764 - val_loss: 0.0483 - val_accuracy: 0.9833\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0485 - accuracy: 0.9787 - val_loss: 0.0477 - val_accuracy: 0.9833\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0479 - accuracy: 0.9819 - val_loss: 0.0487 - val_accuracy: 0.9833\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0495 - accuracy: 0.9778 - val_loss: 0.0483 - val_accuracy: 0.9833\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0513 - accuracy: 0.9755 - val_loss: 0.0507 - val_accuracy: 0.9750\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0599 - accuracy: 0.9699 - val_loss: 0.0508 - val_accuracy: 0.9833\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0482 - accuracy: 0.9745 - val_loss: 0.0507 - val_accuracy: 0.9792\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0517 - accuracy: 0.9764 - val_loss: 0.0492 - val_accuracy: 0.9833\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0470 - accuracy: 0.9796 - val_loss: 0.0492 - val_accuracy: 0.9833\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0513 - accuracy: 0.9778 - val_loss: 0.0502 - val_accuracy: 0.9792\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0588 - accuracy: 0.9741 - val_loss: 0.0467 - val_accuracy: 0.9833\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0496 - accuracy: 0.9810 - val_loss: 0.0506 - val_accuracy: 0.9833\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0474 - accuracy: 0.9745 - val_loss: 0.0473 - val_accuracy: 0.9792\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0519 - accuracy: 0.9782 - val_loss: 0.0465 - val_accuracy: 0.9792\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0544 - accuracy: 0.9731 - val_loss: 0.0482 - val_accuracy: 0.9833\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0502 - accuracy: 0.9755 - val_loss: 0.0503 - val_accuracy: 0.9792\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0476 - accuracy: 0.9764 - val_loss: 0.0507 - val_accuracy: 0.9833\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0538 - accuracy: 0.9792 - val_loss: 0.0495 - val_accuracy: 0.9833\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0533 - accuracy: 0.9741 - val_loss: 0.0481 - val_accuracy: 0.9833\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0508 - accuracy: 0.9736 - val_loss: 0.0503 - val_accuracy: 0.9833\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0525 - accuracy: 0.9764 - val_loss: 0.0469 - val_accuracy: 0.9792\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0455 - accuracy: 0.9806 - val_loss: 0.0489 - val_accuracy: 0.9792\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0580 - accuracy: 0.9694 - val_loss: 0.0519 - val_accuracy: 0.9792\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0509 - accuracy: 0.9750 - val_loss: 0.0481 - val_accuracy: 0.9833\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0473 - accuracy: 0.9819 - val_loss: 0.0505 - val_accuracy: 0.9792\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0527 - accuracy: 0.9745 - val_loss: 0.0511 - val_accuracy: 0.9833\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0553 - accuracy: 0.9759 - val_loss: 0.0488 - val_accuracy: 0.9792\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0426 - accuracy: 0.9852 - val_loss: 0.0483 - val_accuracy: 0.9792\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0453 - accuracy: 0.9819 - val_loss: 0.0490 - val_accuracy: 0.9833\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0464 - accuracy: 0.9792 - val_loss: 0.0472 - val_accuracy: 0.9833\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0466 - accuracy: 0.9792 - val_loss: 0.0481 - val_accuracy: 0.9833\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0524 - accuracy: 0.9755 - val_loss: 0.0491 - val_accuracy: 0.9792\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0503 - accuracy: 0.9750 - val_loss: 0.0486 - val_accuracy: 0.9833\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0523 - accuracy: 0.9796 - val_loss: 0.0506 - val_accuracy: 0.9833\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0542 - accuracy: 0.9759 - val_loss: 0.0489 - val_accuracy: 0.9833\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0507 - accuracy: 0.9778 - val_loss: 0.0479 - val_accuracy: 0.9833\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0405 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9792\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0532 - accuracy: 0.9750 - val_loss: 0.0508 - val_accuracy: 0.9792\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0536 - accuracy: 0.9745 - val_loss: 0.0472 - val_accuracy: 0.9833\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0430 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9833\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0402 - accuracy: 0.9838 - val_loss: 0.0484 - val_accuracy: 0.9833\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0434 - accuracy: 0.9796 - val_loss: 0.0460 - val_accuracy: 0.9833\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0485 - accuracy: 0.9787 - val_loss: 0.0472 - val_accuracy: 0.9833\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0510 - accuracy: 0.9806 - val_loss: 0.0488 - val_accuracy: 0.9833\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0489 - accuracy: 0.9764 - val_loss: 0.0479 - val_accuracy: 0.9833\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0500 - accuracy: 0.9755 - val_loss: 0.0476 - val_accuracy: 0.9792\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0470 - accuracy: 0.9806 - val_loss: 0.0492 - val_accuracy: 0.9792\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0503 - accuracy: 0.9782 - val_loss: 0.0510 - val_accuracy: 0.9833\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0472 - accuracy: 0.9796 - val_loss: 0.0495 - val_accuracy: 0.9792\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0495 - accuracy: 0.9778 - val_loss: 0.0480 - val_accuracy: 0.9833\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0492 - accuracy: 0.9759 - val_loss: 0.0507 - val_accuracy: 0.9792\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0473 - accuracy: 0.9801 - val_loss: 0.0473 - val_accuracy: 0.9833\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0483 - accuracy: 0.9806 - val_loss: 0.0505 - val_accuracy: 0.9792\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0454 - accuracy: 0.9773 - val_loss: 0.0510 - val_accuracy: 0.9792\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0434 - accuracy: 0.9801 - val_loss: 0.0509 - val_accuracy: 0.9833\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0434 - accuracy: 0.9806 - val_loss: 0.0506 - val_accuracy: 0.9792\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0475 - accuracy: 0.9796 - val_loss: 0.0494 - val_accuracy: 0.9833\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0491 - accuracy: 0.9806 - val_loss: 0.0479 - val_accuracy: 0.9833\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0629 - accuracy: 0.9694 - val_loss: 0.0483 - val_accuracy: 0.9833\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0458 - accuracy: 0.9796 - val_loss: 0.0469 - val_accuracy: 0.9792\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0534 - accuracy: 0.9787 - val_loss: 0.0490 - val_accuracy: 0.9833\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0467 - accuracy: 0.9819 - val_loss: 0.0504 - val_accuracy: 0.9833\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0519 - accuracy: 0.9773 - val_loss: 0.0499 - val_accuracy: 0.9792\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0497 - accuracy: 0.9796 - val_loss: 0.0488 - val_accuracy: 0.9833\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0461 - accuracy: 0.9801 - val_loss: 0.0461 - val_accuracy: 0.9833\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0539 - accuracy: 0.9764 - val_loss: 0.0487 - val_accuracy: 0.9833\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0493 - accuracy: 0.9778 - val_loss: 0.0491 - val_accuracy: 0.9833\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0470 - accuracy: 0.9787 - val_loss: 0.0471 - val_accuracy: 0.9833\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0510 - accuracy: 0.9745 - val_loss: 0.0483 - val_accuracy: 0.9833\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0500 - accuracy: 0.9764 - val_loss: 0.0492 - val_accuracy: 0.9833\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0547 - accuracy: 0.9745 - val_loss: 0.0509 - val_accuracy: 0.9792\n",
      "Score for fold 5: loss of 0.050885576754808426; accuracy of 97.91666865348816%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 67s 110ms/step - loss: 0.3132 - accuracy: 0.8060 - val_loss: 0.3447 - val_accuracy: 0.7417\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.2650 - accuracy: 0.8380 - val_loss: 0.6903 - val_accuracy: 0.7083\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.2355 - accuracy: 0.8556 - val_loss: 0.1331 - val_accuracy: 0.9167\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1800 - accuracy: 0.8889 - val_loss: 0.0722 - val_accuracy: 0.9625\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1842 - accuracy: 0.8958 - val_loss: 0.3752 - val_accuracy: 0.7333\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1797 - accuracy: 0.8907 - val_loss: 2.8698 - val_accuracy: 0.6917\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1553 - accuracy: 0.9079 - val_loss: 0.0567 - val_accuracy: 0.9667\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1280 - accuracy: 0.9319 - val_loss: 0.0779 - val_accuracy: 0.9500\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1336 - accuracy: 0.9245 - val_loss: 0.0801 - val_accuracy: 0.9667\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1362 - accuracy: 0.9222 - val_loss: 0.3269 - val_accuracy: 0.8167\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1135 - accuracy: 0.9343 - val_loss: 0.0752 - val_accuracy: 0.9542\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1090 - accuracy: 0.9361 - val_loss: 0.9273 - val_accuracy: 0.6917\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1169 - accuracy: 0.9324 - val_loss: 0.1235 - val_accuracy: 0.9125\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1014 - accuracy: 0.9468 - val_loss: 0.5245 - val_accuracy: 0.8958\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0947 - accuracy: 0.9500 - val_loss: 0.0978 - val_accuracy: 0.9500\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0990 - accuracy: 0.9444 - val_loss: 0.0664 - val_accuracy: 0.9500\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0820 - accuracy: 0.9597 - val_loss: 0.1025 - val_accuracy: 0.9292\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0890 - accuracy: 0.9458 - val_loss: 0.2259 - val_accuracy: 0.8583\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0777 - accuracy: 0.9616 - val_loss: 0.0921 - val_accuracy: 0.9375\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0820 - accuracy: 0.9565 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0685 - accuracy: 0.9671 - val_loss: 0.0410 - val_accuracy: 0.9833\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0624 - accuracy: 0.9727 - val_loss: 0.0425 - val_accuracy: 0.9833\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0635 - accuracy: 0.9681 - val_loss: 0.0393 - val_accuracy: 0.9833\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0610 - accuracy: 0.9722 - val_loss: 0.0393 - val_accuracy: 0.9792\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0539 - accuracy: 0.9769 - val_loss: 0.0426 - val_accuracy: 0.9750\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0512 - accuracy: 0.9792 - val_loss: 0.0393 - val_accuracy: 0.9792\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0552 - accuracy: 0.9782 - val_loss: 0.0397 - val_accuracy: 0.9750\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0466 - accuracy: 0.9806 - val_loss: 0.0358 - val_accuracy: 0.9833\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0493 - accuracy: 0.9819 - val_loss: 0.0369 - val_accuracy: 0.9792\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0437 - accuracy: 0.9847 - val_loss: 0.0373 - val_accuracy: 0.9833\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0486 - accuracy: 0.9806 - val_loss: 0.0410 - val_accuracy: 0.9833\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0573 - accuracy: 0.9708 - val_loss: 0.0442 - val_accuracy: 0.9750\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0564 - accuracy: 0.9764 - val_loss: 0.0409 - val_accuracy: 0.9792\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0504 - accuracy: 0.9796 - val_loss: 0.0376 - val_accuracy: 0.9792\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0508 - accuracy: 0.9764 - val_loss: 0.0379 - val_accuracy: 0.9792\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0424 - val_accuracy: 0.9792\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0479 - accuracy: 0.9787 - val_loss: 0.0392 - val_accuracy: 0.9750\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0475 - accuracy: 0.9792 - val_loss: 0.0391 - val_accuracy: 0.9792\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0501 - accuracy: 0.9787 - val_loss: 0.0398 - val_accuracy: 0.9792\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0400 - accuracy: 0.9870 - val_loss: 0.0479 - val_accuracy: 0.9750\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0593 - accuracy: 0.9704 - val_loss: 0.0395 - val_accuracy: 0.9708\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0531 - accuracy: 0.9796 - val_loss: 0.0405 - val_accuracy: 0.9750\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0541 - accuracy: 0.9731 - val_loss: 0.0409 - val_accuracy: 0.9750\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0443 - accuracy: 0.9787 - val_loss: 0.0407 - val_accuracy: 0.9792\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0592 - accuracy: 0.9708 - val_loss: 0.0432 - val_accuracy: 0.9792\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0540 - accuracy: 0.9713 - val_loss: 0.0390 - val_accuracy: 0.9750\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0463 - accuracy: 0.9796 - val_loss: 0.0403 - val_accuracy: 0.9792\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0487 - accuracy: 0.9769 - val_loss: 0.0406 - val_accuracy: 0.9750\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0462 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9792\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0514 - accuracy: 0.9759 - val_loss: 0.0400 - val_accuracy: 0.9792\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0612 - accuracy: 0.9690 - val_loss: 0.0383 - val_accuracy: 0.9792\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0488 - accuracy: 0.9796 - val_loss: 0.0424 - val_accuracy: 0.9792\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0501 - accuracy: 0.9755 - val_loss: 0.0388 - val_accuracy: 0.9792\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0440 - accuracy: 0.9833 - val_loss: 0.0420 - val_accuracy: 0.9792\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0502 - accuracy: 0.9769 - val_loss: 0.0389 - val_accuracy: 0.9792\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0515 - accuracy: 0.9764 - val_loss: 0.0377 - val_accuracy: 0.9792\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0449 - accuracy: 0.9829 - val_loss: 0.0379 - val_accuracy: 0.9792\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0426 - accuracy: 0.9847 - val_loss: 0.0407 - val_accuracy: 0.9792\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0426 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9750\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0474 - accuracy: 0.9796 - val_loss: 0.0392 - val_accuracy: 0.9750\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0494 - accuracy: 0.9764 - val_loss: 0.0417 - val_accuracy: 0.9792\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0435 - accuracy: 0.9833 - val_loss: 0.0398 - val_accuracy: 0.9792\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0406 - accuracy: 0.9847 - val_loss: 0.0427 - val_accuracy: 0.9792\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0513 - accuracy: 0.9764 - val_loss: 0.0402 - val_accuracy: 0.9792\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0475 - accuracy: 0.9801 - val_loss: 0.0393 - val_accuracy: 0.9792\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0481 - accuracy: 0.9819 - val_loss: 0.0393 - val_accuracy: 0.9750\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0520 - accuracy: 0.9787 - val_loss: 0.0404 - val_accuracy: 0.9792\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0472 - accuracy: 0.9769 - val_loss: 0.0387 - val_accuracy: 0.9792\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0443 - accuracy: 0.9806 - val_loss: 0.0401 - val_accuracy: 0.9792\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0474 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9750\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0462 - accuracy: 0.9819 - val_loss: 0.0404 - val_accuracy: 0.9792\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0439 - accuracy: 0.9806 - val_loss: 0.0399 - val_accuracy: 0.9792\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0493 - accuracy: 0.9782 - val_loss: 0.0398 - val_accuracy: 0.9750\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0521 - accuracy: 0.9755 - val_loss: 0.0394 - val_accuracy: 0.9792\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0401 - accuracy: 0.9880 - val_loss: 0.0402 - val_accuracy: 0.9792\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0482 - accuracy: 0.9755 - val_loss: 0.0402 - val_accuracy: 0.9750\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0529 - accuracy: 0.9741 - val_loss: 0.0402 - val_accuracy: 0.9750\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0552 - accuracy: 0.9782 - val_loss: 0.0385 - val_accuracy: 0.9750\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0475 - accuracy: 0.9801 - val_loss: 0.0395 - val_accuracy: 0.9750\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0521 - accuracy: 0.9773 - val_loss: 0.0377 - val_accuracy: 0.9750\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0466 - accuracy: 0.9833 - val_loss: 0.0386 - val_accuracy: 0.9750\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0495 - accuracy: 0.9806 - val_loss: 0.0386 - val_accuracy: 0.9792\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0425 - accuracy: 0.9856 - val_loss: 0.0379 - val_accuracy: 0.9750\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0621 - accuracy: 0.9653 - val_loss: 0.0391 - val_accuracy: 0.9750\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0487 - accuracy: 0.9778 - val_loss: 0.0393 - val_accuracy: 0.9792\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0637 - accuracy: 0.9681 - val_loss: 0.0391 - val_accuracy: 0.9750\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0552 - accuracy: 0.9764 - val_loss: 0.0425 - val_accuracy: 0.9750\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0498 - accuracy: 0.9782 - val_loss: 0.0409 - val_accuracy: 0.9792\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0464 - accuracy: 0.9787 - val_loss: 0.0396 - val_accuracy: 0.9750\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0500 - accuracy: 0.9773 - val_loss: 0.0398 - val_accuracy: 0.9792\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0379 - accuracy: 0.9833 - val_loss: 0.0399 - val_accuracy: 0.9792\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0480 - accuracy: 0.9838 - val_loss: 0.0405 - val_accuracy: 0.9792\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0450 - accuracy: 0.9806 - val_loss: 0.0378 - val_accuracy: 0.9792\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0470 - accuracy: 0.9829 - val_loss: 0.0396 - val_accuracy: 0.9792\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0473 - accuracy: 0.9769 - val_loss: 0.0384 - val_accuracy: 0.9750\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0533 - accuracy: 0.9764 - val_loss: 0.0394 - val_accuracy: 0.9750\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0433 - accuracy: 0.9870 - val_loss: 0.0389 - val_accuracy: 0.9792\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0518 - accuracy: 0.9764 - val_loss: 0.0382 - val_accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0468 - accuracy: 0.9810 - val_loss: 0.0387 - val_accuracy: 0.9750\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0528 - accuracy: 0.9759 - val_loss: 0.0401 - val_accuracy: 0.9792\n",
      "Score for fold 6: loss of 0.04010511189699173; accuracy of 97.91666865348816%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 67s 109ms/step - loss: 0.3387 - accuracy: 0.7884 - val_loss: 0.1783 - val_accuracy: 0.9042\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.2345 - accuracy: 0.8639 - val_loss: 0.1243 - val_accuracy: 0.9375\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.2034 - accuracy: 0.8815 - val_loss: 0.0929 - val_accuracy: 0.9500\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.2049 - accuracy: 0.8792 - val_loss: 0.0847 - val_accuracy: 0.9625\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1728 - accuracy: 0.9051 - val_loss: 0.2100 - val_accuracy: 0.8625\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1679 - accuracy: 0.9074 - val_loss: 0.1041 - val_accuracy: 0.9417\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1506 - accuracy: 0.9162 - val_loss: 0.1456 - val_accuracy: 0.8917\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1462 - accuracy: 0.9259 - val_loss: 0.0935 - val_accuracy: 0.9500\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1382 - accuracy: 0.9306 - val_loss: 0.0781 - val_accuracy: 0.9583\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1239 - accuracy: 0.9398 - val_loss: 0.0662 - val_accuracy: 0.9583\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1359 - accuracy: 0.9269 - val_loss: 0.0764 - val_accuracy: 0.9750\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1132 - accuracy: 0.9398 - val_loss: 0.1817 - val_accuracy: 0.8583\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1378 - accuracy: 0.9319 - val_loss: 0.4468 - val_accuracy: 0.8292\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.1151 - accuracy: 0.9458 - val_loss: 0.1078 - val_accuracy: 0.9417\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1176 - accuracy: 0.9361 - val_loss: 0.2067 - val_accuracy: 0.8792\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0916 - accuracy: 0.9509 - val_loss: 0.2880 - val_accuracy: 0.8167\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0909 - accuracy: 0.9569 - val_loss: 0.0476 - val_accuracy: 0.9833\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0958 - accuracy: 0.9551 - val_loss: 0.0968 - val_accuracy: 0.9250\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0860 - accuracy: 0.9657 - val_loss: 0.5776 - val_accuracy: 0.6958\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0828 - accuracy: 0.9653 - val_loss: 0.1022 - val_accuracy: 0.9292\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0855 - accuracy: 0.9616 - val_loss: 0.0574 - val_accuracy: 0.9708\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0656 - accuracy: 0.9694 - val_loss: 0.0500 - val_accuracy: 0.9792\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0783 - accuracy: 0.9630 - val_loss: 0.0535 - val_accuracy: 0.9792\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0720 - accuracy: 0.9690 - val_loss: 0.0598 - val_accuracy: 0.9750\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0614 - accuracy: 0.9769 - val_loss: 0.0493 - val_accuracy: 0.9750\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0719 - accuracy: 0.9694 - val_loss: 0.0586 - val_accuracy: 0.9750\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0568 - accuracy: 0.9815 - val_loss: 0.0504 - val_accuracy: 0.9750\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0659 - accuracy: 0.9681 - val_loss: 0.0630 - val_accuracy: 0.9708\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0608 - accuracy: 0.9727 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0640 - accuracy: 0.9699 - val_loss: 0.0529 - val_accuracy: 0.9792\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0598 - accuracy: 0.9722 - val_loss: 0.0543 - val_accuracy: 0.9792\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0534 - accuracy: 0.9796 - val_loss: 0.0524 - val_accuracy: 0.9792\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0695 - accuracy: 0.9644 - val_loss: 0.0540 - val_accuracy: 0.9792\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0669 - accuracy: 0.9681 - val_loss: 0.0506 - val_accuracy: 0.9750\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0576 - accuracy: 0.9755 - val_loss: 0.0524 - val_accuracy: 0.9792\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0546 - accuracy: 0.9778 - val_loss: 0.0491 - val_accuracy: 0.9792\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0579 - accuracy: 0.9787 - val_loss: 0.0515 - val_accuracy: 0.9833\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0734 - accuracy: 0.9694 - val_loss: 0.0446 - val_accuracy: 0.9792\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0707 - accuracy: 0.9667 - val_loss: 0.0480 - val_accuracy: 0.9750\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0598 - accuracy: 0.9736 - val_loss: 0.0528 - val_accuracy: 0.9750\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0615 - accuracy: 0.9708 - val_loss: 0.0479 - val_accuracy: 0.9792\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0698 - accuracy: 0.9694 - val_loss: 0.0476 - val_accuracy: 0.9792\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0560 - accuracy: 0.9782 - val_loss: 0.0529 - val_accuracy: 0.9833\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0735 - accuracy: 0.9671 - val_loss: 0.0541 - val_accuracy: 0.9750\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0587 - accuracy: 0.9741 - val_loss: 0.0539 - val_accuracy: 0.9792\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0508 - val_accuracy: 0.9750\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0599 - accuracy: 0.9764 - val_loss: 0.0499 - val_accuracy: 0.9750\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0569 - accuracy: 0.9759 - val_loss: 0.0491 - val_accuracy: 0.9750\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0580 - accuracy: 0.9741 - val_loss: 0.0505 - val_accuracy: 0.9750\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0589 - accuracy: 0.9727 - val_loss: 0.0496 - val_accuracy: 0.9750\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0665 - accuracy: 0.9690 - val_loss: 0.0501 - val_accuracy: 0.9792\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0657 - accuracy: 0.9736 - val_loss: 0.0495 - val_accuracy: 0.9750\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0564 - accuracy: 0.9755 - val_loss: 0.0487 - val_accuracy: 0.9792\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0660 - accuracy: 0.9704 - val_loss: 0.0513 - val_accuracy: 0.9792\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0536 - accuracy: 0.9782 - val_loss: 0.0510 - val_accuracy: 0.9750\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0513 - accuracy: 0.9773 - val_loss: 0.0464 - val_accuracy: 0.9750\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0698 - accuracy: 0.9676 - val_loss: 0.0477 - val_accuracy: 0.9750\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0549 - accuracy: 0.9773 - val_loss: 0.0486 - val_accuracy: 0.9750\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0560 - accuracy: 0.9778 - val_loss: 0.0501 - val_accuracy: 0.9750\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0550 - accuracy: 0.9769 - val_loss: 0.0520 - val_accuracy: 0.9792\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0617 - accuracy: 0.9699 - val_loss: 0.0497 - val_accuracy: 0.9750\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0462 - accuracy: 0.9824 - val_loss: 0.0498 - val_accuracy: 0.9792\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0573 - accuracy: 0.9741 - val_loss: 0.0514 - val_accuracy: 0.9750\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0596 - accuracy: 0.9741 - val_loss: 0.0523 - val_accuracy: 0.9750\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0632 - accuracy: 0.9685 - val_loss: 0.0532 - val_accuracy: 0.9750\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0648 - accuracy: 0.9722 - val_loss: 0.0507 - val_accuracy: 0.9750\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0653 - accuracy: 0.9708 - val_loss: 0.0512 - val_accuracy: 0.9750\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0564 - accuracy: 0.9764 - val_loss: 0.0517 - val_accuracy: 0.9792\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0639 - accuracy: 0.9704 - val_loss: 0.0515 - val_accuracy: 0.9750\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0619 - accuracy: 0.9718 - val_loss: 0.0481 - val_accuracy: 0.9750\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0506 - accuracy: 0.9796 - val_loss: 0.0527 - val_accuracy: 0.9792\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0559 - accuracy: 0.9759 - val_loss: 0.0515 - val_accuracy: 0.9750\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0695 - accuracy: 0.9667 - val_loss: 0.0475 - val_accuracy: 0.9750\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0587 - accuracy: 0.9769 - val_loss: 0.0507 - val_accuracy: 0.9750\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0726 - accuracy: 0.9662 - val_loss: 0.0544 - val_accuracy: 0.9750\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0617 - accuracy: 0.9736 - val_loss: 0.0506 - val_accuracy: 0.9792\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0633 - accuracy: 0.9708 - val_loss: 0.0488 - val_accuracy: 0.9750\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0557 - accuracy: 0.9829 - val_loss: 0.0549 - val_accuracy: 0.9792\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0675 - accuracy: 0.9653 - val_loss: 0.0514 - val_accuracy: 0.9750\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0778 - accuracy: 0.9653 - val_loss: 0.0499 - val_accuracy: 0.9750\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0659 - accuracy: 0.9704 - val_loss: 0.0522 - val_accuracy: 0.9792\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0544 - accuracy: 0.9796 - val_loss: 0.0524 - val_accuracy: 0.9792\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0552 - accuracy: 0.9750 - val_loss: 0.0511 - val_accuracy: 0.9750\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0597 - accuracy: 0.9759 - val_loss: 0.0492 - val_accuracy: 0.9750\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0544 - accuracy: 0.9769 - val_loss: 0.0496 - val_accuracy: 0.9750\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0679 - accuracy: 0.9690 - val_loss: 0.0512 - val_accuracy: 0.9750\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0460 - accuracy: 0.9815 - val_loss: 0.0500 - val_accuracy: 0.9750\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0528 - accuracy: 0.9764 - val_loss: 0.0540 - val_accuracy: 0.9792\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0589 - accuracy: 0.9741 - val_loss: 0.0517 - val_accuracy: 0.9750\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0690 - accuracy: 0.9685 - val_loss: 0.0514 - val_accuracy: 0.9750\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0641 - accuracy: 0.9727 - val_loss: 0.0506 - val_accuracy: 0.9750\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0620 - accuracy: 0.9708 - val_loss: 0.0498 - val_accuracy: 0.9750\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0573 - accuracy: 0.9736 - val_loss: 0.0511 - val_accuracy: 0.9750\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 56s 105ms/step - loss: 0.0627 - accuracy: 0.9736 - val_loss: 0.0490 - val_accuracy: 0.9750\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0565 - accuracy: 0.9792 - val_loss: 0.0522 - val_accuracy: 0.9750\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0573 - accuracy: 0.9755 - val_loss: 0.0501 - val_accuracy: 0.9750\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0535 - accuracy: 0.9778 - val_loss: 0.0485 - val_accuracy: 0.9792\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0760 - accuracy: 0.9616 - val_loss: 0.0545 - val_accuracy: 0.9792\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0630 - accuracy: 0.9731 - val_loss: 0.0485 - val_accuracy: 0.9750\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0588 - accuracy: 0.9759 - val_loss: 0.0492 - val_accuracy: 0.9750\n",
      "Score for fold 7: loss of 0.049247510731220245; accuracy of 97.50000238418579%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 111ms/step - loss: 0.3230 - accuracy: 0.8005 - val_loss: 0.6706 - val_accuracy: 0.7208\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.2373 - accuracy: 0.8606 - val_loss: 0.6696 - val_accuracy: 0.6917\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2082 - accuracy: 0.8759 - val_loss: 0.5419 - val_accuracy: 0.7708\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.2012 - accuracy: 0.8861 - val_loss: 0.3570 - val_accuracy: 0.8000\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1732 - accuracy: 0.9056 - val_loss: 0.2832 - val_accuracy: 0.8042\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1665 - accuracy: 0.9051 - val_loss: 0.3077 - val_accuracy: 0.8250\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1522 - accuracy: 0.9130 - val_loss: 0.5819 - val_accuracy: 0.7458\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1490 - accuracy: 0.9171 - val_loss: 0.4986 - val_accuracy: 0.7250\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1599 - accuracy: 0.9065 - val_loss: 0.1648 - val_accuracy: 0.9083\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1259 - accuracy: 0.9324 - val_loss: 0.1768 - val_accuracy: 0.9000\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1183 - accuracy: 0.9338 - val_loss: 0.3140 - val_accuracy: 0.8208\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.1304 - accuracy: 0.9259 - val_loss: 0.2191 - val_accuracy: 0.8583\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1062 - accuracy: 0.9449 - val_loss: 0.1457 - val_accuracy: 0.9125\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1001 - accuracy: 0.9505 - val_loss: 0.1915 - val_accuracy: 0.9042\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1078 - accuracy: 0.9454 - val_loss: 0.1212 - val_accuracy: 0.9083\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1092 - accuracy: 0.9384 - val_loss: 0.4125 - val_accuracy: 0.7958\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.1005 - accuracy: 0.9454 - val_loss: 0.1516 - val_accuracy: 0.9083\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1024 - accuracy: 0.9505 - val_loss: 0.1191 - val_accuracy: 0.9167\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0786 - accuracy: 0.9602 - val_loss: 0.1125 - val_accuracy: 0.9250\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0735 - accuracy: 0.9648 - val_loss: 0.1333 - val_accuracy: 0.9167\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0800 - accuracy: 0.9602 - val_loss: 0.0992 - val_accuracy: 0.9333\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.1034 - val_accuracy: 0.9292\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0555 - accuracy: 0.9759 - val_loss: 0.0994 - val_accuracy: 0.9375\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0667 - accuracy: 0.9690 - val_loss: 0.1029 - val_accuracy: 0.9500\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0663 - accuracy: 0.9657 - val_loss: 0.1031 - val_accuracy: 0.9333\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0584 - accuracy: 0.9722 - val_loss: 0.0980 - val_accuracy: 0.9417\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0581 - accuracy: 0.9727 - val_loss: 0.1002 - val_accuracy: 0.9292\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0537 - accuracy: 0.9755 - val_loss: 0.0968 - val_accuracy: 0.9458\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0629 - accuracy: 0.9731 - val_loss: 0.1077 - val_accuracy: 0.9250\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0587 - accuracy: 0.9722 - val_loss: 0.1029 - val_accuracy: 0.9375\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0500 - accuracy: 0.9782 - val_loss: 0.0996 - val_accuracy: 0.9458\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0572 - accuracy: 0.9759 - val_loss: 0.1001 - val_accuracy: 0.9458\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0625 - accuracy: 0.9713 - val_loss: 0.1031 - val_accuracy: 0.9458\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.1040 - val_accuracy: 0.9292\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0581 - accuracy: 0.9741 - val_loss: 0.1007 - val_accuracy: 0.9417\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0577 - accuracy: 0.9759 - val_loss: 0.1047 - val_accuracy: 0.9250\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0610 - accuracy: 0.9685 - val_loss: 0.1040 - val_accuracy: 0.9292\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0519 - accuracy: 0.9782 - val_loss: 0.1012 - val_accuracy: 0.9417\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0642 - accuracy: 0.9704 - val_loss: 0.1014 - val_accuracy: 0.9417\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0489 - accuracy: 0.9829 - val_loss: 0.1007 - val_accuracy: 0.9500\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0508 - accuracy: 0.9755 - val_loss: 0.1018 - val_accuracy: 0.9375\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0513 - accuracy: 0.9806 - val_loss: 0.1014 - val_accuracy: 0.9458\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0626 - accuracy: 0.9713 - val_loss: 0.1031 - val_accuracy: 0.9500\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0508 - accuracy: 0.9759 - val_loss: 0.1023 - val_accuracy: 0.9375\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0499 - accuracy: 0.9778 - val_loss: 0.1012 - val_accuracy: 0.9375\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0540 - accuracy: 0.9722 - val_loss: 0.1030 - val_accuracy: 0.9333\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0571 - accuracy: 0.9759 - val_loss: 0.1035 - val_accuracy: 0.9375\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0473 - accuracy: 0.9824 - val_loss: 0.1012 - val_accuracy: 0.9375\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0654 - accuracy: 0.9667 - val_loss: 0.1048 - val_accuracy: 0.9333\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0569 - accuracy: 0.9745 - val_loss: 0.1004 - val_accuracy: 0.9458\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0560 - accuracy: 0.9745 - val_loss: 0.1035 - val_accuracy: 0.9500\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0536 - accuracy: 0.9792 - val_loss: 0.1008 - val_accuracy: 0.9458\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0580 - accuracy: 0.9727 - val_loss: 0.1021 - val_accuracy: 0.9375\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0482 - accuracy: 0.9833 - val_loss: 0.1035 - val_accuracy: 0.9458\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0547 - accuracy: 0.9769 - val_loss: 0.1015 - val_accuracy: 0.9458\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0643 - accuracy: 0.9708 - val_loss: 0.1031 - val_accuracy: 0.9458\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0553 - accuracy: 0.9755 - val_loss: 0.1040 - val_accuracy: 0.9292\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0630 - accuracy: 0.9681 - val_loss: 0.1022 - val_accuracy: 0.9417\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0681 - accuracy: 0.9648 - val_loss: 0.1013 - val_accuracy: 0.9500\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0581 - accuracy: 0.9741 - val_loss: 0.1032 - val_accuracy: 0.9500\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0515 - accuracy: 0.9769 - val_loss: 0.1045 - val_accuracy: 0.9500\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0539 - accuracy: 0.9773 - val_loss: 0.0996 - val_accuracy: 0.9500\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0528 - accuracy: 0.9787 - val_loss: 0.1007 - val_accuracy: 0.9458\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0580 - accuracy: 0.9801 - val_loss: 0.1030 - val_accuracy: 0.9458\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0579 - accuracy: 0.9755 - val_loss: 0.1027 - val_accuracy: 0.9500\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0571 - accuracy: 0.9745 - val_loss: 0.1029 - val_accuracy: 0.9417\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0525 - accuracy: 0.9787 - val_loss: 0.1024 - val_accuracy: 0.9458\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0559 - accuracy: 0.9699 - val_loss: 0.1039 - val_accuracy: 0.9333\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0497 - accuracy: 0.9801 - val_loss: 0.1009 - val_accuracy: 0.9500\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0670 - accuracy: 0.9644 - val_loss: 0.1037 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0440 - accuracy: 0.9796 - val_loss: 0.1045 - val_accuracy: 0.9292\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0562 - accuracy: 0.9727 - val_loss: 0.1027 - val_accuracy: 0.9500\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0657 - accuracy: 0.9657 - val_loss: 0.1044 - val_accuracy: 0.9375\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0435 - accuracy: 0.9847 - val_loss: 0.1037 - val_accuracy: 0.9375\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0495 - accuracy: 0.9801 - val_loss: 0.1016 - val_accuracy: 0.9458\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0495 - accuracy: 0.9792 - val_loss: 0.1046 - val_accuracy: 0.9292\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0491 - accuracy: 0.9824 - val_loss: 0.1007 - val_accuracy: 0.9500\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0472 - accuracy: 0.9815 - val_loss: 0.1034 - val_accuracy: 0.9375\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0506 - accuracy: 0.9796 - val_loss: 0.1036 - val_accuracy: 0.9333\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0553 - accuracy: 0.9755 - val_loss: 0.1046 - val_accuracy: 0.9375\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0649 - accuracy: 0.9644 - val_loss: 0.1042 - val_accuracy: 0.9375\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0579 - accuracy: 0.9764 - val_loss: 0.1042 - val_accuracy: 0.9458\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0593 - accuracy: 0.9727 - val_loss: 0.1000 - val_accuracy: 0.9417\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0492 - accuracy: 0.9796 - val_loss: 0.1040 - val_accuracy: 0.9417\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0558 - accuracy: 0.9750 - val_loss: 0.1040 - val_accuracy: 0.9458\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0603 - accuracy: 0.9713 - val_loss: 0.1047 - val_accuracy: 0.9292\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0530 - accuracy: 0.9782 - val_loss: 0.1017 - val_accuracy: 0.9500\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0521 - accuracy: 0.9796 - val_loss: 0.1028 - val_accuracy: 0.9458\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.1014 - val_accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0629 - accuracy: 0.9745 - val_loss: 0.1039 - val_accuracy: 0.9458\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0549 - accuracy: 0.9741 - val_loss: 0.1035 - val_accuracy: 0.9375\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0682 - accuracy: 0.9653 - val_loss: 0.1067 - val_accuracy: 0.9375\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0602 - accuracy: 0.9736 - val_loss: 0.1061 - val_accuracy: 0.9333\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0519 - accuracy: 0.9792 - val_loss: 0.1020 - val_accuracy: 0.9458\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0467 - accuracy: 0.9806 - val_loss: 0.1033 - val_accuracy: 0.9417\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0493 - accuracy: 0.9787 - val_loss: 0.1046 - val_accuracy: 0.9458\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0580 - accuracy: 0.9713 - val_loss: 0.1026 - val_accuracy: 0.9375\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0576 - accuracy: 0.9736 - val_loss: 0.1032 - val_accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0637 - accuracy: 0.9694 - val_loss: 0.1030 - val_accuracy: 0.9417\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0519 - accuracy: 0.9759 - val_loss: 0.1026 - val_accuracy: 0.9458\n",
      "Score for fold 8: loss of 0.10257302224636078; accuracy of 94.58333253860474%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 68s 110ms/step - loss: 0.3403 - accuracy: 0.7880 - val_loss: 0.2089 - val_accuracy: 0.8875\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2464 - accuracy: 0.8542 - val_loss: 1.7116 - val_accuracy: 0.5708\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2234 - accuracy: 0.8616 - val_loss: 0.1129 - val_accuracy: 0.9292\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2013 - accuracy: 0.8815 - val_loss: 1.0032 - val_accuracy: 0.6917\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1744 - accuracy: 0.8931 - val_loss: 0.1121 - val_accuracy: 0.9292\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1725 - accuracy: 0.9060 - val_loss: 0.0920 - val_accuracy: 0.9417\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1576 - accuracy: 0.9106 - val_loss: 0.1280 - val_accuracy: 0.9292\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1501 - accuracy: 0.9213 - val_loss: 0.0869 - val_accuracy: 0.9500\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1486 - accuracy: 0.9097 - val_loss: 0.1062 - val_accuracy: 0.9500\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1327 - accuracy: 0.9259 - val_loss: 0.0887 - val_accuracy: 0.9500\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1356 - accuracy: 0.9264 - val_loss: 0.4216 - val_accuracy: 0.7458\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1182 - accuracy: 0.9338 - val_loss: 0.1369 - val_accuracy: 0.9375\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1158 - accuracy: 0.9338 - val_loss: 0.2206 - val_accuracy: 0.8667\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1089 - accuracy: 0.9389 - val_loss: 0.0837 - val_accuracy: 0.9583\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0935 - accuracy: 0.9509 - val_loss: 0.0658 - val_accuracy: 0.9792\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1009 - accuracy: 0.9468 - val_loss: 0.1717 - val_accuracy: 0.8875\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1051 - accuracy: 0.9444 - val_loss: 0.0505 - val_accuracy: 0.9792\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0915 - accuracy: 0.9528 - val_loss: 0.1109 - val_accuracy: 0.9333\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0807 - accuracy: 0.9560 - val_loss: 0.0701 - val_accuracy: 0.9500\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0886 - accuracy: 0.9537 - val_loss: 0.0703 - val_accuracy: 0.9542\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0677 - accuracy: 0.9639 - val_loss: 0.0725 - val_accuracy: 0.9500\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0662 - accuracy: 0.9662 - val_loss: 0.0650 - val_accuracy: 0.9542\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0617 - accuracy: 0.9681 - val_loss: 0.0692 - val_accuracy: 0.9583\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0690 - accuracy: 0.9667 - val_loss: 0.0672 - val_accuracy: 0.9542\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0674 - accuracy: 0.9694 - val_loss: 0.0691 - val_accuracy: 0.9500\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0616 - accuracy: 0.9708 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0803 - accuracy: 0.9528 - val_loss: 0.0743 - val_accuracy: 0.9458\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0721 - accuracy: 0.9639 - val_loss: 0.0823 - val_accuracy: 0.9458\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0615 - accuracy: 0.9690 - val_loss: 0.0801 - val_accuracy: 0.9458\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0538 - accuracy: 0.9769 - val_loss: 0.0701 - val_accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0695 - accuracy: 0.9644 - val_loss: 0.0583 - val_accuracy: 0.9792\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0630 - accuracy: 0.9731 - val_loss: 0.0671 - val_accuracy: 0.9542\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0519 - accuracy: 0.9806 - val_loss: 0.0660 - val_accuracy: 0.9583\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0528 - accuracy: 0.9773 - val_loss: 0.0636 - val_accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0778 - val_accuracy: 0.9458\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0660 - accuracy: 0.9657 - val_loss: 0.0597 - val_accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0527 - accuracy: 0.9759 - val_loss: 0.0604 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0491 - accuracy: 0.9819 - val_loss: 0.0600 - val_accuracy: 0.9625\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0496 - accuracy: 0.9764 - val_loss: 0.1182 - val_accuracy: 0.9250\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0649 - accuracy: 0.9685 - val_loss: 0.0703 - val_accuracy: 0.9583\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0544 - accuracy: 0.9755 - val_loss: 0.0713 - val_accuracy: 0.9583\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0520 - accuracy: 0.9755 - val_loss: 0.0611 - val_accuracy: 0.9667\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0551 - accuracy: 0.9745 - val_loss: 0.0692 - val_accuracy: 0.9583\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0637 - accuracy: 0.9657 - val_loss: 0.0805 - val_accuracy: 0.9417\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0549 - accuracy: 0.9745 - val_loss: 0.0741 - val_accuracy: 0.9500\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0573 - accuracy: 0.9727 - val_loss: 0.0685 - val_accuracy: 0.9583\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0546 - accuracy: 0.9736 - val_loss: 0.0737 - val_accuracy: 0.9500\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0538 - accuracy: 0.9769 - val_loss: 0.0639 - val_accuracy: 0.9667\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0640 - accuracy: 0.9694 - val_loss: 0.0697 - val_accuracy: 0.9542\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0585 - accuracy: 0.9731 - val_loss: 0.0645 - val_accuracy: 0.9583\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0527 - accuracy: 0.9773 - val_loss: 0.0757 - val_accuracy: 0.9500\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0621 - accuracy: 0.9690 - val_loss: 0.0732 - val_accuracy: 0.9500\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0657 - accuracy: 0.9704 - val_loss: 0.0695 - val_accuracy: 0.9542\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0598 - accuracy: 0.9722 - val_loss: 0.0691 - val_accuracy: 0.9542\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0755 - accuracy: 0.9597 - val_loss: 0.0805 - val_accuracy: 0.9458\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0489 - accuracy: 0.9815 - val_loss: 0.0672 - val_accuracy: 0.9625\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0626 - accuracy: 0.9718 - val_loss: 0.0678 - val_accuracy: 0.9542\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0532 - accuracy: 0.9755 - val_loss: 0.0823 - val_accuracy: 0.9417\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0535 - accuracy: 0.9773 - val_loss: 0.0700 - val_accuracy: 0.9583\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0602 - accuracy: 0.9690 - val_loss: 0.0751 - val_accuracy: 0.9458\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0630 - accuracy: 0.9690 - val_loss: 0.0697 - val_accuracy: 0.9500\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0607 - accuracy: 0.9718 - val_loss: 0.0693 - val_accuracy: 0.9542\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0507 - accuracy: 0.9801 - val_loss: 0.0756 - val_accuracy: 0.9500\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0498 - accuracy: 0.9773 - val_loss: 0.0799 - val_accuracy: 0.9458\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0550 - accuracy: 0.9731 - val_loss: 0.0741 - val_accuracy: 0.9500\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0611 - accuracy: 0.9667 - val_loss: 0.0722 - val_accuracy: 0.9500\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0592 - accuracy: 0.9741 - val_loss: 0.0717 - val_accuracy: 0.9500\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0595 - accuracy: 0.9699 - val_loss: 0.0719 - val_accuracy: 0.9500\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0503 - accuracy: 0.9796 - val_loss: 0.0667 - val_accuracy: 0.9583\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0566 - accuracy: 0.9755 - val_loss: 0.0851 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0536 - accuracy: 0.9769 - val_loss: 0.0740 - val_accuracy: 0.9500\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0586 - accuracy: 0.9681 - val_loss: 0.0682 - val_accuracy: 0.9542\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0621 - accuracy: 0.9704 - val_loss: 0.0649 - val_accuracy: 0.9625\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0685 - accuracy: 0.9634 - val_loss: 0.0636 - val_accuracy: 0.9625\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0540 - accuracy: 0.9741 - val_loss: 0.0700 - val_accuracy: 0.9500\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0650 - accuracy: 0.9662 - val_loss: 0.0687 - val_accuracy: 0.9542\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0650 - accuracy: 0.9648 - val_loss: 0.0728 - val_accuracy: 0.9500\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0638 - accuracy: 0.9671 - val_loss: 0.0672 - val_accuracy: 0.9542\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0559 - accuracy: 0.9778 - val_loss: 0.0703 - val_accuracy: 0.9500\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0588 - accuracy: 0.9745 - val_loss: 0.0721 - val_accuracy: 0.9500\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0789 - accuracy: 0.9556 - val_loss: 0.0669 - val_accuracy: 0.9542\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0553 - accuracy: 0.9731 - val_loss: 0.0696 - val_accuracy: 0.9542\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0663 - accuracy: 0.9657 - val_loss: 0.0577 - val_accuracy: 0.9667\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0530 - accuracy: 0.9736 - val_loss: 0.0711 - val_accuracy: 0.9500\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0526 - accuracy: 0.9764 - val_loss: 0.0701 - val_accuracy: 0.9500\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0566 - accuracy: 0.9745 - val_loss: 0.0654 - val_accuracy: 0.9625\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0578 - accuracy: 0.9745 - val_loss: 0.0642 - val_accuracy: 0.9625\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0651 - val_accuracy: 0.9625\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0567 - accuracy: 0.9713 - val_loss: 0.0716 - val_accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0661 - accuracy: 0.9616 - val_loss: 0.0629 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0578 - accuracy: 0.9736 - val_loss: 0.0813 - val_accuracy: 0.9458\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0653 - accuracy: 0.9694 - val_loss: 0.0682 - val_accuracy: 0.9500\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0526 - accuracy: 0.9764 - val_loss: 0.0620 - val_accuracy: 0.9667\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0608 - accuracy: 0.9657 - val_loss: 0.0720 - val_accuracy: 0.9500\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0542 - accuracy: 0.9731 - val_loss: 0.0737 - val_accuracy: 0.9500\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0550 - accuracy: 0.9769 - val_loss: 0.0702 - val_accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0576 - accuracy: 0.9727 - val_loss: 0.0734 - val_accuracy: 0.9500\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0528 - accuracy: 0.9782 - val_loss: 0.0677 - val_accuracy: 0.9583\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0720 - accuracy: 0.9644 - val_loss: 0.0809 - val_accuracy: 0.9458\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0605 - accuracy: 0.9690 - val_loss: 0.0740 - val_accuracy: 0.9500\n",
      "Score for fold 9: loss of 0.07396426796913147; accuracy of 94.9999988079071%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 113ms/step - loss: 0.3417 - accuracy: 0.7806 - val_loss: 2.1014 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2502 - accuracy: 0.8514 - val_loss: 0.1288 - val_accuracy: 0.9167\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2166 - accuracy: 0.8755 - val_loss: 0.1336 - val_accuracy: 0.9083\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1999 - accuracy: 0.8880 - val_loss: 0.1404 - val_accuracy: 0.9125\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1739 - accuracy: 0.8991 - val_loss: 2.3681 - val_accuracy: 0.4875\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1681 - accuracy: 0.9042 - val_loss: 0.0914 - val_accuracy: 0.9375\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1441 - accuracy: 0.9236 - val_loss: 0.3340 - val_accuracy: 0.7917\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1460 - accuracy: 0.9204 - val_loss: 0.3206 - val_accuracy: 0.7708\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1223 - accuracy: 0.9324 - val_loss: 0.1357 - val_accuracy: 0.9042\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1302 - accuracy: 0.9273 - val_loss: 0.0881 - val_accuracy: 0.9458\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1331 - accuracy: 0.9301 - val_loss: 0.0826 - val_accuracy: 0.9542\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1300 - accuracy: 0.9301 - val_loss: 0.3269 - val_accuracy: 0.7792\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1034 - accuracy: 0.9454 - val_loss: 0.2503 - val_accuracy: 0.8625\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0997 - accuracy: 0.9495 - val_loss: 0.0898 - val_accuracy: 0.9417\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1019 - accuracy: 0.9468 - val_loss: 0.0981 - val_accuracy: 0.9292\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1008 - accuracy: 0.9463 - val_loss: 0.2731 - val_accuracy: 0.8333\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0985 - accuracy: 0.9472 - val_loss: 0.0989 - val_accuracy: 0.9417\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0810 - accuracy: 0.9593 - val_loss: 0.0831 - val_accuracy: 0.9417\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0763 - accuracy: 0.9630 - val_loss: 0.2659 - val_accuracy: 0.8583\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0822 - accuracy: 0.9579 - val_loss: 0.2034 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0758 - accuracy: 0.9597 - val_loss: 0.0517 - val_accuracy: 0.9750\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0652 - accuracy: 0.9741 - val_loss: 0.0575 - val_accuracy: 0.9667\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0706 - accuracy: 0.9671 - val_loss: 0.0448 - val_accuracy: 0.9917\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0607 - accuracy: 0.9727 - val_loss: 0.0614 - val_accuracy: 0.9625\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0581 - accuracy: 0.9745 - val_loss: 0.0547 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0581 - accuracy: 0.9736 - val_loss: 0.0418 - val_accuracy: 0.9833\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0578 - accuracy: 0.9782 - val_loss: 0.0552 - val_accuracy: 0.9750\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0579 - accuracy: 0.9704 - val_loss: 0.0607 - val_accuracy: 0.9583\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0624 - accuracy: 0.9727 - val_loss: 0.0462 - val_accuracy: 0.9833\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0579 - accuracy: 0.9745 - val_loss: 0.0376 - val_accuracy: 0.9917\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0569 - accuracy: 0.9741 - val_loss: 0.0423 - val_accuracy: 0.9833\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0755 - accuracy: 0.9657 - val_loss: 0.0616 - val_accuracy: 0.9625\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0564 - accuracy: 0.9769 - val_loss: 0.0430 - val_accuracy: 0.9792\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0648 - accuracy: 0.9657 - val_loss: 0.0411 - val_accuracy: 0.9875\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0578 - accuracy: 0.9755 - val_loss: 0.0496 - val_accuracy: 0.9792\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0468 - accuracy: 0.9824 - val_loss: 0.0684 - val_accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0644 - accuracy: 0.9708 - val_loss: 0.0412 - val_accuracy: 0.9833\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0629 - accuracy: 0.9699 - val_loss: 0.0652 - val_accuracy: 0.9583\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0588 - accuracy: 0.9736 - val_loss: 0.0483 - val_accuracy: 0.9792\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0512 - accuracy: 0.9787 - val_loss: 0.0395 - val_accuracy: 0.9875\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0614 - accuracy: 0.9741 - val_loss: 0.0434 - val_accuracy: 0.9833\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0512 - accuracy: 0.9801 - val_loss: 0.0441 - val_accuracy: 0.9792\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0584 - accuracy: 0.9722 - val_loss: 0.0478 - val_accuracy: 0.9792\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0628 - accuracy: 0.9704 - val_loss: 0.0485 - val_accuracy: 0.9792\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0448 - accuracy: 0.9796 - val_loss: 0.0450 - val_accuracy: 0.9833\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0588 - accuracy: 0.9731 - val_loss: 0.0508 - val_accuracy: 0.9792\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0520 - accuracy: 0.9759 - val_loss: 0.0465 - val_accuracy: 0.9792\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0490 - accuracy: 0.9824 - val_loss: 0.0475 - val_accuracy: 0.9792\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0607 - accuracy: 0.9741 - val_loss: 0.0491 - val_accuracy: 0.9792\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0575 - accuracy: 0.9731 - val_loss: 0.0472 - val_accuracy: 0.9792\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0582 - accuracy: 0.9759 - val_loss: 0.0449 - val_accuracy: 0.9833\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0625 - accuracy: 0.9722 - val_loss: 0.0434 - val_accuracy: 0.9792\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0535 - accuracy: 0.9773 - val_loss: 0.0487 - val_accuracy: 0.9792\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0479 - accuracy: 0.9819 - val_loss: 0.0480 - val_accuracy: 0.9792\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0583 - accuracy: 0.9755 - val_loss: 0.0446 - val_accuracy: 0.9792\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0490 - accuracy: 0.9787 - val_loss: 0.0462 - val_accuracy: 0.9833\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0500 - accuracy: 0.9796 - val_loss: 0.0477 - val_accuracy: 0.9792\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0448 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9750\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0473 - accuracy: 0.9843 - val_loss: 0.0461 - val_accuracy: 0.9833\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0572 - accuracy: 0.9708 - val_loss: 0.0486 - val_accuracy: 0.9792\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0527 - accuracy: 0.9792 - val_loss: 0.0449 - val_accuracy: 0.9792\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.0452 - val_accuracy: 0.9792\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0447 - accuracy: 0.9819 - val_loss: 0.0499 - val_accuracy: 0.9792\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0449 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9708\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0613 - accuracy: 0.9713 - val_loss: 0.0461 - val_accuracy: 0.9792\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0563 - accuracy: 0.9741 - val_loss: 0.0487 - val_accuracy: 0.9792\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0519 - accuracy: 0.9759 - val_loss: 0.0453 - val_accuracy: 0.9792\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0525 - accuracy: 0.9759 - val_loss: 0.0410 - val_accuracy: 0.9875\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0587 - accuracy: 0.9722 - val_loss: 0.0453 - val_accuracy: 0.9792\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0572 - accuracy: 0.9736 - val_loss: 0.0422 - val_accuracy: 0.9792\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0533 - accuracy: 0.9778 - val_loss: 0.0488 - val_accuracy: 0.9792\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0555 - accuracy: 0.9727 - val_loss: 0.0499 - val_accuracy: 0.9750\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0529 - accuracy: 0.9745 - val_loss: 0.0551 - val_accuracy: 0.9708\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0615 - accuracy: 0.9708 - val_loss: 0.0512 - val_accuracy: 0.9792\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0499 - accuracy: 0.9801 - val_loss: 0.0400 - val_accuracy: 0.9875\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0547 - accuracy: 0.9745 - val_loss: 0.0437 - val_accuracy: 0.9792\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0494 - accuracy: 0.9838 - val_loss: 0.0425 - val_accuracy: 0.9792\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0568 - accuracy: 0.9736 - val_loss: 0.0450 - val_accuracy: 0.9792\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0525 - accuracy: 0.9787 - val_loss: 0.0479 - val_accuracy: 0.9833\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0561 - accuracy: 0.9727 - val_loss: 0.0490 - val_accuracy: 0.9792\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 56s 104ms/step - loss: 0.0541 - accuracy: 0.9731 - val_loss: 0.0474 - val_accuracy: 0.9792\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 57s 105ms/step - loss: 0.0556 - accuracy: 0.9736 - val_loss: 0.0477 - val_accuracy: 0.9792\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0579 - accuracy: 0.9736 - val_loss: 0.0523 - val_accuracy: 0.9792\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0490 - accuracy: 0.9796 - val_loss: 0.0485 - val_accuracy: 0.9792\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0588 - accuracy: 0.9722 - val_loss: 0.0458 - val_accuracy: 0.9792\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.0513 - val_accuracy: 0.9750\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0567 - accuracy: 0.9745 - val_loss: 0.0439 - val_accuracy: 0.9792\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0538 - accuracy: 0.9745 - val_loss: 0.0431 - val_accuracy: 0.9833\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 66s 123ms/step - loss: 0.0517 - accuracy: 0.9778 - val_loss: 0.0424 - val_accuracy: 0.9792\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 65s 121ms/step - loss: 0.0460 - accuracy: 0.9806 - val_loss: 0.0434 - val_accuracy: 0.9792\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0546 - accuracy: 0.9764 - val_loss: 0.0533 - val_accuracy: 0.9708\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0637 - accuracy: 0.9694 - val_loss: 0.0523 - val_accuracy: 0.9750\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0685 - accuracy: 0.9676 - val_loss: 0.0436 - val_accuracy: 0.9792\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0642 - accuracy: 0.9708 - val_loss: 0.0405 - val_accuracy: 0.9875\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0540 - accuracy: 0.9778 - val_loss: 0.0421 - val_accuracy: 0.9875\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0642 - accuracy: 0.9681 - val_loss: 0.0459 - val_accuracy: 0.9875\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0495 - accuracy: 0.9801 - val_loss: 0.0498 - val_accuracy: 0.9792\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0497 - accuracy: 0.9792 - val_loss: 0.0467 - val_accuracy: 0.9792\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0610 - accuracy: 0.9713 - val_loss: 0.0474 - val_accuracy: 0.9792\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0576 - accuracy: 0.9685 - val_loss: 0.0543 - val_accuracy: 0.9708\n",
      "Score for fold 10: loss of 0.05430171638727188; accuracy of 97.08333611488342%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.0634547546505928 - Accuracy: 96.24999761581421%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.08136415481567383 - Accuracy: 94.58333253860474%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.06849122047424316 - Accuracy: 97.08333611488342%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.05439552292227745 - Accuracy: 97.08333611488342%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.050885576754808426 - Accuracy: 97.91666865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.04010511189699173 - Accuracy: 97.91666865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.049247510731220245 - Accuracy: 97.50000238418579%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.10257302224636078 - Accuracy: 94.58333253860474%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.07396426796913147 - Accuracy: 94.9999988079071%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.05430171638727188 - Accuracy: 97.08333611488342%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 96.50000095367432 (+- 1.2527761599034104)\n",
      "> Loss: 0.06387828588485718\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        np.savetxt('E:/3.19/RP/train/' + f'train_{fold_no}.csv', train, delimiter=\",\")\n",
    "        np.savetxt('E:/3.19/RP/test/' + f'test_{fold_no}.csv', test, delimiter=\",\")\n",
    "\n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = callbacks_list) #  Validation set  ?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/3.19/RP/weight/' + f'RP_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7ec7915fb6663623914b2919607c696d31d8503b403c6d54bcb30c0bb224a7"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
